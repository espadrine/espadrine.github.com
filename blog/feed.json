{
  "version": "https://jsonfeed.org/version/1",
  "title": "The espadrine blog.",
  "description": "Tech deep dives, discoveries, and analyses.",
  "home_page_url": "https://espadrine.github.io/blog/",
  "feed_url": "https://espadrine.github.io/blog/feed.json",
  "author": {
    "name": "Thaddée Tyl",
    "url": "https://github.com/espadrine"
  },
  "favicon": "https://avatars.githubusercontent.com/u/100689?s=64",
  "items": [
      {
        "id":  "https://espadrine.github.io/blog/posts/chinchilla-s-death.html",
        "url": "https://espadrine.github.io/blog/posts/chinchilla-s-death.html",
        "title": "Chinchilla’s Death",
        "tags": "gpu ml",
        "date_published": "2023-07-23T17:35:02Z"
        "content_html": "<h1 id=\"Chinchilla_s_Death\">Chinchilla’s Death <a href=\"#Chinchilla_s_Death\" class=\"autolink-clicker\" aria-hidden=\"true\">§</a></h1>\n<blockquote>\n<p>“With more careful calculations, one can win; with less, one cannot”\n— Sun Tzu, <em>The Art of War</em>.</p>\n</blockquote>\n<p>Making extrapolations is crucial to avoid wasting our computing power on slow\nconvergence. After all, if you had to walk to the Everest,\nyou wouldn’t eyeball it: you would use a GPS.</p>\n<p>Sometimes you have to look away from the GPS and onto the road, though.\nSometimes things don’t extrapolate through simple formulae.\nIt was true for XIXth-century physicists with the <a href=\"https://en.wikipedia.org/wiki/Ultraviolet_catastrophe\">ultraviolet catastrophe</a>;\nit is true for LLMs too.\nWhat we estimate to be true near the center can deviate widely in the far lands…</p>\n<p><img src=\"https://i.imgur.com/Mf85NuW.png\" alt=\"Image of Minecraft far lands: a terrain that suddenly becomes distorted and overlaps itself cliffly\" /></p>\n<h2 id=\"What_s_this_Chinchilla_thing_anyway_\">What’s this Chinchilla thing anyway? <a href=\"#What_s_this_Chinchilla_thing_anyway_\" class=\"autolink-clicker\" aria-hidden=\"true\">§</a></h2>\n<p>Smaller models have fewer multiplications.\nThus they run faster. Thus they train faster.\nHowever, the theory goes, they eventually reach the limit of their capacity for\nknowledge, and their learning slows, while that of a larger model,\nwith a larger capacity, will overtake them and reach better performance\npast a given amount of training time.</p>\n<p>While estimating how to get the best bang for the buck during training,\nboth <a href=\"https://arxiv.org/abs/2001.08361\">OpenAI</a> and <a href=\"https://arxiv.org/abs/2203.15556\">DeepMind</a> attempted to draw the Pareto\nfrontier. They don’t state explicitly that they use that theory to draw it;\nthe closest quote that hints at this hidden assumption is from OpenAI:</p>\n<blockquote>\n<p>We expect that larger models should always perform better than smaller models.\n[…]\nA model with fixed size will be capacity-limited.</p>\n</blockquote>\n<p>This presumption is the bedrock of how they compute the Pareto frontier.\nIn the Chinchilla work, figure 2 shows the training loss of a large number of\ntraining runs for models with varying size.\nAt a first glance, those curves follow the theory:\nthe smaller models initially have a lower loss (good),\nbut eventually it slows down,\nand gets overtaken by the curve from a larger model (bad).</p>\n<p><img src=\"../assets/chinchilla-s-death/chinchilla.png\" alt=\"Chinchilla graph comparing the loss curves for many different model sizes\" /></p>\n<p>In that chart, they drew grey dots every time they pinpointed the smaller model\nstarting to lose out to a larger model.\nThe grey line, the Pareto frontier, is how they computed their scaling laws.</p>\n<p>The problem with this assumption is that\nwe have no idea what would happen if we let the smaller model train for longer,\nsince they stopped its training as soon as it was overtaken.</p>\n<p>Enter the LLaMA paper.</p>\n<h2 id=\"Can_Chinchillas_picture_a_Llama_s_sights_\">Can Chinchillas picture a Llama’s sights? <a href=\"#Can_Chinchillas_picture_a_Llama_s_sights_\" class=\"autolink-clicker\" aria-hidden=\"true\">§</a></h2>\n<p>Earlier this year, Meta trained four models with varying sizes.\nUnlike other works, they trained each of them for a very large amount of time;\neven the smaller ones.</p>\n<p>They published the training run curves:</p>\n<p><img src=\"../assets/chinchilla-s-death/llama1-training.png\" alt=\"Training loss curves for the four LLaMA model sizes\" /></p>\n<ol>\n<li>Each curve first plummets in a <strong>power law</strong>,</li>\n<li>and then seemingly enters a <strong>nearly-linear</strong> decrease in loss\n(corresponding to a fairly constant rate of knowledge acquisition).</li>\n<li>At the very tip of the curve, they all break this line by <strong>flattening</strong>\nslightly.</li>\n</ol>\n<p>Right off the bat, I want to tackle a subtle misconception that people can have\nrelated to the end-of-curve flattening.\nThey are all trained with gradient descent using a variable learning rate\n(which is, roughly,\na hyperparameter for how much to go in the direction of the gradient).\nTo get a good training, they had to constantly decrease the learning rate,\nso that it can detect ever-subtler patterns in the source material.\nThe formula they use for that decrease is the most widely used:\nthe cosine schedule.</p>\n<p><img src=\"../assets/chinchilla-s-death/warmup_cosine_schedule.png\" alt=\"Learning rate as a function of training steps under a cosine schedule with\nwarmup: it first increases linearly, then slopes down with increasing speed,\nbefore reaching an inflection point halfway and slowing down ever slower. Image from Huggingface documentation\" /></p>\n<p>As you can see from the graph, towards the end of the training run,\nthe cosine schedule stops decreasing the learning rate at the speed which\nyielded such a good, near-linear training loss curve.\nThe slowdown in learning is an artefact of that.\nThe model does not necessarily cease to have\nthe capacity to learn at the same near-linear rate!\nIn fact, if we had more text to give it,\nwe would have stretched the cosine schedule,\nso its learning rate would have continued to go down at the same rate.</p>\n<p>The model’s fitness landscape does not depend on the amount of data\nwe can feed its training; so the change in learning rate decrease\nis not well-justified.</p>\n<p>That is not the main point of this article, though.</p>\n<p>The training loss curve can be misleading in another way.\nSure, they are all trained on the same data;\nbut they don’t go through that data at the same speed.\nWhat we want to know is <strong>not</strong> how sample-efficient the model is\n(on this front, the larger model clearly learns more from what it saw).\nLet’s picture instead a race:\nall those models start at the same time,\nand we want to know which one crosses the finish line first.\nIn other words, when throwing a fixed amount of compute at the training,\nwho learns the most in that time?</p>\n<p>Thankfully, we can combine the loss curves with another piece of data that Meta\nprovided: the amount of time that each model took to train.</p>\n<table>\n <tr><th>   Model   </th><th> GPU-hours </th><th> Tokens/second </th>\n <tr><td> LLaMA1-7B  </td><td>   82432  </td><td>    3384.3    </td>\n <tr><td> LLaMA1-13B </td><td>  135168  </td><td>    2063.9    </td>\n <tr><td> LLaMA1-33B </td><td>  530432  </td><td>     730.5    </td>\n <tr><td> LLaMA1-65B </td><td> 1022362  </td><td>     379.0    </td>\n</table>\n<p><img src=\"../assets/chinchilla-s-death/llama1-training-speed.svg\" alt=\"LLaMA 1 training loss vs GPU-hours spent\" /></p>\n<p><a href=\"https://github.com/espadrine/espadrine.github.com/blob/master/blog/assets/chinchilla-s-death/llama-data.py\"><em>(Code for generating the graph here.)</em></a></p>\n<p>Let’s first mention that the whole Chinchilla graph that we saw,\ncovers only a small sliver on the left of this graph.\nIn that sliver, we see the same behaviour that Chinchilla documents.\nLook at the 7B, for instance (which in the Chinchilla graph would actually be\namong the top two curves in terms of size):\nit initially drops its loss much faster than the bigger models, then slows down,\nand the 13B model overtakes it and reaches 1.9 first.</p>\n<p>But then, comes a far-lands, unexpected twist: the 7B enters a near-linear\nregime, with a steep downward trend, and seems on its way to maybe overpass the\n13B again? It is hard to tell on that graph what would happen if the 7B was\ntrained for longer.</p>\n<p>However, the same behaviour seemed to be true between the 13B and the 33B,\nwhere the initial Chinchilla slowdown also gives way to a near-linear regime,\nat which point the 13B goes down fast! It is only surpassed by the 33B unfairly,\nby granting the latter more than double the compute time.</p>\n<p>And the same slowdown-then-speedup occurs between the 33B and the 65B,\nto such an extent that the 33B never actually gets overtaken by the 65B.\nWhat the graph shows breaks OpenAI’s and Chinchilla’s assumption:\n<strong>the bigger model hasn’t won</strong> (yet).\nThe slowdown they detected is not actually caused by reaching some capacity limit!</p>\n<p>Still, that 7B line is a bit unsatisfactory.\nIf only Meta had trained it for longer…</p>\n<p>Suspense over: they did! They released LLaMA 2 this week!</p>\n<h2 id=\"Time_to_confirm_our_suspicions\">Time to confirm our suspicions <a href=\"#Time_to_confirm_our_suspicions\" class=\"autolink-clicker\" aria-hidden=\"true\">§</a></h2>\n<p><img src=\"../assets/chinchilla-s-death/llama2-training.png\" alt=\"Training loss curves for the four LLaMA 2 model sizes\" /></p>\n<p>We also, again, got the training times:</p>\n<table>\n <tr><th>   Model   </th><th> GPU-hours </th><th> Tokens/second </th>\n <tr><td> LLaMA2-7B  </td><td>  184320  </td><td>    3031.9    </td>\n <tr><td> LLaMA2-13B </td><td>  368640  </td><td>    1515.9    </td>\n <tr><td> LLaMA2-34B </td><td> 1038336  </td><td>     533.7    </td>\n <tr><td> LLaMA2-70B </td><td> 1720320  </td><td>     322.1    </td>\n</table>\n<p><img src=\"../assets/chinchilla-s-death/llama2-training-speed.svg\" alt=\"LLaMA 2 training loss vs GPU-hours spent\" /></p>\n<p>Immediately, at a glance, we notice that the training curves don’t match those\nof LLaMA 1, even when the models are identical.\nAs it turns out, LLaMA 2 was trained on double the context size,\nand a longer cosine schedule, which unfortunately\nhas negatively impacted all model sizes.\nHowever, smaller models have been impacted worse than larger ones.\nAs a result, the 34B model, which in LLaMA 1 remained always better than the 65B\nmodel at any training time spent, now dips slightly above the 70B model,\nbefore overtaking it:</p>\n<p><img src=\"../assets/chinchilla-s-death/llama-training-speed-comparison.webp\" alt=\"LLaMA 1 vs 2 training loss vs GPU-hours spent\" /></p>\n<p>More importantly, comparing the training speeds strongly confirms our suspicions\nfrom LLaMA 1:</p>\n<ol>\n<li>First, they are faster than bigger models,</li>\n<li>Then, they slow down, and are overtaken by larger models (as per\nChinchilla),</li>\n<li>BUT THEN, they enter the near-linear regime, in which smaller models have a\nsteeper descent into superior knowledge, and they overtake larger models\nyet again!</li>\n</ol>\n<p>A fascinating consequence ties into making the right choices\nwhen starting a training run:\ncontrary to popular belief, <strong>larger models yield worse results</strong>.\nIf you had to pick a parameter size and dataset, you might be better off opting\nfor a 7B model and training for 7 epochs on trillions of tokens.</p>\n<p>Look at the near-linear regime of the 7B model, and extrapolate its line to when\nthe 70B model stopped:\nhad the 70B computation been spent on the 7B instead,\nit would potentially have reached a lower perplexity!</p>\n<p>Another thing we notice from LLaMA 2 is that the learning slowdown at the end of\nthe LLaMA 1 curves was indeed an artefact of the cosine schedule.\nThat slowdown is completely absent from the LLaMA 2 training run at the\ncorresponding mark of 1 trillion tokens read.</p>\n<p>In fact, maybe the reason that, at that same mark, the LLaMA 2 7B model has a\nworse quality than the LLaMA 1 7B model had,\nmay be because <em>its cosine schedule is stretched</em>!</p>\n<p>Let’s go back to the Chinchilla paper to argue that point.\nIn appendix A, figure A1, they show an ablation study for various cosine\nschedule parameters (phrased another way:\nvarious ways to stretch the learning rate curve).</p>\n<p><img src=\"../assets/chinchilla-s-death/chinchilla-cosine-ablation-study.png\" alt=\"Chinchilla cosine schedule ablation study\" /></p>\n<p>They make the point that the lowest loss is achieved when the curve is not\nstretched. That is supported by the graphs, but we notice something off.\nAfter reading 6 million tokens, the training loss at the top is below 2.8;\nmeanwhile, at the same mark, the training loss of the bottom model is above.\nYet the only difference between the models is the cosine schedule!\nBecause the bottom model was slated to go through more training data,\nthe “unstretched” cosine schedule was computed for a bigger number of steps,\nwhich effectively stretches it.\nIf the learning rate had instead followed\nthe schedule assigned to fewer training steps,\nit would have had a better loss for the same amount of training time.</p>\n<p>More broadly, that raises a question that I leave open:\nif the cosine schedule is not optimal,\nhow should the shape of its tail be instead?</p>\n<script type=\"application/ld+json\">\n{ \"@context\": \"http://schema.org\",\n  \"@type\": \"BlogPosting\",\n  \"datePublished\": \"2023-07-23T17:35:02Z\",\n  \"keywords\": \"gpu, ml\" }\n</script>\n",
      },
      {
        "id":  "https://espadrine.github.io/blog/posts/recomputing-gpu-performance.html",
        "url": "https://espadrine.github.io/blog/posts/recomputing-gpu-performance.html",
        "title": "Recomputing ML GPU performance: AMD vs. NVIDIA",
        "tags": "gpu ml",
        "date_published": "2023-06-18T21:40:09Z"
        "content_html": "<h1 id=\"Recomputing_ML_GPU_performance_AMD_vs_NVIDIA\">Recomputing ML GPU performance: AMD vs. NVIDIA <a href=\"#Recomputing_ML_GPU_performance_AMD_vs_NVIDIA\" class=\"autolink-clicker\" aria-hidden=\"true\">§</a></h1>\n<p>I am pretty impressed seeing <a href=\"https://twitter.com/LisaSu/status/1669848494637735936\">Lisa Su</a> doing her best to steer the AMD ship towards\nbetter AI support in GPUs, with the <a href=\"https://huggingface.co/blog/huggingface-and-amd\">Huggingface partnership</a> and by convincing\nGeorge Hotz to submit more bug reports.</p>\n<p>(For context, <a href=\"https://geohot.github.io//blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html\">Hotz raised $5M</a> to improve RX 7900 XTX support and sell a $15K\nprebuilt consumer computer that runs 65B-parameter LLMs. A plethora of driver\ncrashes later, he almost <a href=\"https://github.com/RadeonOpenCompute/ROCm/issues/2198#issuecomment-1574383483\">gave up on AMD</a>.)</p>\n<p>There’s quite a few issues to overcome, though.\nWhile that GPU is great\n(<a href=\"https://www.tomshardware.com/news/stable-diffusion-gpu-benchmarks\">Stable Diffusion iteration speed per GPU cost</a> is top-tier),\na cursory study would be flawed:\npublic GPU benchmarks like TechPowerUp, TomsHardware, etc. give:</p>\n<ul>\n<li><strong>RX 7900 XTX:</strong> <a href=\"https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889\">123 TFLOPS</a></li>\n<li><strong>RTX 4090:</strong> <a href=\"https://www.tomshardware.com/reviews/amd-radeon-rx-7900-xtx-and-xt-review-shooting-for-the-top\">82.58 TFLOPS</a></li>\n</ul>\n<p>Where do the figures come from?</p>\n<p>While there is no official breakdown,\nonly <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">official figures</a>, people widely compute it this way:</p>\n<ul>\n<li>For <strong>NVIDIA</strong>:\n<a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0\">Boost Clock (THz) × CUDA Cores × 2</a>\n(since the FMA instruction does two floating-point operations\n(a multiplication and an addition) in 1 CUDA core cycle).</li>\n<li>For <strong>AMD</strong> on RDNA3:\n<a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">Boost Frequency (THz) × Stream processors × 2 (dual issue) × 4 (dot product)</a>,\nas <a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\">RDNA3 has <code>V_DUAL_DOT2ACC_F32_F16</code></a>,\nwhich does two dot products (a×b+c×d+e, 4 operations),\nin 1 processor cycle.</li>\n</ul>\n<table>\n  <tr><th> Name </th><th> Price </th><th> Processors </th><th> Frequency </th><th> TFLOPS (FP16) </th><th> Perf/€ </th>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">RX 7900 XTX</a> </td>\n      <td> €1110 </td><td>  6144 </td><td>  2.5 GHz </td><td> 122.88 </td><td> 0.1107 </td>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xt\">RX 7900 XT</a> </td>\n      <td>  €942 </td><td>  5376 </td><td>  2.4 GHz </td><td> 103.22 </td><td> 0.1096 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0\">RTX 4090</a> </td>\n      <td> €1770 </td><td> 16384 </td><td> 2.52 GHz </td><td>  82.58 </td><td> 0.0467 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3060</a> </td>\n      <td>  €314 </td><td>  3584 </td><td> 1.78 GHz </td><td>  12.76 </td><td> 0.0405 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3080</a> </td>\n      <td>  €905 </td><td>  8704 </td><td> 1.71 GHz </td><td>  29.76 </td><td> 0.0329 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3090</a> </td>\n      <td> €1500 </td><td> 10496 </td><td> 1.70 GHz </td><td>  35.68 </td><td> 0.0238 </td>\n</table>\n<p>That is an unjust comparison, though, because AMD’s instruction is more niche\nthan FMA (hitting this performance sweet spot is thus uncommon),\nand because both of those GPUs have other tricks up their sleeves,\nyielding superior FLOPS.</p>\n<p>The <a href=\"https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#Will_AMD_GPUs_ROCm_ever_catch_up_with_NVIDIA_GPUs_CUDA\">big one</a> on NVIDIA are <a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\">Tensor cores</a>.\nWith them, you can run an instruction that does\n<a href=\"https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf\">a 4×4 to 4×8 matrix multiplication (page 25)</a>\nin 1 cycle within a single Tensor Core (32 CUDA cores).</p>\n<p>2×4^2×8 (matmul ops) ÷ 1 (cycles) = 256 ops/TC/cycle.</p>\n<p>(There is <a href=\"https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/\">some variation between NVIDIA GPUs</a>\non which matrix sizes are supported and on how many cycles the instruction takes,\nand NVIDIA keeps major aspects of their instruction set secret,\nbut on recent 30- and 40-series, this 256 number seems fairly constant.)</p>\n<p><img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/gtcs22/tensor-cores/hopper-tensor-core-ampere-2c50-t.jpg\" alt=\"Official image showing the matrix size for third-generation tensor cores with V100 FP32\" /></p>\n<p>That actually puts the RTX 4090 at\n256 × 512 (Tensor Cores) × 2.52 (GHz)\n÷ 1K (GHz per teracycle/s) = <a href=\"https://en.wikipedia.org/wiki/GeForce_40_series#Desktop\">330 TFLOPS in FP16</a>…\nMuch higher than the 123 TFLOPS that impressed Hotz on the RX 7900 XTX!</p>\n<p>But AMD now has the same trick.\nIn <a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\">RDNA3</a>, with <a href=\"https://gpuopen.com/learn/wmma_on_rdna3/\">WMMA</a>, the RX 7900 XTX has an instruction,\n<a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\"><code>V_WMMA_F16_16X16X16_F16</code></a>\nthat do two 16×16 matrix multiplications in <a href=\"https://github.com/RadeonOpenCompute/amd_matrix_instruction_calculator/blob/339d784e56e55752495192b0781ea162fc32e323/matrix_calculator.py#LL1139C26-L1139C26\">32 cycles</a>,\nin a single Compute Unit (two sets of 32 threads).</p>\n<p>2×16^3 (matmul ops) × 2 ÷ 32 (cycles) = 512 ops/CU/cycle.</p>\n<p>This uses the same underlying silicon circuits as <code>V_DUAL_DOT2ACC_F32_F16</code>:\nthe architecture lays out the matrices in Vector General-Purpose Registers.\nEach cell of the output matrix is computed by multiplying\none row from input matrix A with one column from input matrix B,\ntwo input cells at a time\n(two adjacent input A row cells packed inside the same VGPR,\nand two adjacent input B column cells packed together inside another VGPR),\nso they can be used by the packed dot product single-cycle instruction.\nWithin that same instruction, encoded in VOPQ\n(a SIMD-like system to execute one operation\non an even register while it executes on an odd one at the same time),\nan adjacent output cell also multiplies through its first two input cells\nat the same time using dual issue.</p>\n<p>The input row has size 16, so those two output cells are completed in 8 cycles.\nEach two adjacent output cells in their diagonal\nare computed with 16 parallel threads (on separate stream processors)\nwithin the same 8 cycles.\nWe have done two diagonals (32 output cells); there are 14 diagonals left.\nInside that Compute Unit, we still have 16 stream processors that we can use;\nthey can handle two more output diagonals within the same 8 cycles.</p>\n<p>Once our first four diagonals are computed,\nwe sequentially compute the next 4 diagonals in the next 8 cycles.\nSo forth for the next 4, and the last 4 after that.\nIn total, we have computed the matrix multiplication\nin 32 cycles, which checks out.</p>\n<p>Why can’t we do the matrix multiplication in 16 cycles\nby using all 64 threads inside of the Compute Unit?\n<a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\">Section 7.6 of the instruction set manual</a> indicates:</p>\n<blockquote>\n<p>[Dual issue] is legal only for wave32.</p>\n</blockquote>\n<p>WMMA supports both wave32 and wave64, but it sounds like dual issue is\ndeactivated in wave64, and thus it would still take 32 cycles,\nmaking it an ill-documentedly unfavorable proposition, I believe.</p>\n<p>All in all, using <a href=\"https://gpuopen.com/learn/wmma_on_rdna3/\">WMMA</a>, the RX 7900 XTX can crank through\n512 × <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">96 (Compute Units) × 2.5 (GHz)</a>\n÷ 1K (GHz per teracycle/s) = <a href=\"https://en.wikipedia.org/wiki/RDNA_3#Desktop\">123 TFLOPS in FP16</a>…</p>\n<p>That ends up being less than half the performance of the RTX 4090.\nThe superior number of operations per Compute Unit is offset by the\ncrushingly lower number of cores.\nPerhaps the AMD strategy is to have the better circuit ready\nbefore migrating to the TSMC N5 (“5 nm”) process at a less affordable price.</p>\n<p>In practice, the lower performance is less of an issue for AI training,\nbecause they are famously limited in the amount of parallelization opportunities\n(even the best training runs typically incur only 50% GPU use at a given time).\nThe VRAM bandwidth then matters a lot for large models,\nand the <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">RX 7900 XTX</a>, despite using GDDR6 instead of GDDR6X,\nhas a higher bandwidth than the RTX 3090, thanks to its faster memory clock.\nStill, it also is lower than the RTX 4090 on that front\n(but at a lower price point).</p>\n<table>\n  <tr><th> Name </th><th> Price </th><th> TFLOPS (FP16) </th><th> Memory bandwidth (GB/s)</th><th> Value (TFLOPS·GB/s/€) </th>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0\">RTX 4090</a> </td>\n      <td> €1770 </td><td> 330 </td><td> 1008 </td><td> 188 </td>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">RX 7900 XTX</a> </td>\n      <td> €1110 </td><td> 123 </td><td> 960 </td><td> 106 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3080</a> </td>\n      <td>  €905 </td><td> 119 </td><td> 760 </td><td> 100 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3090</a> </td>\n      <td> €1500 </td><td> 143 </td><td> 936 </td><td> 89 </td>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xt\">RX 7900 XT</a> </td>\n      <td>  €942 </td><td> 103 </td><td> 800 </td><td> 87 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3060</a> </td>\n      <td>  €314 </td><td> 51 </td><td> 360 </td><td> 58 </td>\n</table>\n<p>Thus the RX 7900 XTX is not technically the best TFLOPS per price,\nas was presumed in Hotz’s <a href=\"https://geohot.github.io//blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html\">raise announcement</a>.\nBut that metric is not crucial for the purpose of making LLM machines,\nand purely looking at hardware, that GPU is a fine choice for that,\nin part because it has a fairer RAM per dollar offer,\nso that it can hold a large model without needing pricier GPUS,\nyet likely reaching reasonable inference speeds.</p>\n<p>The other thorns on the side of AMD in AI, though, rear their ugly heads:</p>\n<ul>\n<li><a href=\"https://chipsandcheese.com/2023/01/07/microbenchmarking-amds-rdna-3-graphics-architecture/\">The compilers don’t produce great instructions</a>;</li>\n<li>The drivers crash frequently: ML workloads feel experimental;</li>\n<li>Software adoption is getting there,\nbut kernels are less optimized within frameworks,\nin particular because of the fracture between ROCm and CUDA.\nWhen you are a developer and you need to write code twice,\none version won’t be as good, and it is the one with less adoption;</li>\n<li>StackOverflow mindshare is lesser. Debugging problems is thus harder,\nas fewer people have encountered them.</li>\n</ul>\n<p>(I will note, however, that the wealth of information provided by AMD\noutshines that from NVIDIA tremendously,\neven though they could better vulgarize those subtleties and\nexplain how to perform specific workloads like BERT training,\ninto which NVIDIA puts welcome care.\nJust contrast <a href=\"https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html\">NVIDIA’s matmul page</a> to <a href=\"https://gpuopen.com/learn/wmma_on_rdna3/\">AMD’s</a>.\n<a href=\"https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html\">AMD doesn’t even recognize its own flagship GPUs as supported for ROCm</a>,\nwhich is mindboggling coming from NVIDIA’s superior CUDA support.)</p>\n<hr />\n<p><a href=\"https://www.reddit.com/r/espadrine/comments/156bbmj/recomputing_gpu_performance/\">Comments on Reddit</a>.</p>\n<script type=\"application/ld+json\">\n{ \"@context\": \"http://schema.org\",\n  \"@type\": \"BlogPosting\",\n  \"datePublished\": \"2023-06-18T21:40:09Z\",\n  \"keywords\": \"gpu, ml\" }\n</script>\n",
      }
  ]
}
