{
  "version": "https://jsonfeed.org/version/1",
  "title": "Espadrine’s blog",
  "description": "Let’s talk about whatever I learn!",
  "home_page_url": "https://espadrine.github.io/blog/",
  "feed_url": "https://espadrine.github.io/blog/feed.json",
  "author": {
    "name": "Thaddée Tyl",
    "url": "https://github.com/espadrine"
  },
  "favicon": "https://avatars.githubusercontent.com/u/100689?s=64",
  "items": [
      {
        "id":  "https://espadrine.github.io/blog/posts/a-primer-on-randomness.html",
        "url": "https://espadrine.github.io/blog/posts/a-primer-on-randomness.html",
        "title": "A Primer On Randomness",
        "tags": "prng crypto",
        "date_published": "2020-03-27T15:17:57Z"
        "content_html": "<h1>A Primer On Randomness</h1>\n<p>Last October, during a one-week hiking holiday in the birthplace of alpinism,\nI got particularly interested in random generators.</p>\n<p>Four reasons why they are fascinating:</p>\n<ol>\n<li>It is only once you track it that you realize just in which gargatuan proportions you <strong>exude information</strong>. Even tiny systems that encode very little data and whose entire purpose is to never leak it (ie, random generators), do so in ways that can be measured, and even exploited. In every instant of your life, during every interaction with someone, billions of muscle movements, tiny and large, only occur because of past events burnt into your brain’s circuits, and betray this private history. Given enough of it, an aggregator could rewind the world and extract minute details from the past.</li>\n<li>All of <strong>symmetric cryptography</strong> completely hinges on randomness. Security proofs fully rely on the analysis of how little information you can extract from a stream, which requires the stream to effectively look random.</li>\n<li>Studying them, and trying your hand at making them, helps you understand the <strong>scientific method</strong> better. Most real-world principles can never be proved with absolute certainty; you need to accurately detect a signal in the noise, and measure the likelihood that this signal is not just you seeing patterns in the static.</li>\n<li>Finally, it helps both understand <strong>the virtue of mixing</strong>, and how best to stir. The effect of mixing is exponential, which is unnatural to mentally harness. On the plus side, when done well, you get fluid exchange of information, remix, and cultural explosion. On the minus side, you get COVID-19 everywhere. Striking the right balance gets you far: many optimizing algorithms rely on it such as genetic algorithms, stochastic gradient descent, or cross-validation sampling in machine learning, which each are heavy users of pseudo-random sources. The results speak for themselves: AlphaGo, for instance, beat the best human player at one of the hardest games on Earth, using Monte-Carlo Tree Search. Yes, you guessed it, they call it Monte Carlo for a reason.</li>\n</ol>\n<h2>Information Theory</h2>\n<p>A good Pseudo-Random Number Generator (or PRNG for short) is indistinguishable from a true random output.</p>\n<p><em>So, where do we get this true random output you speak of?</em></p>\n<p>True randomness has statistical meaning, but it is impossible to prove or disprove.\nYou can only have a high confidence.</p>\n<p>You might hope that true randomness can be extracted from nature, but that is also not true.\nThe physical realm contains a large quantity of data storage (“space”),\nand laws that alter it: gravity, electromagnetism, …\nNature is a state transition function and an output; that is also the structure of a PRNG.</p>\n<p>Physical processes that claim to output “true” randomness rely on the large amount of information stored in the environment, and that environment’s diffuse state scrambling, that is presumably extremely hard for an attacker to detect.</p>\n<p>For instance, the fine trajectory of electrons attracted from atom to atom through an electrical circuit causing minuscule delays, or the chaotic motion of gaseous atoms, or stronger yet, quantum behavior of particles.</p>\n<p>Some physicists may argue that the world is not fully deterministic.\nHowever, the Copenhagen Interpretation or Multiverse fans\ncannot disprove the possibility of a non-local world that complies with the Bell-EPR paradox,\nfor instance through superdeterminism or pilot waves.\n(Sorry for those that don’t care about quantum mechanics;\nyou don’t need to understand this paragraph to carry on.)</p>\n<p>Since true randomness is not real, how do we get close?</p>\n<p>Let’s say that you generate bits. If all the bits were <code>1</code>, it would be pretty predictable, right?\nSo the frequency of ones should converge to one out of two, which is what probability half is.</p>\n<p>But if the output was a one followed by a zero continuously (<code>101010…</code>), it would be predictable too!\nSo the frequency of the sequence <code>10</code> in the output should converge to one out of four.</p>\n<p>More generally, every possible sequence of <code>n</code> bits should appear with a frequency converging to <code>1÷2ⁿ</code>.</p>\n<p>(A common romanticization of that idea is the comment that the decimals of π encode the entire works of Shakespeare.\nπ being irrational, its formulation is <a href=\"https://mathworld.wolfram.com/WeylsCriterion.html\">orthogonal to any fractional representation</a>, which is what decimals are.\nThat gives strong credence to the conjecture that its digits form a truly random sequence.)</p>\n<p>That idea might make you uneasy. After all, it gives an impossible requirement on the memory size of a generator.</p>\n<h3>Memory</h3>\n<p>If your state contains <code>i</code> bits, what is the largest sequence of consecutive ones it can output?</p>\n<p>Well, since the PRNG is deterministic, a given state will always yield the same output.\nThere are <code>2ⁱ</code> possible state configurations, so with this entropy, you can at best output <code>i·2ⁱ</code> bits\nbefore you arrive at a previous state and start repeating the same output sequence again and again.</p>\n<p>At least, with an ideal PRNG, you know that one given configuration will output a sequence of <code>i</code> ones.\nThe previous configuration (which transitioned to the configuration that outputs the <code>i</code> ones)\ncannot also output a sequence of <code>i</code> ones:\nif two configurations yielded the same output, then there would be some <code>i</code>-bit output that no configuration produced.\nThat would not be an ideal PRNG.</p>\n<p>So let’s say that the previous configuration gives <code>i-1</code> ones (a zero followed by a ton of ones),\nand that the next configuration gives <code>i-1</code> ones (a ton of ones followed by a zero).\nThat is a total of a maximum of <code>3×i-2</code> consecutive ones.</p>\n<p>Thus, you cannot get <code>3×i-1</code> consecutive ones…\nwhich a true random generator would output with a frequency of <code>1 ÷ 2^(3×i-1)</code>.\nA statistical deviation that you can detect to disprove that a generator is truly random!</p>\n<p>Conversely, it means that <em>true generators require infinite memory</em>, which is impossible in the real world.</p>\n<p>(By the way, yes, it does seem like computing all the digits of π requires infinite memory.\nAll current algorithms need more memory the more digits are output.)</p>\n<p>In practice, you get around the issue by picking a state size <code>i</code> large enough that\ndetecting this statistical anomaly requires a millenia’s worth of random output, too much for anyone to compute.</p>\n<h3>Cycle Analysis</h3>\n<p>So, once we have picked a state size, now we have an upper bound for the period of the PRNG:\nit will repeat the same sequence at least every <code>2ⁱ</code> bits.</p>\n<p>But of course, your mileage may vary. An imperfect generator might have a much lower period.\nUnless you have a mathematical proof for a <strong>lower bound</strong>, maybe your family of generators\nhas a seed (an initialization parameter) which results in the same output being repeated over and over…\nThat is called a fixed point.</p>\n<p>Even if there are no fixed point, there could be a large number of seeds that start repeating soon!\n(That was a real <a href=\"https://www.cs.cornell.edu/people/egs/615/rc4_ksaproc.pdf\">vulnerability in the RC4 cipher</a>, by the way.)</p>\n<p>On the plus side, there is a counterintuitive phenomenon that develops\nwhen a set of links randomly connect with each other in closed chains.\nMost links end up on long chains.\nFor instance, with two links, they will be connected in a chain half the time;\nwith three links, each link will be connected to another link with probability ⅔; etc.</p>\n<p>Better yet, if you increase the number of links linearly,\nyou decrease the proportion of links that are part of small chains exponentially.</p>\n<p>The bottom line is this: you can always put lipstick on the pig by increasing the state size,\nand your generator will look good.</p>\n<p>However, a fundamentally better generator would have become even better yet with an increased state size.</p>\n<h3>Reversibility</h3>\n<p>If you build out the design at random, a danger lingers.\nUnless you are careful, you might build an irreversible generator.\nGiven a state after a generation, can you mathematically compute the previous state?</p>\n<p>If you can’t, then there are multiple initial states that can transition to the current state.\nThat means some states can never happen, because there are no initial state that transitions to them;\nthey got stolen by the states with multiple previous states pointing to it!</p>\n<p>That is bad. Why?</p>\n<p>First, it reduces the potency of your state size (since a percentage of possible states are dead).</p>\n<p>Second, many seeds merge into the rail tracks of other seeds,\nconverging to a reduced set of possible streams and outputting the same values!</p>\n<p>If you build a <strong>reversible</strong> algorithm, at least each possible state is useful,\nand inter-seed correlation is less probable.</p>\n<p>Note that a reversible design does not mean that the state cycles through all possible combinations.\nIt just means that each state points to exactly one other state, and has exactly one state leading to it.\nIn other words, it is a <em>bijection</em>, but not a <em>circular permutation</em>.</p>\n<h3>Diffusion</h3>\n<p>Claude Shannon made <a href=\"https://www.iacr.org/museum/shannon/shannon45.pdf\">a very good point the other day</a> (I think it was in 1945?) about ciphers.\nAn ideal pseudo-random source is such that any bit of the input flips half the bits of the output.</p>\n<p>More precisely, ideally, the probability that any bit of the stream flips if a given bit of the state flips, should be ½.\nThat is called <strong>diffusion</strong> of the state.</p>\n<p>After all, if it wasn’t ½, I could start making good guesses about whether this bit of the state is set,\nand slowly recover pieces of the state or even the key.\nAnd suddenly, I can predict the whole stream.</p>\n<p>A related concept is <strong>confusion</strong> of the key.\nIdeally, each bit of the output depends equally on a combination of all bits of the key.\nSo, each bit of the key should change each bit of the stream,\nfor half of the set of possible configurations of the key’s other bits.</p>\n<p>Each bit of the stream should therefore be a complex combination of all of the key’s bits,\nwhile each bit of the key should have an impact stretched along the whole stream.</p>\n<p>These properties particularly matter for cryptographic primitives such as ChaCha20,\nwhere the seed of the PRNG is essentially the cipher key.\nTheir analysis and understanding still matter for PRNG quality;\nalthough some designs don’t take confusion seriously,\nleading to severe correlation of distinct seeds.</p>\n<h2>Tooling</h2>\n<p>Back in the seventies, there was no tooling to pragmatically study the quality of a generator.\nThat made the PRNG hobby somewhat impractical.</p>\n<p>As a sad result, some people produced subpar results, such as IBM’s infamous <a href=\"https://en.wikipedia.org/wiki/RANDU\">RANDU</a>:</p>\n<blockquote>\n<p>It fails the spectral test badly for dimensions greater than 2, and every integer result is odd.</p>\n</blockquote>\n<p>Fortunately, great strides were made since.\nAnyone can get going quickly, up until they start having competitive results.</p>\n<h3>History</h3>\n<p>A first step was Donald Knuth’s description of the use of <strong>Chi-Squared tests</strong> in 1969.</p>\n<p>While its application to generators was described in Knuth’s seminal work\n<em>The Art of Computer Programming</em>, we have to thank Karl Pearson for the concept.</p>\n<p>As the story goes, Pearson was disgruntled at scientists estimating all their results\nbased on the assumption that their statistical distributions were always normal,\nwhen in some cases they very clearly were not. They just didn’t really have any other tool.</p>\n<p>So he worked through the theory. Say you make a claim that some value, for which you have samples,\nfollows a given statistical distribution. (A uniform one perhaps? Like our PRNG outputs?)\nCall that “<strong>the Null Hypothesis</strong>”, because it sounds cool.</p>\n<p>Your evidence is a set of samples that belong in various categories.\nYour null hypothesis is the belief that each category <code>i ∈ {1,…,k}</code> appears with probability <code>pᵢ</code>.\nMaybe the two classes are 0 and 1; maybe they are the 256 possible bytes.</p>\n<p>There are <code>oᵢ</code> <em>observed</em> samples in category <code>i</code>.\nThe theoretical, <em>expected</em> number of samples should be <code>eᵢ</code> = <code>n·pᵢ</code>.\nYou compute the <strong>Chi-Squared statistic</strong>: <code>χ²</code> = <code>Σ (eᵢ - oᵢ)² ÷ eᵢ</code>.</p>\n<p>That statistic follows a distribution of probabilities,\ndepending on the degrees of freedom of the problem at hand.\nIf we are looking at random bytes, each generation must be one of 256 possible outputs:\nso there are 255 degrees of freedom.\n(If it is not in the first 255, it must be in the last, so the last one is not a degree of freedom.)</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/35/Chi-square_pdf.svg\" alt=\"Chi-Squared probability density\" /></p>\n<p>Each possible value of <code>χ²</code> you get has a probability of being valid for your null hypothesis.\nOne value is the most probable one. The further you get from it, the least likely it is that your samples are random.</p>\n<p>But by how much?</p>\n<p>You want to know the probability that a true random generator’s <code>χ²</code> lands\nas far from the ideal value as your pseudo-random generator did.\n(After all, even a perfect generator rarely precisely lands on the most probable <code>χ²</code>,\nwhich for random bytes is 253 with probability 1.8%.)</p>\n<p>You can compute the probability that a true random generator’s <code>χ²</code> is bigger (more extreme) than yours.\nThat probability is called a <strong>p-value</strong>.\nIf it is tiny, then it is improbable that a true random generator would get this value;\nand so, it is improbable that what you have is one.</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8e/Chi-square_distributionCDF-English.png\" alt=\"Chi-Squared distribution\" /></p>\n<p>With this tool in hand, you can easily check that a process that pretends to be random is not actually so.</p>\n<p>Or, as <a href=\"http://www.economics.soton.ac.uk/staff/aldrich/1900.pdf\">Pearson puts it</a>:</p>\n<blockquote>\n<p>From this it will be more than ever evident how little chance had to do\nwith the results of the Monte Carlo roulette in July 1892.</p>\n</blockquote>\n<p>(Not sure why his academic paper suddenly becomes so specific;\nmaybe he had a gambling problem on top of being a well-known racist.)</p>\n<p>Fun sidenote: if you look at the <code>χ²</code> formula, notice that if your observed values all hit their expectations,\nyou will always end up with a <code>χ²</code> equal to zero, whose p-value is 1.</p>\n<p>Uniform random numbers have this awesome property that their p-values should also be uniformly random,\nand the p-values of the p-values too, and so on.</p>\n<p>The p-value you want is simply one that is not too extreme (eg, higher than 10¯⁵, lower than 1-10¯⁵).\nA p-value of 1 immediately disqualifies your null hypothesis!\nPerfect fits are not random; you must have anomalies some of the time.</p>\n<p>Let’s get back to Donald Knuth. His advice of using this tool to study pseudo-random efforts defined all subsequent work.</p>\n<p>In 1996, another PRNG fellow, George Marsaglia, looked at the state of tooling with discontent.\nSure, those Chi-Squared tests were neat.\nBut writing them by hand was tedious.</p>\n<p>Worse, nothing defined what to observe. Bytes are one thing, but they only detect byte-wise bias.\nWhat about bitwise? What if we count bits, and compare that count to a <em>Known Statistic</em> (<strong>bit counting</strong>)?\nWhat if we count the number of successive times one byte is bigger than the one generated just before (<strong>runs test</strong>)?\nOr maybe count the number of outputs between the appearance of the same value (<strong>gap test</strong>)?\nOr take a random matrix, compute its rank, verify that it validates the <em>Known Statistic</em> (<strong>binary rank</strong>)?</p>\n<p>Well, he didn’t think about all those tests,\nbut he did publish a software package that automatically computed p-values\nfor a dozen of tests. He called it <em>DIEHARD</em>.</p>\n<p>Some are like the ones I described, some are a bit wilder and somewhat redundant,\nsome have a bit too many false positives to be relied upon.</p>\n<p>But it was the start of automation!</p>\n<p>And the start of the systematic extermination of the weak generators.</p>\n<p>In 2003, Robert G. Brown extended it with an easy-to-use command-line interface, <em><a href=\"https://webhome.phy.duke.edu/~rgb/General/dieharder.php\">Dieharder</a></em>,\nthat allowed testing without having to fiddle with compilation options, just by piping data to a program.\nHe aggregated a few tests from elsewhere, such as the NIST’s STS\n(which are surprisingly weak for their cryptographic purpose… Those were simpler times.)</p>\n<p>A big jump in quality came about in 2007.\nPierre L’Écuyer &amp; Richard Simard published <em><a href=\"http://simul.iro.umontreal.ca/testu01/tu01.html\">TestU01</a></em>, a test suite consisting of three bars to clear.</p>\n<ul>\n<li>SmallCrush picks 10 smart tests that killed a number of weak generators in 30 seconds.</li>\n<li>Crush was a very intensive set of 96 tests that killed even more weaklings, but it took 1h to do so.</li>\n<li>BigCrush was the real monster. In 8 hours, its set of 106 tests brutalizes 8 TB of output, betraying subtler biases never before uncovered, even in many previously-beloved PRNGs, such as the still-popular Mersenne Twister. A very sobering moment.</li>\n</ul>\n<p>TestU01 installed two fresh ideas: having multiple levels of intensity, and parameterizing each test.\nThe latter in particular really helped to weed out bad generators.\nMaybe if you look at all the bits, they look fine, but if you look at every eigth bit, maybe not so much?</p>\n<p>The feel of using the programs was still similar, though: you ran the battery of tests,\nyou waited eight hours, and at the end, you were shown the list of all tests whose p-value was too extreme.</p>\n<p>Thence came the current nec-plus-ultra: Chris Doty-Humphrey’s <em>Practically Random</em>,\naffectionately called <a href=\"http://pracrand.sourceforge.net/\">PractRand</a>, published in 2010.</p>\n<p>It was a step up still from TestU01:</p>\n<ul>\n<li>Instead of eating one output for one test and throwing it away, it uses output for multiple tests, and even overlaps the same test families along the stream, maximizing the extraction of statistics from each bit of output.</li>\n<li>It took the concept of levels of intensity to a new level. The program technically never stops; it continuously eats more random data until it finds an unforgivable p-value. On paper, it is guaranteed to find one, at least once it reaches the PRNG’s cycle length; but that assumes you have enough memory for it to store its statistics. In practice, you can go very far: for instance, the author’s own sfc16 design reached flaws after 512 TiB — which took FOUR MONTHS to reach!</li>\n<li>It displays results exponentially. For instance, once at 1 MB of random data read, then at 2, then at 4, then at 8, … Every time, it either tells you that there are no anomalies, or the list of tests with their bad p-values.</li>\n</ul>\n<p><em>(A small note: don’t expect this tooling to be satisfactory for anything cryptographic.\nTheir study relies on much more advanced tooling and analysis pertaining to diffusion,\ndifferential cryptanalysis, algebraic and integral attacks.)</em></p>\n<p>I am a big believer in tooling.\nI believe it is THE great accelerator of civilization by excellence.\nThe step that makes us go from running at 30 km/h, to speeding at 130 km/h, to rocketing at 30 Mm/h.\nIn fact, by the end of this series of posts, I hope to publish one more tool to add to the belt.</p>\n<h3>Hands-On</h3>\n<p>I don’t actually recommend you start out with PractRand for the following reasons:</p>\n<ul>\n<li>You might make silly mistakes. PractRand can kill generators that looked OK in the 80s fairly instantly. You won’t know if your design didn’t even stand a chance back then, or if it was competitive.</li>\n<li>You might have a coding bug. It would be too bad if you threw away a good starting design just because a mask had the wrong bit flipped.</li>\n<li>Seeing Chi-Square failures helps understand the beginner design space. Yes, you want the output to have high entropy; but while it is obvious that you don’t want a poorly balanced output (eg. one possible sequence appears too often), you also don’t want a highly structured output (eg. all possible sequences appear exactly as often), since random noise must contain anomalies. Seeing a high-entropy generator fail because bytes were slightly too equiprobable helped me appreciate what was undesirable. It is often counter-intuitive, so these beginner lessons help a lot.</li>\n</ul>\n<p>I would encourage you to build a silly idea, then pipe 10 MB to <a href=\"https://www.fourmilab.ch/random/\">ent</a>.\nCheck the entropy calculation (it should be somewhere around 7.9999),\nand verify that the Chi-Square p-value is between 0.1% and 99.9% with a set of seeds.</p>\n<p>Compare it to a good randomness source: <code>&lt;/dev/urandom head -c 10M | ent</code>.\n(When I say good, I mean ChaCha20, which is what Linux uses.)</p>\n<p>See what happens when you go from 10M to 100M: does the p-value always decrease, or always increase?\nThat would be bad, very bad indeed.</p>\n<p>Once your Chi-Squared is good, skip all the old tests, and hop into PractRand: <code>./prng | RNG_test stdin64</code>.\nI recommend specifying the size of your output, so that PractRand can know what to look out for.</p>\n<p>Then, goes the contest.</p>\n<p>If you pass 1 MiB: you have beat the sadly very widely-used <a href=\"http://man7.org/linux/man-pages/man3/drand48.3.html\">drand48</a>! (Java, C, …)</p>\n<p>If you pass 256 GiB: you are now better than the widely-used <a href=\"http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html\">Mersenne Twister</a>! (Ruby, Python, …)</p>\n<p>If you pass 1 TiB: congratulations, you beat the famous <a href=\"https://cypherpunks.venona.com/archive/1994/09/msg00304.html\">RC4</a> stream cipher!\n(Used as macOS’s old arc4random source, and actually most websites used it for TLS at some point…)</p>\n<p>If you pass 32 TiB: you have won. The <code>RNG_test</code> program automatically stops.\nBeware: it takes about a week to compute… when your generator is fast.</p>\n<p>Quick advice: remember that p-values should be uniformly random.\nIt is inevitable to have some of them be labeled “unusual”, or even, more rarely, “suspicious”.\nIt does not mean you failed.</p>\n<p>When the p-value is too extreme, PractRand will show “FAIL!” with a number of exclamation marks proportional to how horrified it is.\nThen, the program will stop immediately.</p>\n<p>Some tests will fail progressively.\nIf the same test shows “unusual” at 4 GiB, and “suspicious” at 8 GiB,\nit will probably fail at 16 GiB.</p>\n<h3>Speed</h3>\n<p>Once you beat 32 TiB of PractRand, you know your generator is good —\nbut to be useful, it also must be the fastest in its class.</p>\n<p>A few notes can really help you get it up to speed.</p>\n<p>First, you want to be as close to the metal as you can. Code in C, C++, or Rust.</p>\n<p>Second, understand the assembly output. Looking at the compiled assembly with <code>gcc prng.c -S -o prng.asm</code> can help.\nI recommend <a href=\"https://software.intel.com/en-us/articles/introduction-to-x64-assembly\">Intel’s introduction</a>, <a href=\"https://www.amd.com/system/files/TechDocs/24592.pdf\">AMD’s manual</a> and <a href=\"https://www.agner.org/optimize/instruction_tables.pdf\">Agner’s instruction tables</a>.</p>\n<p>In particular, a number of amd64 opcodes are inaccessible from the programming language.\nYou can access them in various ways:</p>\n<ul>\n<li>The compiler will smartly use them when they apply. For instance, there is an opcode to rotate the bits of a variable leftward: <code>ROL</code>. But all the C programming language offers is shift (<code>&gt;&gt;</code> for <code>SHR</code>, <code>&lt;&lt;</code> for <code>SHL</code>). However, the compiler will map <code>(a &lt;&lt; 1) | (a &gt;&gt; 63)</code> to the 64-bit <code>ROL</code>.</li>\n<li>Compilers usually include header files or libraries to access those instructions, by exporting functions that compile down to the corresponding instruction. Those are called <strong><a href=\"https://software.intel.com/sites/landingpage/IntrinsicsGuide/\">intrinsics</a></strong>. For instance, our friend the 64-bit <code>ROL</code> appears as <code>_rotl64(a, 1)</code>, if you <code>#include &lt;immintrin.h&gt;</code>.</li>\n<li>SIMD operations heavily depend on your mastery of the compiler. You can either access them through assembly, compiler flags, or intrinsics (my favorite).</li>\n</ul>\n<p>Third, understand the way <a href=\"https://www.agner.org/optimize/microarchitecture.pdf\">the CPU processes the assembly</a>.</p>\n<ul>\n<li><strong><a href=\"https://software.intel.com/en-us/blogs/2011/11/22/pipeline-speak-learning-more-about-intel-microarchitecture-codename-sandy-bridge\">Instruction pipelining</a></strong>: Every instruction executed goes through a number of phases:<br />\n① the instruction is decoded from memory and cut in micro-operations (μops);<br />\n② each μop is assigned internal input and output registers;<br />\n③ the μop reads input registers;<br />\n④ it is executed;<br />\n⑤ it writes to the output register; and finally<br />\n⑥ the output register is written to the target register or memory.<br />\nEach of those stages start processing the next instruction as soon as they are done with the previous one, without waiting for the previous instruction to have cleared all steps. As a result, a good number of instructions are being processed at the same time, each being in a different stage of processing.<br />\n<em>Example gain: successive instructions go faster if each stage of the second one does not depend on the first one’s later stages.</em></li>\n<li><strong>Superscalar execution</strong>: Each μop can be executed by one of multiple execution units; two μops can be executed by two execution units in parallel as long as they don’t have inter-dependencies. There might be one execution unit with logic, arithmetic, float division, and branches; one execution unit with logic, arithmetic, integer and float multiplication; two with memory loads; one with memory stores; one with logic, arithmetic, SIMD permutations, and jumps. Each have a different combination of capabilities.<br />\n<em>Example gain: adding a second instruction doing the same thing, or something belonging to another unit, may not add latency if it acts on independent data.</em></li>\n<li><strong>Out-of-order execution</strong>: Actually, after the μop is assigned internal registers, it is queued in a ReOrder Buffer (ROB) which can store about a hundred. As soon as a μop’s input registers are ready (typically because of a read/write constraint: another μop wrote the information that this μop needs to read), it gets processed by the first execution unit that can process it and is idle. As a consequence, the CPU can process instructions 2, 3, etc. while instruction 1 waits on a read/write dependency, as long as the next instructions don’t have read/write dependencies with stalled instructions.<br />\n<em>Example gain: you can put fast instructions after a slow (or stalled) instruction without latency cost, if they don’t depend on the slow instruction’s output.</em></li>\n<li><strong>Speculative execution</strong>: When there is a branch (eg. an if condition), it would be awful if the whole out-of-order instruction pipeline had to stop until the branch opcode gave its boolean output. So the CPU doesn’t wait to know if the branch is taken: it starts processing the instructions that come after the branch opcode. Once it gets the branch opcode output, it tracks all μops that wrongly executed, and reverts all their work, rewrites the registers, etc.</li>\n<li><strong>Branch prediction</strong>: To get the best out of speculative execution, CPUs make guesses as to what the boolean output of a branch is going to be. It starts executing the instructions it believes will occur.<br />\n<em>Example gain: make your branches nearly always take the same path. It will minimize branch mispredictions, which avoids all the reverting work.</em></li>\n</ul>\n<p>Finally, beware of the way you test performance. A few tips:</p>\n<ol>\n<li>Use the <code>RDTSC</code> CPU opcode to count cycles, as below.</li>\n<li>Disable CPU frequency variability. CPUs nowadays have things like Turbo Boost that change your frequency based on how hot your processor gets and other factors. You want your CPU to have a fixed frequency for the whole process.</li>\n<li>Have as few other processes running as possible. If a process runs in the background, eating CPU, it will affect the results.</li>\n</ol>\n<pre><code>#include &lt;x86intrin.h&gt;\n\nint main() {\n  __int64_t start = _rdtsc();\n  generate_one_gigabyte();\n  __int64_t cycles = _rdtsc() - start;\n  fprintf(stderr, &quot;%f cpb\\n&quot;, ((double)cycles) / 1073741824);\n}\n</code></pre>\n<h3>Designs</h3>\n<p>The earliest design is the <strong>LCG</strong> (Linear Congruent Generator).\nYou can recognize its dirt-simple state transition (a constant addition or multiplication),\nwhich has neat consequences on the analysis of its cycle length (typically 2^statesize).\nUsually, the output is treated with a shift or rotation before delivery.\nWhile they look fairly random, they can have severe issues, such as hyperplane alignment.\nThey also tend to be easy to predict once you reverse-engineer them,\nwhich is why they are not used for anything remotely in need of security.</p>\n<p>Examples of LCG abound: <a href=\"http://man7.org/linux/man-pages/man3/drand48.3.html\">drand48</a>, <a href=\"https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/\">Lehmer128</a>, <a href=\"https://www.pcg-random.org/\">PCG</a>, …</p>\n<p>Then come <strong>Shufflers</strong> (eg. <a href=\"https://cypherpunks.venona.com/archive/1994/09/msg00304.html\">RC4</a>, <a href=\"http://burtleburtle.net/bob/rand/isaacafa.html\">ISAAC</a>, <a href=\"http://pracrand.sourceforge.net/RNG_engines.txt\">EFIIX</a>).\nUsually have an “I” in the name (standing for “indirection”).\nThey try to get randomness by shuffling a list, and they shuffle the list from the randomness they find.\nDo not recommend. It is so easy for bias to seep through and combine destructively.\nBesides, weeding out bad seeds is often necessary.</p>\n<p><strong>Mixers</strong> rely on a simple transition function,\nusually addition to what is sometimes called a “gamma” or “<a href=\"https://mathworld.wolfram.com/WeylsCriterion.html\">Weyl coefficient</a>”.\nA common non-cryptographic pattern is a state multiplication, just like in LCG,\nand the output is XORed with a shifted or rotated version of itself before delivery.\nThe second step is basically a hash.\n(To the security-minded readers: I am not talking about collision-resistant compression functions.)\nIn cryptography, usually, the mixer uses some ARX combination for bit diffusion (ARX = Add, Rotate, XOR),\nand is scheduled in multiple rounds (which are basically skipping outputs).\nExamples include <a href=\"https://github.com/wangyi-fudan/wyhash\">wyrand</a>, <a href=\"http://gee.cs.oswego.edu/dl/papers/oopsla14.pdf\">SplitMix</a>, <a href=\"http://vigna.di.unimi.it/ftp/papers/xorshiftplus.pdf\">Xorshift128+</a>, <a href=\"https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.197.pdf\">AES-CTR</a>, and the beloved <a href=\"https://cr.yp.to/chacha/chacha-20080128.pdf\">ChaCha20</a>.</p>\n<p>Finally, the most haphazard of them: <strong>chaotic generators</strong>.\nThey typically have no minimal cycle length, and they just try to stir things up in the state.\nFor instance, <a href=\"https://burtleburtle.net/bob/rand/smallprng.html\">jsf</a> and <a href=\"http://www.romu-random.org/\">Romu</a>.</p>\n<h2>Parting Fun Facts</h2>\n<p>I mentionned ChaCha20 a lot, because it is one of my favorite cryptographic primitives.\nI’ll give you a few fun facts about it, as goodbye.</p>\n<ol>\n<li>ChaCha20 <a href=\"https://cr.yp.to/snuffle/salsafamily-20071225.pdf\">initializes its state</a> with the ASCII for “expand 32-byte k”. It’s a wink on the purpose of the cipher: it takes a 256-bit key, and expands it to a large random stream.</li>\n<li>It is based on the design of <a href=\"https://cr.yp.to/export/1996/0726-bernstein.txt\">a joke cipher that plays on a US law</a> cataloguing encryption as munition, except if it is a hash. He built it as a simple construction on top of a carefully-constructed hash. Calling the core construction a hash caused him trouble later as <a href=\"https://cr.yp.to/snuffle/reoncore-20080224.pdf\">reviewers misunderstood it</a>.</li>\n<li>The initial name of that cipher was Snuffle. (Yes.)</li>\n</ol>\n<script type=\"application/ld+json\">\n{ \"@context\": \"http://schema.org\",\n  \"@type\": \"BlogPosting\",\n  \"datePublished\": \"2020-03-27T15:17:57Z\",\n  \"keywords\": \"prng, crypto\" }\n</script>\n",
      },
      {
        "id":  "https://espadrine.github.io/blog/posts/two-postgresql-sequence-misconceptions.html",
        "url": "https://espadrine.github.io/blog/posts/two-postgresql-sequence-misconceptions.html",
        "title": "Two PostgreSQL Sequence Misconceptions",
        "tags": "sql",
        "date_published": "2019-09-05T17:28:59Z"
        "content_html": "<h1>Two PostgreSQL Sequence Misconceptions</h1>\n<p>✨ <em>With Examples!</em> ✨</p>\n<p>Some constructs seem more powerful than the promises they make.</p>\n<p>PostgreSQL sequences are like that. Many assume it offers stronger properties\nthan it can deliver.</p>\n<p>They trust them to be the grail of SQL ordering, the one-size-fits-all of strict\nserializability. However, there is a good reason Amazon spent design time on\nvector clocks in <a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Dynamo</a>, Google invested significantly into <a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf\">Chubby</a>, then\n<a href=\"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf\">Percolator</a>’s timestamp oracle, then <a href=\"https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf\">Spanner</a>’s expensive,\natomic-clock-based TrueTime; why Twitter built <a href=\"https://developer.twitter.com/en/docs/basics/twitter-ids.html\">Snowflake</a>, and so many others\nbuilt custom timestamp systems.</p>\n<ol>\n<li>Strict serializability is hard to achieve, especially in a distributed\nsystem, but even in a centralized system with the possibility of failure.</li>\n<li>Developers assume the system is strict-serializable, but it usually is not.</li>\n<li>When a system provides timestamps, developers will use those as if they were\nmonotonically strictly increasing atomically throughout the distributed\nsystem, but they often are not, which causes subtle bugs.</li>\n</ol>\n<h2>The problem space</h2>\n<p>To design your system’s properties right, it is often useful or necessary to\ndetermine the order in which events happened. Ideally, you wish for the <strong>“wall\nclock” order</strong> (looking at your watch), although instantaneity gets tricky when\nevents occur at a distance, even within the same motherboard, but especially\nacross a datacenter, or between cities.</p>\n<p>At the very least, you want to reason about <strong>causal ordering</strong>: when that event\nhappened, did it already see this other event?</p>\n<p>A nice property to have, even for a single centralized database, is to give a\nmonotonically increasing identifier for each row. Most PostgreSQL users rely on\nthe <code>SERIAL</code> type for that – a sequence. Each insertion will call <code>nextval()</code>\nand store an increasing value.</p>\n<p>What you implicitly want is to list rows by insertion order, Your mental model\nis that each insertion happens at a set “wall clock” time. A first insertion\nwill happen at T0 and set the identifier 1, the next one happens at T1 and get\nnumber 2, and so on. Therefore, <em>you expect a row with ID N to have causally\nbeen inserted after a row with ID M &lt; N</em>.</p>\n<p>Operational order is a consistency constraint strongly associated with isolation\nlevels. A PostgreSQL database can handle multiple simultaneous operations.</p>\n<p><em>(Side note: I could be talking about threads and locks, but I will not, because\nthose are just tools to achieve properties. PostgreSQL may switch tools to\nbetter meet a given promise (they did so with the serializable level in 2011),\nbut the promise won’t change.)</em></p>\n<p>By default, it promises <strong>Read Committed</strong> isolation: a transaction can witness\nthe effects of all transactions that commit “before” it does (but not those that\nhave not committed yet). Their commits are therefore causally ordered by commit\ntime.</p>\n<p>However, nothing else within a transaction has any causal promise with respect\nto other transactions. The same <code>SELECT</code> can yield different values;\nsimultaneous insertions can happen either before, after, or anything in between,\nyour own insertion.</p>\n<p>The highest isolation level PostgreSQL offers is <strong>Serializable</strong> isolation: all\ntransactions are causally ordered; from <code>BEGIN</code> to <code>COMMIT</code>. Of course,\ntransactions still execute in parallel; but the database makes sure that\neverything that a transaction witnesses can be explained by executing all its\nstatements either after all statements of another transaction, or before all of\nthem. It won’t see a changing state within the execution of the transaction.</p>\n<p><em>(By the way, PostgreSQL only achieved serializability in 2011, when they\nreleased <a href=\"https://www.postgresql.org/docs/release/9.1.0/\">version 9.1</a> with support for predicate locks. It is hard.)</em></p>\n<p>Having a causal order does not mean that this order follows <em>real time</em>: one\ninsertion may complete at 9:30am <em>after (in causal order)</em> another that\ncompletes later at 10:40am. If you want the additional property that the order\nis consistent with wall clock time, you want <strong><a href=\"https://jepsen.io/consistency/models/strict-serializable\">Strict Serializability</a></strong>.</p>\n<p>However, <strong>PostgreSQL makes no claim of Strict Serializability</strong>.</p>\n<p>Given all this, sequences probably feel much weaker than you initially thought.</p>\n<p>You want them to give a continuous set of numbers, but a sequence can yield\nvalues with gaps (1 2 4).</p>\n<p>You want them to give a causal order <em>(2 was inserted before 3)</em>, but it can\nyield values out of order (1 3 2).</p>\n<p>All a sequence promises is to give values that have an order. Not a continuous\norder, nor a time order.</p>\n<p>Let’s demonstrate both.</p>\n<h2>Gaps</h2>\n<p>Let’s create a table with a <code>SERIAL</code> identifier. For the purpose of showing\nthings going right, let’s insert a row.</p>\n<pre><code class=\"language-sql\">CREATE TABLE gaps (id SERIAL);\nBEGIN;\nINSERT INTO order DEFAULT VALUES;\nSELECT * FROM gaps;\n</code></pre>\n<pre><code> id \n----\n  1\n(1 row)\n</code></pre>\n<p>Now comes the gap.</p>\n<pre><code class=\"language-sql\">BEGIN;\nINSERT INTO order DEFAULT VALUES;\nROLLBACK;\n</code></pre>\n<p>Since we rolled back, nothing happened – or did it?</p>\n<p>Let’s now insert another row.</p>\n<pre><code class=\"language-sql\">INSERT INTO order DEFAULT VALUES;\nSELECT * FROM gaps;\n</code></pre>\n<pre><code> id \n----\n  1\n  3\n(2 rows)\n</code></pre>\n<p>Oops! Despite the rollback, the sequence was incremented without being reverted.\nNow, there is a gap.</p>\n<p>This is not a PostgreSQL bug per se: the way sequences are stored, it just does\nnot keep the information necessary to undo the <code>nextval()</code> without potentially\nbreaking other operations.</p>\n<p>Let’s now break the other assumption.</p>\n<h2>Order violation</h2>\n<p>First, a table with a sequence and a timestamp:</p>\n<pre><code class=\"language-sql\">CREATE TABLE orders (id SERIAL, created_at TIMESTAMPTZ);\n</code></pre>\n<p>Let’s set up two concurrent connections to the database. Each will have the same\ninstructions. I started the first one yesterday:</p>\n<pre><code class=\"language-sql\">-- Connection 1\nBEGIN;\n</code></pre>\n<p>I launch the second one today:</p>\n<pre><code class=\"language-sql\">-- Connection 2\nBEGIN;\nINSERT INTO orders (created_at) VALUES (NOW());\nCOMMIT;\n</code></pre>\n<p>Let’s go back to the first one:</p>\n<pre><code class=\"language-sql\">-- Connection 1\nINSERT INTO orders (created_at) VALUES (NOW());\nCOMMIT;\n</code></pre>\n<p>Simple enough. But we actually just got the order violation:</p>\n<pre><code class=\"language-sql\">SELECT * FROM orders ORDER BY created_at;\n</code></pre>\n<pre><code> id |          created_at           \n----+-------------------------------\n  2 | 2019-09-04 21:10:38.392352+02\n  1 | 2019-09-05 08:19:34.423947+02\n</code></pre>\n<p>The order of the sequence does not follow creation order.</p>\n<p>From then on, developers may write some queries ordering by ID, and some\nordering by timestamp, expecting an identical order. That incorrect assumption\nmay break their business logic.</p>\n<p>Lest you turn your heart to another false god, that behavior remains the same\nwith serializable transactions.</p>\n<h2>Are we doomed?</h2>\n<p>No.</p>\n<p>Sure, the systems we use have weak assumptions. But that is true at every level.\nThe nice thing about the world is that you can combine weak things to make\nstrong things. Pure iron is ductile, and carbon is brittle, but their alloy is\nsteel.</p>\n<p>For instance, you can get the best of both worlds, causal order and “wall clock”\ntimestamps, by having a <code>TIMESTAMPTZ</code> field, only inserting rows within\nserializable transactions, and setting the <code>created_at</code> field to now, or after\nthe latest insertion:</p>\n<pre><code class=\"language-sql\">BEGIN ISOLATION LEVEL SERIALIZABLE;\nINSERT INTO orders (created_at)\nSELECT GREATEST(NOW(), MAX(created_at) + INTERVAL '1 microsecond') FROM orders;\nCOMMIT;\n</code></pre>\n<p>Indeed, PostgreSQL’s <code>TIMESTAMPTZ</code> has a precision up to the microsecond. You\ndon’t want to have conflicts in your <code>created_at</code> (otherwise you could not\ndetermine causal order between the conflicting rows), so you add a microsecond\nto the current time if there is a conflict.</p>\n<p>However, here, concurrent operations are likely to fail, as we acquire a\n(non-blocking) SIReadLock on the whole table (what the documentation calls a\nrelation lock):</p>\n<pre><code class=\"language-sql\">SELECT l.mode, l.relation::regclass, l.page, l.tuple, substring(a.query from 0 for 19)\nFROM pg_stat_activity a JOIN pg_locks l ON l.pid = a.pid\nWHERE l.relation::regclass::text LIKE 'orders%'\n  AND datname = current_database()\n  AND granted\nORDER BY a.query_start;\n</code></pre>\n<pre><code>       mode       | relation | page | tuple |     substring\n------------------+----------+------+-------+--------------------\n SIReadLock       | orders   |      |       | INSERT INTO orders\n RowExclusiveLock | orders   |      |       | INSERT INTO orders\n AccessShareLock  | orders   |      |       | INSERT INTO orders\n</code></pre>\n<p>The reason for that is that we perform a slow Seq Scan in this trivial example,\nas the <a href=\"https://www.postgresql.org/docs/current/using-explain.html\">EXPLAIN</a> proves.</p>\n<pre><code>                                  QUERY PLAN\n-------------------------------------------------------------------------------\n Insert on orders  (cost=38.25..38.28 rows=1 width=8)\n   -&gt;  Aggregate  (cost=38.25..38.27 rows=1 width=8)\n         -&gt;  Seq Scan on orders orders_1  (cost=0.00..32.60 rows=2260 width=8)\n</code></pre>\n<p>With an <a href=\"https://www.postgresql.org/docs/current/sql-createindex.html\">index</a>, concurrent operations are much more likely to work:</p>\n<pre><code class=\"language-sql\">CREATE INDEX created_at_idx ON orders (created_at);\n</code></pre>\n<p>We then only take a tuple lock on the table:</p>\n<pre><code>       mode       | relation | page | tuple |     substring      \n------------------+----------+------+-------+--------------------\n SIReadLock       | orders   |    0 |     5 | INSERT INTO orders\n RowExclusiveLock | orders   |      |       | INSERT INTO orders\n AccessShareLock  | orders   |      |       | INSERT INTO orders\n</code></pre>\n<p>However, the tuple in question is the latest row in the table. Any two\nconcurrent insertions will definitely read from the same one: the one with the\nlatest <code>created_at</code>. Therefore, only one of concurrent insertion will succeed;\nthe others will need to be retried until they do too.</p>\n<h2>Subset Ordering</h2>\n<p>In cases where you only need a unique ordering for a subset of rows based on\nanother field, you can set a combined index with that other field:</p>\n<pre><code class=\"language-sql\">CREATE TABLE orders (\n  account_id UUID DEFAULT gen_random_uuid(),\n  created_at TIMESTAMPTZ);\nCREATE INDEX account_created_at_idx ON orders (account_id, created_at DESC);\n</code></pre>\n<p>Then the <a href=\"https://www.postgresql.org/docs/current/using-explain.html\">query planner</a> goes through the account index:</p>\n<pre><code class=\"language-sql\">INSERT INTO orders (account_id, created_at)\nSELECT account_id, GREATEST(NOW(), created_at + INTERVAL '1 microsecond')\nFROM orders WHERE account_id = '9c99bef6-a05a-48c4-bba3-6080a6ce4f2e'::uuid\nORDER BY created_at DESC LIMIT 1\n</code></pre>\n<pre><code>                                                      QUERY PLAN\n-----------------------------------------------------------------------------------------------------------------------\n Insert on orders  (cost=0.15..3.69 rows=1 width=24)\n   -&gt;  Subquery Scan on &quot;*SELECT*&quot;  (cost=0.15..3.69 rows=1 width=24)\n         -&gt;  Limit  (cost=0.15..3.68 rows=1 width=32)\n               -&gt;  Index Only Scan using account_created_at_idx on orders orders_1  (cost=0.15..28.35 rows=8 width=32)\n                     Index Cond: (account_id = '9c99bef6-a05a-48c4-bba3-6080a6ce4f2e'::uuid)\n</code></pre>\n<p>And concurrent insertions on different accounts work:</p>\n<pre><code>       mode       | relation | page | tuple |     substring\n------------------+----------+------+-------+--------------------\n SIReadLock       | orders   |    0 |     1 | INSERT INTO orders\n RowExclusiveLock | orders   |      |       | INSERT INTO orders\n AccessShareLock  | orders   |      |       | INSERT INTO orders\n SIReadLock       | orders   |    0 |     2 | COMMIT;\n</code></pre>\n<p>(The first three row are from one not-finished transaction on account 1, the\nlast is from a finished one on account 2.)</p>\n<script type=\"application/ld+json\">\n{ \"@context\": \"http://schema.org\",\n  \"@type\": \"BlogPosting\",\n  \"datePublished\": \"2019-09-05T17:28:59Z\",\n  \"keywords\": \"sql\" }\n</script>\n",
      }
  ]
}
