{
  "version": "https://jsonfeed.org/version/1",
  "title": "The espadrine blog.",
  "description": "Tech deep dives, discoveries, and analyses.",
  "home_page_url": "https://espadrine.github.io/blog/",
  "feed_url": "https://espadrine.github.io/blog/feed.json",
  "author": {
    "name": "Thaddée Tyl",
    "url": "https://github.com/espadrine"
  },
  "favicon": "https://avatars.githubusercontent.com/u/100689?s=64",
  "items": [
      {
        "id":  "https://espadrine.github.io/blog/posts/gpu-performance.html",
        "url": "https://espadrine.github.io/blog/posts/gpu-performance.html",
        "title": "ML GPU performance: AMD facing NVIDIA",
        "tags": "gpu ml",
        "date_published": "2023-06-18T21:40:09Z"
        "content_html": "<h1 id=\"ML_GPU_performance_AMD_facing_NVIDIA\">ML GPU performance: AMD facing NVIDIA <a href=\"#ML_GPU_performance_AMD_facing_NVIDIA\" class=\"autolink-clicker\" aria-hidden=\"true\">§</a></h1>\n<p>I am pretty impressed seeing <a href=\"https://twitter.com/LisaSu/status/1669848494637735936\">Lisa Su</a> doing her best to steer the AMD ship towards\nbetter AI support in GPUs, with the <a href=\"https://huggingface.co/blog/huggingface-and-amd\">Huggingface partnership</a> and by convincing\nGeorge Hotz to submit more bug reports.</p>\n<p>(For context, <a href=\"https://geohot.github.io//blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html\">Hotz raised $5M</a> to improve RX 7900 XTX support and sell a $15K\nprebuilt consumer computer that runs 65B-parameter LLMs. A plethora of driver\ncrashes later, he almost <a href=\"https://github.com/RadeonOpenCompute/ROCm/issues/2198#issuecomment-1574383483\">gave up on AMD</a>.)</p>\n<p>There’s quite a few issues to overcome, though.\nWhile that GPU is great\n(Stable Diffusion iteration speed per GPU cost is top-tier),\na cursory study would be flawed:\npublic GPU benchmarks like TechPowerUp, TomsHardware, etc. give:</p>\n<ul>\n<li><strong>RX 7900 XTX:</strong> <a href=\"https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889\">123 TFLOPS</a></li>\n<li><strong>RTX 4090:</strong> <a href=\"https://www.tomshardware.com/reviews/amd-radeon-rx-7900-xtx-and-xt-review-shooting-for-the-top\">82.58 TFLOPS</a></li>\n</ul>\n<p>Where do the figures come from?</p>\n<p>While there is no official breakdown,\nonly <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">official figures</a>, people widely compute it this way:</p>\n<ul>\n<li>For <strong>NVIDIA</strong>:\n<a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0\">Boost Clock (THz) × CUDA Cores × 2</a>\n(since the FMA instruction does two floating-point operations\n(a multiplication and an addition) in 1 CUDA core cycle).</li>\n<li>For <strong>AMD</strong> on RDNA3:\n<a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">Boost Frequency (THz) × Stream processors × 2 (dual issue) × 4 (dot product)</a>,\nas <a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\">RDNA3 has <code>V_DUAL_DOT2ACC_F32_F16</code></a>,\nwhich does two dot products (a×b+c×d+e, 4 operations),\nin 1 processor cycle.</li>\n</ul>\n<table>\n  <tr><th> Name </th><th> Price </th><th> Processors </th><th> Frequency </th><th> TFLOPS (FP16) </th><th> Perf/€ </th>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">RX 7900 XTX</a> </td>\n      <td> €1110 </td><td>  6144 </td><td>  2.5 GHz </td><td> 122.88 </td><td> 0.1107 </td>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xt\">RX 7900 XT</a> </td>\n      <td>  €942 </td><td>  5376 </td><td>  2.4 GHz </td><td> 103.22 </td><td> 0.1096 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0\">RTX 4090</a> </td>\n      <td> €1770 </td><td> 16384 </td><td> 2.52 GHz </td><td>  82.58 </td><td> 0.0467 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3060</a> </td>\n      <td>  €314 </td><td>  3584 </td><td> 1.78 GHz </td><td>  12.76 </td><td> 0.0405 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3080</a> </td>\n      <td>  €905 </td><td>  8704 </td><td> 1.71 GHz </td><td>  29.76 </td><td> 0.0329 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3090</a> </td>\n      <td> €1500 </td><td> 10496 </td><td> 1.70 GHz </td><td>  35.68 </td><td> 0.0238 </td>\n</table>\n<p>That is an unjust comparison, though, because AMD’s instruction is more niche\nthan FMA (hitting this performance sweet spot is thus uncommon),\nand because both of those GPUs have other tricks up their sleeves,\nyielding superior FLOPS.</p>\n<p>The <a href=\"https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#Will_AMD_GPUs_ROCm_ever_catch_up_with_NVIDIA_GPUs_CUDA\">big one</a> on NVIDIA are <a href=\"https://www.nvidia.com/en-us/data-center/tensor-cores/\">Tensor cores</a>.\nWith them, you can run an instruction that does\n<a href=\"https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf\">a 4×4 to 4×8 matrix multiplication (page 25)</a>\nin 1 cycle within a single Tensor Core (32 CUDA cores).</p>\n<p>2×4^2×8 (matmul ops) ÷ 1 (cycles) = 256 ops/TC/cycle.</p>\n<p>(There is <a href=\"https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/\">some variation between NVIDIA GPUs</a>\non which matrix sizes are supported and on how many cycles the instruction takes,\nand NVIDIA keeps major aspects of their instruction set secret,\nbut on recent 30- and 40-series, this 256 number seems fairly constant.)</p>\n<p><img src=\"https://www.nvidia.com/content/dam/en-zz/Solutions/gtcs22/tensor-cores/hopper-tensor-core-ampere-2c50-t.jpg\" alt=\"Official image showing the matrix size for third-generation tensor cores with V100 FP32\" /></p>\n<p>That puts the actual RTX 4090 at\n256 × 512 (Tensor Cores) × 2.52 (GHz)\n÷ 1K (GHz per teracycle/s) = <a href=\"https://en.wikipedia.org/wiki/GeForce_40_series#Desktop\">330 TFLOPS in FP16</a>…\nMuch higher than the 123 TFLOPS that impressed Hotz on the RX 7900 XTX!</p>\n<p>But AMD now has the same trick.\nIn <a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\">RDNA3</a>, with <a href=\"https://gpuopen.com/learn/wmma_on_rdna3/\">WMMA</a>, the RX 7900 XTX has an instruction,\n<a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\"><code>V_WMMA_F16_16X16X16_F16</code></a>\nthat do two 16×16 matrix multiplications in <a href=\"https://github.com/RadeonOpenCompute/amd_matrix_instruction_calculator/blob/339d784e56e55752495192b0781ea162fc32e323/matrix_calculator.py#LL1139C26-L1139C26\">32 cycles</a>,\nin a single Compute Unit (two sets of 32 threads).</p>\n<p>2×16^3 (matmul ops) × 2 ÷ 32 (cycles) = 512 ops/CU/cycle.</p>\n<p>This uses the same underlying silicon circuits as <code>V_DUAL_DOT2ACC_F32_F16</code>:\nthe architecture lays out the matrices in Vector General-Purpose Registers.\nEach cell of the output matrix is computed by multiplying\none row from input matrix A with one column from input matrix B,\ntwo input cells at a time\n(two adjacent input A row cells packed inside the same VGPR,\nand two adjacent input B column cells packed together inside another VGPR),\nso they can be used by the packed dot product single-cycle instruction.\nWithin that same instruction, encoded in VOPQ\n(a SIMD-like system to execute one operation\non an even register while it executes on an odd one at the same time),\nan adjacent output cell also multiplies through its first two input cells\nat the same time using dual issue.</p>\n<p>The input row has size 16, so those two output cells are completed in 8 cycles.\nEach two adjacent output cells in their diagonal\nare computed with 16 parallel threads (on separate stream processors)\nwithin the same 8 cycles.\nWe have done two diagonals (32 output cells); there are 14 diagonals left.\nInside that Compute Unit, we still have 16 stream processors that we can use;\nthey can handle two more output diagonals within the same 8 cycles.</p>\n<p>Once our first four diagonals are computed,\nwe sequentially compute the next 4 diagonals in the next 8 cycles.\nSo forth for the next 4, and the last 4 after that.\nIn total, we have computed the matrix multiplication\nin 32 cycles, which checks out.</p>\n<p>Why can’t we do the matrix multiplication in 16 cycles\nby using all 64 threads inside of the Compute Unit?\n<a href=\"https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf\">Section 7.6 of the instruction set manual</a> indicates:</p>\n<blockquote>\n<p>[Dual issue] is legal only for wave32.</p>\n</blockquote>\n<p>WMMA supports both wave32 and wave64, but it sounds like dual issue is\ndeactivated in wave64, and thus it would still take 32 cycles,\nmaking it an ill-documentedly unfavorable proposition, I believe.</p>\n<p>All in all, using <a href=\"https://gpuopen.com/learn/wmma_on_rdna3/\">WMMA</a>, the RX 7900 XTX can crank through\n512 × <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">96 (Compute Units) × 2.5 (GHz)</a>\n÷ 1K (GHz per teracycle/s) = <a href=\"https://en.wikipedia.org/wiki/RDNA_3#Desktop\">123 TFLOPS in FP16</a>…</p>\n<p>That ends up being less than half the performance of the RTX 4090.\nThe superior number of operations per Compute Unit is offset by the\ncrushingly lower number of cores.\nPerhaps the AMD strategy is to have the better circuit ready\nbefore migrating to the TSMC N5 (“5 nm”) process at a less affordable price.</p>\n<p>In practice, the lower performance is less of an issue for AI training,\nbecause they are famously limited in the amount of parallelization opportunities\n(even the best training runs typically incur only 50% GPU use at a given time).\nThe VRAM bandwidth then matters a lot for large models,\nand the <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">RX 7900 XTX</a>, despite using GDDR6 instead of GDDR6X,\nhas a higher bandwidth than the RTX 3090, thanks to its faster memory clock.\nStill, it also is lower than the RTX 4090 on that front\n(but at a lower price point).</p>\n<table>\n  <tr><th> Name </th><th> Price </th><th> TFLOPS (FP16) </th><th> Memory bandwidth (GB/s)</th><th> Value (TFLOPS·GB/s/€) </th>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0\">RTX 4090</a> </td>\n      <td> €1770 </td><td> 330 </td><td> 1008 </td><td> 188 </td>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx\">RX 7900 XTX</a> </td>\n      <td> €1110 </td><td> 123 </td><td> 960 </td><td> 106 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3080</a> </td>\n      <td>  €905 </td><td> 119 </td><td> 760 </td><td> 100 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3090</a> </td>\n      <td> €1500 </td><td> 143 </td><td> 936 </td><td> 89 </td>\n  <tr><td> <a href=\"https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xt\">RX 7900 XT</a> </td>\n      <td>  €942 </td><td> 103 </td><td> 800 </td><td> 87 </td>\n  <tr><td> <a href=\"https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85\">RTX 3060</a> </td>\n      <td>  €314 </td><td> 51 </td><td> 360 </td><td> 58 </td>\n</table>\n<p>The other thorns on the side of AMD in AI rear their ugly head:</p>\n<ul>\n<li><a href=\"https://chipsandcheese.com/2023/01/07/microbenchmarking-amds-rdna-3-graphics-architecture/\">The compilers don’t produce great instructions</a>;</li>\n<li>The drivers crash frequently: ML workloads feel experimental;</li>\n<li>Software adoption is getting there,\nbut kernels are less optimized within frameworks,\nin particular because of the fracture between ROCm and CUDA.\nWhen you are a developer and you need to write code twice,\none version won’t be as good, and it is the one with less adoption;</li>\n<li>StackOverflow mindshare is lesser. Debugging problems is thus harder,\nas fewer people have encountered them.</li>\n</ul>\n<p>(I will note, however, that the wealth of information provided by AMD\noutshines that from NVIDIA tremendously,\neven though they could better vulgarize those subtleties and\nexplain how to perform specific workloads like BERT training,\ninto which NVIDIA puts welcome care.\nJust contrast <a href=\"https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html\">NVIDIA’s matmul page</a> to <a href=\"https://gpuopen.com/learn/wmma_on_rdna3/\">AMD’s</a>.\n<a href=\"https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html\">AMD doesn’t even recognize its own flagship GPUs as supported for ROCm</a>,\nwhich is mindboggling coming from NVIDIA’s superior CUDA support.)</p>\n<script type=\"application/ld+json\">\n{ \"@context\": \"http://schema.org\",\n  \"@type\": \"BlogPosting\",\n  \"datePublished\": \"2023-06-18T21:40:09Z\",\n  \"keywords\": \"gpu, ml\" }\n</script>\n",
      }
  ]
}
