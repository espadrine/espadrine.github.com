<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Espadrine’s blog</title>
  <subtitle>Let’s talk about whatever I learn!</subtitle>
  <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/"/>
  <link rel="self" type="application/atom+xml" href="https://espadrine.github.io/blog/feed.xml"/>
  <id>https://espadrine.github.io/blog/feed.xml</id>
  <updated>2020-07-05T20:19:02Z</updated>
      <entry>
        <id>https://espadrine.github.io/blog/posts/webidentity.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/webidentity.html"/>
        <title>WebIdentity: One-click passwordless signups & logins</title>
        <published>2020-07-05T20:19:02Z</published>
        <category term="crypto"/>
<category term="web"/>
        <content type="html">
          <![CDATA[ <h1>WebIdentity: One-click passwordless signups &amp; logins</h1>
<p>I talked about <a href="https://espadrine.github.io/blog/posts/memorable-passwords.html">best-practices for password-based logins last time</a>,
and gave tools to help you follow them.</p>
<p>Password managers (and generators) must become prevalent.
Thankfully, it is becoming a reality: beyond services such as 1password or Dashlane,
browsers themselves now offer those features built-in.
It sprouted from the bookmark sync feature, became password sync,
and now has suggested random passwords.</p>
<p>But <strong>passwords are inherently flawed</strong> as a security user experience.
Honestly, they slow down both registration to a new service, and logins.
It annoys users, allows terrible security practices, and
loses businesses billions yearly, both on users that give up,
and reputation from security issues.</p>
<p>There is a high cost to websites to implement and maintain security practices
around password storage.
By the way, this is the most significant example of “roll your own crypto”,
as each website defines its own authentication protocol.</p>
<p>There is also a cost for browsers: maintaining a list of passwords,
one for each website, makes for a fairly large total storage.
A significant, pernicious consequence is the emergence of siloes,
encouraging browser monopolies:
why would I switch browsers, when it is so hard to copy all those passwords over?</p>
<p>My hope for the future of authentication:</p>
<ul>
<li>Become so seamless that <strong>both signing up and logging in is a single click</strong>.</li>
<li>We want that improved UX, not just with the same security level that we have now, but a much better one.
<ul>
<li>An attacker that got its hands on a fresh plaintext copy of the website’s database and secrets should be unable to authenticate on behalf of users.</li>
<li>Even if someone can decrypt traffic on-the-fly, from seeing the authentication information, they also can’t impersonate the user for more than 30 seconds afterwards.</li>
<li>Even with both a full-on breach of the website’s servers <em>and</em> on-the-fly traffic decryption, they cannot sign up nor log in as you.</li>
</ul>
</li>
<li>And, cherry on top, we want to do so in such a way that <em>exporting identities between browsers and devices is extremely easy</em>.</li>
</ul>
<p>I called this new scheme <strong>WebIdentity</strong>.</p>
<h2>The place of WebAuthn</h2>
<p>Your mind may be itching to yell: “WebAuthn!”
So, before digging into the gritty details of WebIdentity,
let’s talk about the awesomeness that is WebAuthn.</p>
<p><a href="https://webauthn.guide/">WebAuthn</a> is an authentication scheme that relies on public-key cryptography.</p>
<p>A phenomenal advantage of WebAuthn over WebIdentity is
in the ability to leverage a wide range of different security devices
(Yubico, TouchID, Windows Hello, and so many more).
In short, it is <em>absolutely amazing</em> at second-factor authentication (2FA),
which is completely outside the scope of WebIdentity and would complement it beautifully.</p>
<p>However, WebAuthn has severe disadvantages for use as main authentication,
which WebIdentity solves wonderfully.
Betting on WebAuthn as the main sign-up and log-in system
risks significantly delaying the wide adoption
of passwordless authentication on the Web:</p>
<ul>
<li>Websites do not implement it because:
<ul>
<li>on the backend, it requires them to implement a PKI, which is an endeavor.
Multiple public keys can map to the same user, and management of the keys
(and of the certificate validation upon sign-up, when necessary) needs proper handling.</li>
<li>on the front-end, it requires subtle cryptography to handle the challenge handshake,
that needs to be implemented not just for the Web, but for iOS and Android apps.
With WebIdentity, there is no front-end change at all.</li>
</ul>
</li>
<li><strong>Feature detection</strong> is involved. Both the front-end and the backend must know
that the user can do WebAuthn on that device, and ideally, the backend stores it
on secure httpOnly cookies. It also must store the list of credential IDs:
there could be multiple users on the same device.
WebIdentity relies on the user management feature already built into the browser’s Sync.</li>
<li>For websites, it requires a <strong>database access</strong> for each authentication.
That contributes to a requirement to only use WebAuthn for login, not session authentication.
WebIdentity does both in one fell swoop, and database interaction is only needed for sign-ups and logins.</li>
<li>More damningly, the protocol requires a back-and-forth;
in other words, it cannot be used directly for all HTTP requests.
It needs a separate, <strong>unspecified session key management scheme</strong> such as JWT.
With WebIdentity, the user authentication scheme is indistinguishable from
the session authentication scheme.</li>
<li>Most damningly, <strong>exporting the public/private key pairs between browsers</strong> was not part of the design process.
In fact, synchronizing those keys (some of which can change) between browsers is a complex operation.
It goes so far that the <a href="https://developer.apple.com/videos/play/wwdc2020/10670/">official recommendation from browser makers</a> is to never have WebAuthn log-in
be the only log-in mechanism, because private keys are tied to the device.
Changing (or losing) devices require logging in through another means.
Thus, it is only a faster log-in process (click login, select identity, accept request, enter biometrics or press Yubikey button)
than passwords, but passwords remain the main log-in weak link that won’t die.
Meanwhile, WebIdentity has a faster log-in still (there is literally just the click login step),
and can fully replace passwords.</li>
<li>To roll it out slowly, it started being <strong>used only as 2FA</strong>.
It is now usually tied to things like Yubikey in the UX,
and seems doomed to remain only used for 2FA,
as a consequence of everything listed above.</li>
<li>WebAuthn suffers from <strong>crypto-agility</strong>, similar to JWT where it caused security faults.
In WebIdentity, algorithms are set for each version,
and version changes are only performed when the previous algorithm is broken.
Since there is really only one cryptographic algorithm, it is easy to keep track of.</li>
<li>In the same vein, it is easy for a service operator to <strong>misuse</strong> WebAuthn,
and end up losing major security protections. There are many subtle traps.
For instance, they may generate challenges with a poor random source,
or forget to validate some metadata field received.</li>
<li>Where is the <strong>NFC antenna</strong> on this phone?</li>
<li>Public-key cryptography is currently at risk from <strong>quantum computing</strong> advances,
whereas WebIdentity relies on primitives that are essentially <a href="https://en.wikipedia.org/wiki/SHA-3#Security_against_quantum_attacks">quantum-resistant</a>.</li>
</ul>
<p>All in all, WebAuthn is harder for website operators to put in place than passwords,
while WebIdentity is simpler.
(Although they would only do so once all major evergreen browsers implement it.)</p>
<p>The status quo: despite browsers’ efforts to add support not just for Yubikeys but also TouchID,
website owners are very shy in implementing support even just for 2FA,
in part because of the implementation complexity,
and the user experience is frustrating enough currently that few actually use it.
I do not know any independent commercial website
that uses WebAuthn as its primary log-in system, instead of passwords.</p>
<p>WebIdentity can quickly replace password authentication with a much simpler system,
both for users, website operators, and browsers;
while WebAuthn is likely to have a slow, 10-year adoption across websites.</p>
<p>However, WebIdentity does not replace WebAuthn!
WebAuthn is still extremely valuable as 2FA,
which should really be used on all sensitive actions on the website.</p>
<h2>Initialization</h2>
<p>First, the browser stores a single random 256-bit secret for each user,
called the <strong>Browser Key (BK)</strong>,
synchronized across all devices through its Sync feature.
That key never leaves the browser’s Sync servers.</p>
<p>Each website keeps around a random 256-bit secret key (<strong>Website Key, WK</strong>)
identified by a <strong>key ID (KID)</strong>,
typically a number incremented every time the key needs to change,
which should be rare, eg. yearly.
It must be generated by a CSPRNG, for instance from /dev/urandom.</p>
<h2>Sign-Up</h2>
<p>When going on a page, the website advertizes that it supports WebIdentity
by delivering this header (even on 200 status results):</p>
<pre><code>WWW-Authenticate: Identity v1
</code></pre>
<p>Upon seeing it, the browser displays a “Log in” button <em>in its chrome</em>
(above the <a href="https://textslashplain.com/2017/01/14/the-line-of-death/">line of death</a>, thus not in the webpage),
if the website uses TLS and HSTS.</p>
<p>When the user clicks it, from then on, for all HTTPS requests to that website,
the browser will authenticate the user to the website,
and display a “Log out” button instead of the “Log in” one.</p>
<p>But first, there is a tiny sign-up handshake.
First, the browser computes the <strong>User’s Website Key (UWK)</strong> as the MAC
of the effective Top-Level Domain <a href="https://publicsuffix.org/">eTLD+1</a>:
it is unique for each entity that has control over a given cookie space,
and incidentally will also soon be <a href="https://chromium.googlesource.com/chromium/src/+/master/docs/security/url_display_guidelines/url_display_guidelines.md#registrabledomain">the only URL part shown</a> (to fight phishing).
So the security UX will be consistent for identity and website trust here.
The UWK MAC is keyed with BK, the user’s Sync secret kept by the browser.
The UWK is a <em>secret that the browser has for each user and each website</em>.
It is never stored and only transmitted between the browser’s Sync servers
and the user’s browser upon sign-up and login.</p>
<p>Then, the browser takes a MAC of the ASCII string “AUID”, keyed with UWK:
this becomes the <strong>Authentication User ID (AUID)</strong>
which will <em>identify the user in each HTTP request</em>.
Eavesdropper cannot find the UWK from it, which is good,
since it is only used for very rare, sensitive operations.</p>
<p>Finally, the browser picks a <strong>Log-In Date (LID)</strong> to send as an HTTP header,
and computes its MAC, keyed with the User’s Website Key (UWK).
The result is the <strong>Log-In Proof token (LIP)</strong>,
a piece of information kept secret by the browser,
which will be later revealed to the website when logging back in,
to strongly prove that the initiator is the initial user.</p>
<p>Aside: as you can imagine, there will be a whole tree of hashes,
each with a different purpose and name.
To help you follow along, here is a picture of the entire tree:</p>
<p><img src="../assets/webidentity/webidentity-hash-tree.svg" alt="WebIdentity hash tree" /></p>
<p>The browser reloads the page with the Date header set to the LID,
and the following new header
(all in one line, with spaces instead of newlines; the newlines are for readability):</p>
<pre><code>Authorization: Identity v1 SignUp
  auid=&quot;VzX3h8VumdWIY7MiUCcYwnS8kz9DxdtFzQftFhLvkFkK&quot;
  liv=&quot;7deoyUWH9wk-x15mb-vr7i57rU0VojDLwc99EjtKUlUK&quot;
</code></pre>
<ul>
<li><code>Identity</code> indicates that it uses this authorization scheme.</li>
<li><code>v1</code> is the version of the scheme; it likely would change very rarely, typically when the hash function gets broken.</li>
<li><code>SignUp</code> is the action: here, we sign up as a new user.</li>
<li><code>VzX3…</code> is the Authentication User ID (AUID), which the website will rely on to identify the user.</li>
<li><code>7deo…</code> is the <strong>Log-In Verification token (LIV)</strong>,
a MAC of the AUID keyed with the Log-In Proof token (LIP).</li>
<li>The LID is sent in the Date header so that the website can store it.</li>
</ul>
<p>The website cannot guess the LIP, nor can any eavesdropper,
which is good, since the LIP will be used to prove knowledge of BK
for rare, sensitive updates.</p>
<p>The website identifies the user from the AUID (indirectly),
but it cannot guess the user’s AUID for another website.
Besides, two websites cannot know that their respective AUIDs correspond to the same user
without seriously endangering the security of their own authentication.
That protects the user’s privacy across websites.</p>
<p>Upon receiving a SignUp request,
the website takes a MAC of the AUID, keyed with WK (the website’s secret key).
That is the <strong>Website’s User Key (WUK)</strong>, a secret kept by the website, unique to a user.
It is roughly the opposite of the User’s Website Key (UWK).
The user cannot know the website’s other users’ WUK,
since it would need both their BK and the WK to do so.</p>
<p>Then, the website computes the <strong>User ID (UID)</strong> as the MAC of the AUID
keyed with its Website’s User Key (WUK).
The UID will be stored in database, etc.
Intruders cannot find the AUID nor the WUK from read-only access to the database,
preventing them from crafting fake authenticated requests.</p>
<p>Then it does the following:</p>
<ol>
<li>It verifies that the LID is within one minute of its known date. If not, the sign-up is denied.</li>
<li>It stores in database the UID, the LID, and the LIV, likely with a mapping to its internal user identifier.
In our example, the UID is <code>XvP5sxmrh8UmpgYqJ9OmKs9HqhxcdS5-lUxlaEuhBc4</code>.</li>
</ol>
<p>Then, the website prepares a response.</p>
<p>First, it constructs the <strong>Log-In Shared Key (LISK)</strong>
as the MAC of the Log-In Date (LID) keyed with the Website’s User Key (WUK).
That key will be <em>shared between the website and the browser</em> for one hour,
and will be used to compute a TOTP.</p>
<p>If the website sees that the user was already signed up,
it will accept it, but with slight differences in the response
that are discussed in the Log In section.
Otherwise, it returns a page with the following header:</p>
<pre><code>WWW-Authenticate: Identity v1 Key
  kid=&quot;2020&quot;
  auid=&quot;VzX3h8VumdWIY7MiUCcYwnS8kz9DxdtFzQftFhLvkFkK&quot;
  lisk=&quot;Ii6JLfnbWJgcy0WtworWKRIlJIPSGkQwSAvBtQM1OEgK&quot;
</code></pre>
<ul>
<li><code>Key</code> is the action instructing the browser to store the website’s key.</li>
<li><code>2020</code> is the KID, placed first to ease inspection.</li>
<li><code>VzX3…</code> is the AUID, identifying the user in all future requests.</li>
<li><code>Ii6J…</code> is the LISK, which will be used to prove that the user is who they claim to be for one hour.</li>
</ul>
<p>(The website can also send identifying data,
such as its internal ID (eg. a username or its database’s generated <code>user_id</code>),
in a payload encrypted with the WUK as key,
in the Cookies header, ideally Secure and httpOnly.
That lets it avoid a database fetch when it relies on an internally-produced ID
instead of the UID provided by WebIdentity.
That part is outside the definition of WebIdentity, however.)</p>
<p>The browser stores the version (v1), the KID, the LID and the LISK in its Sync feature.</p>
<h2>Authentication</h2>
<p>On each HTTP request while logged in, the browser sends the AUID,
along with a MAC of the Date HTTP header keyed with the LISK:</p>
<pre><code>Authorization: Identity v1 Auth
  kid=&quot;2020&quot;
  auid=&quot;VzX3h8VumdWIY7MiUCcYwnS8kz9DxdtFzQftFhLvkFkK&quot;
  lid=&quot;Fri, 03 Jul 2020 10:11:22 GMT&quot;
  totp=&quot;YrrliECBpS34lKob4xMOIKgM5zw8_zxMsBBleIIfGHIK&quot;
</code></pre>
<ul>
<li><code>Auth</code> is the action to authenticate the request.</li>
<li><code>2020</code> is the KID in use.</li>
<li><code>VzX3…</code> is the AUID, as returned from the SignUp response.</li>
<li>The Log-In Date (LID) lets the website compute the LISK.</li>
<li><code>Yrrl…</code> is the <strong>Time-based One-Time Password (TOTP)</strong>:
the MAC of the Date (<code>Fri, 03 Jul 2020 14:32:19 GMT</code>), keyed with the LISK.</li>
</ul>
<p>When receiving an Auth request, the website must:</p>
<ol>
<li>Verify that the Date sent is within one minute of the accurate date. The request is denied otherwise.</li>
<li>Verify that the Log-In Date (LID) is not more than one hour old.
The request is denied otherwise: the browser always knows to make a LogIn request (seen below) instead.
(Note that it does not matter if the LID does not match the stored LID.
That way, multiple browsers can share the same BK and still authenticate in parallel.)</li>
<li>Compute the MAC of the request’s AUID, keyed with the WK. That is the WUK.</li>
<li>Compute the MAC of the LID, keyed with the WUK. That is the LISK.</li>
<li>Compute the MAC of the Date, keyed with the LISK. Verify that it matches the TOTP. The request is denied otherwise.</li>
<li>Compute the MAC of the request’s AUID, keyed with the WUK: that is the UID, which can be used for application purposes.</li>
</ol>
<p>Note that this computation does not require database access, and is quite efficient in software.</p>
<p>The explanation of the main principle of operation is already finished.
Let’s look at a few events that may occur,
ranging in order from uncommon (monthly?) to extremely rare (every 20 years?).</p>
<h3>Log Out</h3>
<p>When logged in, the browser’s Log In button changes to a Log out button.</p>
<p>When clicking the Log out button,
the browser deletes the protocol version, KID, AUID and LISK in Sync;
and no longer sends Authorization headers.</p>
<p>The browser logs out and logs back in automatically every hour,
to ensure it does not use the same LISK for too long.
Because of the way log-outs and log-ins work,
this is entirely seamless and invisible to the user.</p>
<h3>Log In</h3>
<p>When the browser tries to log in, in fact, it starts by simply doing the sign-up procedure.</p>
<p>The website detects that a sign-up already occured, and initiates the login procedure:</p>
<pre><code>WWW-Authenticate: Identity v1 LogIn
  lid=&quot;Fri, 03 Jul 2020 10:11:22 GMT&quot;
</code></pre>
<p>You can find after the LogIn keyword, the Log-In Date (LID) that the website registered for this UID.</p>
<p>The browser’s Sync server computes the User’s Website Key
(UWK, a MAC of the eTLD+1 keyed with BK),
and keys with it a MAC of that LID.
That gives it the Log-In Proof (LIP) that was created during sign-up.</p>
<p>Just as with a normal sign-up,
the browser picks a new <strong>Log-In Date (LID)</strong> to send as an HTTP header,
and computes its MAC, keyed with the User’s Website Key (UWK).
The result is a brand-new <strong>Log-In Proof (LIP)</strong>.
(In our example, the new LID is <code>Fri, 03 Jul 2020 15:27:43 GMT</code>.)</p>
<p>It then sends a LogIn request,
which is essentially identical to the SignUp request, but with the new LIV:</p>
<pre><code>Authorization: Identity v1 LogIn
  auid=&quot;VzX3h8VumdWIY7MiUCcYwnS8kz9DxdtFzQftFhLvkFkK&quot;
  olip=&quot;8x8HgKzEl5nok-JNwT2PCiwnfwrCD2rOxtMTUotU4hgK&quot;
  liv=&quot;S4GFp0Xh8rSeV9-VgpNTCW2iDPd36sABZrGPqwj8oJkK&quot;
</code></pre>
<ul>
<li><code>VzX3…</code> is the Authentication User ID (AUID).</li>
<li><code>8x8H…</code> is the old Log-In Proof (LIP).</li>
<li><code>S4GF…</code>, is a new <strong>Log-In Verification token (LIV)</strong>,
a MAC of the AUID keyed with the Log-In Proof (LIP).</li>
<li>The LID is sent in the Date header, so that the website can store it.</li>
</ul>
<p>The website constructs the WUK as the MAC of the AUID keyed with its WK,
and gets the UID as the MAC of the AUID keyed with the WUK.
Then it validates the following:</p>
<ol>
<li>The LID must be within one minute of its known date.</li>
<li>The old LIV must be the one associated with this UID as stored in database.</li>
<li>Computing the MAC of the AUID keyed with the old LIP transmitted in the request,
yields the old LIV stored in database.</li>
</ol>
<p>If the validation fails, the LogIn request is denied.
Then, if both validated OK, it updates in database the sign-up Date and the new LIV.</p>
<p>You may notice that neither the website,
nor any eavesdropper with full read access to the website,
could guess the LIP until they see it in the Log In request.
Thus, they could not perform a Log In request;
and when they see it in the HTTPS payload, it is too late to take advantage of it,
as the LIV is updated with a new one for which they don’t have the LIP.</p>
<p>The rest goes exactly like a Sign Up:</p>
<pre><code>WWW-Authenticate: Identity v1 Key
  kid=&quot;2020&quot;
  auid=&quot;VzX3h8VumdWIY7MiUCcYwnS8kz9DxdtFzQftFhLvkFkK&quot;
  lisk=&quot;zhgoQXVsATIUd-S2mB1gUlKi5yj_iO7K7KrsI_H8rBEK&quot;
</code></pre>
<ul>
<li><code>Key</code> is the action instructing the browser to store the website’s key.</li>
<li><code>2020</code> is the KID, placed first to ease inspection.</li>
<li><code>VzX3…</code> is the AUID, identifying the user in all future requests.</li>
<li><code>zhgo…</code> is the Log-In Shared Key (LISK), which will be used to prove that the user is who they claim to be.</li>
</ul>
<p>The browser stores the version (v1), the KID, the new LID and the LISK in its Sync feature.</p>
<h3>Website key update</h3>
<p>If the website worries its key may be compromised, it will rekey.
However, it must keep all past keys, and accept all of them,
so that users can authenticate even years after the last request.</p>
<p>(The main point of rekeying is to migrate users to a key
that is not compromised, such that they don’t run the risk of being
impersonated if the website has futher severe security failures.)</p>
<p>Once rekeyed, when the website receives an Auth request with an old key,
it authenticates the request with the corresponding key and accepts the request,
but responds with a new Key action, similar to a sign-up:</p>
<pre><code>WWW-Authenticate: Identity v1 Key
  kid=&quot;2021&quot;
  auid=&quot;VzX3h8VumdWIY7MiUCcYwnS8kz9DxdtFzQftFhLvkFkK&quot;
  lisk=&quot;zhgoQXVsATIUd-S2mB1gUlKi5yj_iO7K7KrsI_H8rBEK&quot;
</code></pre>
<p>When receiving this, the browser updates its KID and LISK in its Sync storage for the website.
It then uses the new LISK on future authentications.</p>
<p>As long as the website only performs the rekeying after they regained full access
and ensured that their TLS connections were not (or no longer) compromised,
this sensitive transmission of a LISK should not be eavesdropped.
After rekeying, they can therefore safely communicate to all customers
the need to log out and log back in.</p>
<h3>Browser export</h3>
<p>Browsers must provide a way to export the Browser Key to another browser.
It is recommended that the browser export format be encrypted with the user’s master password.
Additionally, any export event should be authenticated with a second factor.</p>
<p>From just the BK, the new browser can perform the Log In procedure on all websites.</p>
<h3>Account takeover or Browser Sync breach</h3>
<p>When a user’s BK is leaked, the website owner (if customer service detects an account takeover)
or browser (in the case of a breach of their Sync service)
will instruct the user to trigger the <strong>Browser Key Reset procedure</strong>.</p>
<p>The browser must have a button in its UI (for instance, in the Sync Preferences page) triggering the procedure:</p>
<p>First, it will create a new BK (say, <code>0dP_ocrzSwieAuLUNCD6P660HLLOGl9zyfxYwdSLI0kK</code>),
but keep the old BK around.</p>
<p>Then, for each website for which the user has a LISK associated to the old BK,
the browser will make a ReSignUp request, very similar to a LogIn request:</p>
<pre><code>Authorization: Identity v1 ReSignUp v1
  oauid=&quot;VzX3h8VumdWIY7MiUCcYwnS8kz9DxdtFzQftFhLvkFkK&quot;
  olip=&quot;R05PEuFZHCngevxsxJZsIDeJe66IDGYqoH3JBVtT-DcK&quot;
  auid=&quot;yFvfOjHW68qyhMIPobZdL6oZmIIOD7aEVquwkkbbxS4&quot;
  liv=&quot;yWPeXDGFi3q8ZAwVOAvbv5swl6oVoOScw7Y3CDVPQCM&quot;
</code></pre>
<ul>
<li><code>ReSignUp</code> is a new action to instruct the website to reset the UIDs everywhere where it matters, and provide a new LISK.</li>
<li><code>v1</code> means that the protocol used for the old IDs and tokens is v1. This is useful for the “Hash function theoretically broken” section.</li>
<li><code>VzX3…</code> is the old Authentication User ID (AUID).</li>
<li><code>R05P…</code> is the old Log-In Proof (LIP).</li>
<li><code>yFvf…</code> is a new Authentication User ID (AUID).</li>
<li><code>yWPe…</code>, is a new Log-In Verification token (LIV),
a MAC of the new AUID keyed with the new Log-In Proof (LIP).</li>
<li>The new LID is sent in the Date header (<code>Fri, 03 Jul 2020 16:03:26 GMT</code>).</li>
</ul>
<p>The website treats it just like a LogIn request, except it also updates the UID in database.</p>
<p>A Browser Sync breach would obviously be a major event.
In the old password world, it is equivalent to
having the worldwide Google Chrome passwords leaked. It would cause all Chrome users
to need to reset their passwords one by one on every website.</p>
<p>Thankfully, with WebIdentity, this can be automated by the browser seamlessly.</p>
<p>First, the browser will need to close the breach.
Then, for each user, it will automatically trigger the Browser Key Reset procedure remotely.</p>
<p>Obviously, just as with a Google Chrome password leak,
adversaries could take control of user accounts by doing a ReSignUp on their behalf.
WebIdentity is better in this respect: the browsers can automatically update information,
leaving a small window for attackers to take over accounts;
while a password leak may have users that take years to update a given password.</p>
<p>Just as with passwords, it is recommended that browsers implement Sync
in such a way that the user decypts Sync secrets on their machine
through the use of a master password.
As a result, the Sync servers would only contain encrypted data without the key.
Obviously, even a leak of the mere encrypted data should trigger a ReSignUp,
but at least the risk of user account takeover would be greatly reduced.</p>
<h3>Hash function theoretically broken</h3>
<p>It took ten years from SHA-1 publication to full-round collisions.
While SHA-2 has already survived twenty,
it is plausible that it gets eventually broken theoretically.
That was the motivation for the SHA-3 process,
which yielded a primitive seemingly likely to take even more time
than SHA-2 to get broken, thanks to its sponge construction.</p>
<p>When SHA-2 gets theoretically broken,
we will release v2 of the protocol.
Browser vendors and website operators will need to implement it
before it gets practically broken
(which for SHA-1 took ten years).</p>
<p>Websites supporting v2 must also support v1 for at least ten years,
which ought to be enough time for browser vendors to implement v2.</p>
<p>When browsers only support v1, and see support for v2 from a website,
they must send v1 requests, and the website must follow the v1 protocol.</p>
<p>When browsers implement v2 and hold a v1 authentication AUID/LISK,
they must follow the Browser Key Reset procedure.</p>
<h3>Threat models</h3>
<ul>
<li>Website attack:
<ul>
<li>From a third party:
<ul>
<li><strong>Replay attack</strong>: If they replay the encrypted request of an authenticated user within the 30-second window, they may trigger the corresponding action (eg. a bank transfer) twice. We recommend that websites implement idempotency checks, as this could also happen from network glitches.</li>
<li>If they get <strong>read access to the website</strong> database, the UID gives no information that can be used to authenticate on behalf of the user.</li>
<li>If they also compromise WK, the <strong>website key</strong>, they still lack the AUID (which is not stored) to be able to do anything.</li>
<li>If they compromise the <strong>website’s TLS encryption</strong>, such as with the <a href="https://blog.cloudflare.com/incident-report-on-memory-leak-caused-by-cloudflare-parser-bug/">CloudFlare memory leak</a>, they can read the encrypted payloads between the user and the website.
<ul>
<li>Reading the requests gives them a valid AUID/LID/TOTP set, but they only have a 30-second window (1 minute in rare worst-case clock drifts) to perform authenticated requests on behalf of the user, as they lack the LISK to be able to MAC new dates. They cannot issue a LogIn or ReSignUp request, lacking the LIP; and this remains true even if they additionally compromise the WK and database. Securitywise, this is a significant improvement compared to JWT, PASETO and similar session token approaches, which typically have anywhere from 5 minutes (for access tokens) to months of lifetime (for refresh tokens). An attacker reading a JWT refresh token in the encrypted payloads can typically use it to get unlimited access tokens for weeks if not ever. By contrast, with WebIdentity, the longest this attacker would be able to make authenticated queries is a minute, but usually half that (as most clients will not have much clock drift).</li>
<li>They can also read SignUp requests, although those will be rarer. The response includes the LISK, letting them fabricate valid TOTPs past 30 seconds. However, it will be invalidated through automatic logout after up to one hour. LISKs older than one hour will be useless to an attacker. On the other hand, if they can read the TLS traffic on-the-fly, they can view the new LISKs. As long as they maintain this ability, they can authenticate on behalf of the user. The flaw must be fixed by the website, and all LIDs invalidated, forcing a re-login.</li>
</ul>
</li>
<li>If they compromise both the <strong>website’s TLS encryption and its WK</strong>:
<ul>
<li>For each AUID/LID/TOTP they see in the encrypted traffic, if the LID is still valid, they can derive the current LISK, and with it, perform authenticated requests for up to one hour (after which the automatic logout will prevent that).</li>
<li>Similarly, they can get the LISK from SignUps and LogIns. If they can read the traffic on-the-fly, they can see the new LISKs produced even after the one-hour logout. Again, the solution is to fix the flaw and invalidate the LIDs.</li>
</ul>
</li>
</ul>
</li>
<li>From <strong>another website</strong>: knowledge of that website’s AUID is insufficient to guess other websites’ AUID (that requires knowing the BK), let alone the LISK (which requires that website’s WK).</li>
<li>From the <strong>user</strong>: knowledge of the LISK is insufficient to guess WK, the Website Key, and therefore, to make authenticated requests on behalf of other users of the website. Additionally, even if they could, knowledge of other users’ AUID would be necessary, which requires knowing their BK.</li>
<li>From the <strong>browser</strong>: since it has access to the Sync secrets, it can perform authenticated requests and account takeover for all its registered users. However, it cannot do so for users of other browsers, if their BK is not explicitly shared.</li>
</ul>
</li>
<li>Browser attack:
<ul>
<li><strong>XSS</strong>: Since WebIdentity is controlled by the browser and has no JS integration, JS code cannot access secrets or perform authentication. All the exchanges and headers related to WebIdentity must be hidden from the page transparently. All same-origin requests are authenticated or not depending on whether the user has clicked the Log In button, and depending on the <a href="https://fetch.spec.whatwg.org/#ref-for-concept-request-credentials-mode">credentials mode</a>. Cross-site requests comply with CORS. The Authorization and WWW-Authenticate headers already have the right protections in place.</li>
<li>Browsers should never have BK on the device. They can store the websites’ KID, AUID and LISK. An attacker that gains access to the <strong>device’s permanent or memory storage</strong> will be unable to obtain the BK, and therefore sign up on new websites. They can however make authenticated requests on behalf of the user to websites in which they are signed up, for up to one hour after they lose access. It is therefore necessary for browsers to encrypt the Sync database (with the LISK) if they cache it locally, which is already the case. They should not use an encryption key that is common to multiple users (also already the case IIUC).</li>
<li>The Operating System and the CPU, on the other hand, can obviously access the BK <strong>in memory</strong> and perform authenticated requests and account takeover on behalf of the user, but not of other users.</li>
<li><strong>BK loss</strong>: the Browser Sync could experience complete loss of data, including the BK, maliciously or accidentally. The consequence would be the same as a password manager, today, losing the passwords (which indeed is the main thing it wishes to guarantee as a business), or a website using WebAuthn as only primary authentication and the user losing their device (Yubico etc.): users would no longer be able to log in. However, people that switched browsers or backed up their BK would be able to add it back in using the <em>Browser Export</em> procedure.</li>
</ul>
</li>
</ul>
<h3>Cryptographic comments</h3>
<p>The whole scheme is both extremely inexpensive and simple to implement both for websites and browsers,
especially compared to current techniques (which involve, for instance, the expensive Argon2 for passwords).
The payload weigh is marginal.</p>
<p>It also does not make use of public-key cryptography,
which protects it from the threat of quantum computing advances.
The main impact might be longer hashes, although even that is in question.</p>
<p>The protocol is versioned in such a way that there is no cryptographic algorithm agility,
in line with common practices such as <a href="https://github.com/FiloSottile/age">age</a> and <a href="https://paragonie.com/files/talks/NoWayJoseCPV2018.pdf">PASETO</a>.</p>
<p>The MAC algorithm for v1 of the protocol is HMAC-SHA256.</p>
<p>(I would love to put BLAKE3 here, but some websites will object to a non-NIST-approved primitive.
And SHA3 (with no HMAC!) would also be nice, I would love to argue for its use;
but it is true that some websites may have legacy and dependency constraints;
and unlike WebAuthn, the goal of WebIdentity is to quickly get very widespread adoption
as the primary authentication mechanism on the Web.)</p>
<p>All <a href="https://tools.ietf.org/html/rfc4648#section-5">base64url</a> inputs must be converted to a byte buffer prior to use.
The implementation should be constant-time.</p>
<p>The eTLD+1 must be in ASCII punycode form for use in WebIdentity (simplifying debugging).</p>
<h2>Vectors</h2>
<p>The examples use:</p>
<ul>
<li>Website eTLD+1: <code>example.org</code>.</li>
<li>BK: <code>GVr2rsMpdVKNMYkIohdCLhOeHSBIL8KBjoCvleDbsJsK</code> (generated with <code>head -c 32 &lt;/dev/urandom | base64 | tr +/ -_</code>).</li>
<li>WK: <code>DCmk1xzu05QmT578_9QUSckIjCYRyr19W0bf0bMb46MK</code>.</li>
<li>MACs generated with <code>echo -n &quot;$input&quot; | openssl sha256 -hmac &quot;$key&quot; | cut -f2 -d' ' | xxd -r -p | base64 | tr +/ -_ | tr -d =</code>.</li>
</ul>
<p>The script to generate the examples is available <a href="https://github.com/espadrine/espadrine.github.com/blob/master/blog/assets/webidentity/test-vectors.sh">here</a>;
running it yields all values used in examples.</p>
<h2>Acknowledgements</h2>
<p>Thanks go to /u/josejimeniz2 for considering the risk of Sync data loss,
and to /u/haxelion for raising the risk of having the BK on the device
(which is no longer the case in the current draft).</p>
<h2>Comments and links</h2>
<p><a href="https://www.reddit.com/r/espadrine/comments/hlrx40/webidentity_oneclick_passwordless_signups_logins/">Blog comments here</a>.</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2020-07-05T20:19:02Z",
  "keywords": "crypto, web" }
</script> ]]>
        </content>
      </entry>
      <entry>
        <id>https://espadrine.github.io/blog/posts/memorable-passwords.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/memorable-passwords.html"/>
        <title>Memorable passwords</title>
        <published>2020-06-24T19:50:27Z</published>
        <category term="crypto"/>
        <content type="html">
          <![CDATA[ <h1>Memorable passwords</h1>
<p>We are slowly getting to a comfortable password situation.</p>
<p>Research has improved on which passwords are easier to remember.
Cryptographers have <a href="https://password-hashing.net/argon2-specs.pdf">strenghtened the cost</a> of cracking weak passwords.
People are more aware of the security risks,
and the usage of password managers grows.</p>
<p>The consensus on password handling is this:</p>
<ol>
<li>Keep a very strong master password in your head, stored nowhere.</li>
<li>Use it to unlock your password manager.</li>
<li>Use your password manager to store and create very random passwords for individual websites.
You would never be able to remember them, but you only need to remember the master password.
Typically, for alphanumerical outputs, you need ⌈128÷log2(26·2+10)⌉ = 22 characters.</li>
<li>The websites, and more importantly, the password manager,
use a key derivation function such as <a href="https://password-hashing.net/argon2-specs.pdf">Argon2</a> either on the front-end
(server relief) or on the backend, and only stores the output.
It ensures computation is both time-hard and memory-hard, with settings kept up-to-date
to ensure that each computation takes 0.5 seconds and/or 4 GB of RAM.</li>
</ol>
<p>But some details are left unset: exactly how strong should the master password be?
How do we even know?
Can this situation converge to an easier user experience for login on the Web?</p>
<h2>Password hashing</h2>
<p>Some accurate statements may be surprising to the general population.
This is one:</p>
<p><strong>Multiple passwords can unlock your account.</strong></p>
<p>The reason? Your password is not compared byte-for-byte (thankfully!)
but through a hashing method that does not map one-to-one.</p>
<p>Indeed, hashes have fixed sizes (typically 256 bits),
while passwords have arbitrary length.</p>
<p>Overall, this consideration is unimportant,
because virtually no password is strong enough
to even compete with the collision risk of the hash:
it is tremendously more likely for a collision to be caused by
the generation process, than by the hash,
whose collision risk is 2<sup>N÷2</sup>
where N is the size of the hash, typically 256 bits nowadays.</p>
<p>On top of this, some companies build their login system
in a way that is more resilient to user error,
such as <a href="https://www.zdnet.com/article/facebook-passwords-are-not-case-sensitive-update">having caps lock on</a>.</p>
<p>That too is irrelevant, since the search space is typically only reduced
by one bit (corresponding to the choice between setting caps lock or not).</p>
<h2>Target strength</h2>
<p><a href="https://crypto.stackexchange.com/questions/60815/recommended-minimum-entropy-for-online-passwords-in-2018">Some suggestions target specific cryptographic algorithms</a>.
But this pushes machine limits into human constraints:
algorithms require 128-bit security, not because 127 is not enough,
but because it is a power of two that neatly fits with various engineering techniques.</p>
<p>The real human constraint is your lifetime.
Once you are dead, it does not matter too much to your brain whether your secrets are out,
since your brain becomes mulch.</p>
<p>The longest person alive is a French woman that died nearly reaching 123.
Let’s imagine that health will improve
such that someone will live double that amount, Y = 246 years.
What is the minimum strength needed to ensure they won’t have their secrets cracked alive?</p>
<p>Current compute costs hover around €3/month on low-end machines.
Let’s imagine that it will improve a hundredfold in the coming century.</p>
<p>The NSA yearly budget is estimated at B = €10 billion.
Can they hack you before you die?</p>
<p>First, under those assumptions,
assuming the NSA consumes its whole budget cracking you,
how many computers will it use to crack you in parallel?
The result is P = B ÷ 12 ÷ 0.03 = 28 billion servers.</p>
<p>If your password has an N-bit entropy,
it will take 2<sup>N-1</sup>·0.005÷P÷3600÷24÷365 years on average,
assuming the NSA is brute-forcing with CPUs that can do one attempt every 5 milliseconds
(a hundredth of the <a href="https://password-hashing.net/argon2-specs.pdf">Argon2</a> recommended setting,
to account for the possibility that the NSA has machines a hundred times more powerful
than the rest of us, which is both unlikely, and would not cost what we estimated).</p>
<p>As a result, our formula for picking strength is
N = log2(B÷12÷0.03 · Y·365·24·3600÷0.005) + 1 = 77 bits of security.</p>
<p>Note that we can assume that a good KDF is used,
since we are only worried about password strength for the password manager,
which should be pretty good at choosing the right design.
The password manager will generate all normal passwords above 128 bits of security anyway.
(Except for those pesky websites that inexplicably have an upper password length limit.
But those are beyond saving.)</p>
<p>I parameterized some values so that you can plug your own situation.
For instance, if you make a password for your startup
that you believe will beat the odds of an average 5-year lifespan,
and become a behemoth a thousand years into the future, you can set Y = 1000
and get a very slight increase to 79 bits.</p>
<p>If you instead believe that your adversary will spend a trillion euros every year,
you can bump things up to 83 bits of security.</p>
<h2>Master password generation</h2>
<p>How do you convert a number of bits of security into a master password?
Well, those bits represent the amount of entropy of the random generator.
Or in other words, the quantity of uncertainty of the password-making process.</p>
<p>Each bit represents one truly random choice between two options.
If you have four options, it is as if you made two choices, and so on.</p>
<p>A good way to make memorable master passwords is to pick words among large dictionaries,
since picking from a long list adds a lot of entropy (since there are so many binary choices)
but each word is very distinctively evocative.</p>
<p>However, each word is independent, and therefore,
making stories in your head that combines those words gets harder the more words there are.
So we randomize the word separators as symbols,
which both adds entropy (so that we can have less words),
and is not too hard to remember. Besides, breaking words apart ensures that
we don’t lose entropy by ending up with two words that, concatenated,
are actually a single word from the same dictionary.</p>
<p>I implemented these principles on <a href="https://espadrine.github.io/passphrase/">this passphrase generation page</a>.</p>
<h2>Thank you, Next</h2>
<p>I feel strongly that passwords are passé.
I would love to talk about my hopes for the future of Web authentication.</p>
<p><a href="https://www.reddit.com/r/programming/comments/hf63bp/generate_cryptographically_secure_passphrases_at/">Reddit comments here</a>.
<a href="https://news.ycombinator.com/item?id=23632533">HN comments here</a>.</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2020-06-24T19:50:27Z",
  "keywords": "crypto" }
</script> ]]>
        </content>
      </entry>
      <entry>
        <id>https://espadrine.github.io/blog/posts/shishua-the-fastest-prng-in-the-world.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/shishua-the-fastest-prng-in-the-world.html"/>
        <title>SHISHUA: The Fastest Pseudo-Random Generator In the World</title>
        <published>2020-04-18T16:59:00Z</published>
        <category term="prng"/>
<category term="crypto"/>
        <content type="html">
          <![CDATA[ <h1>SHISHUA: The Fastest Pseudo-Random Generator In the World</h1>
<p><em>(TLDR: see the <a href="#benchmark">benchmark</a> and the <a href="https://github.com/espadrine/shishua">code</a>.)</em></p>
<p>Six months ago, I wanted to make the best PRNG with an unconventional design,
whatever that design may be.
I expected it to start easy, and slowly get harder;
I wondered whether I would learn fast enough to pass the highest bar.</p>
<p>Surprisingly, difficulty did not increase linearly.
Passing the bytewise Chi-Squared tests was very hard!</p>
<p>Then, when I got the concepts, passing dieharder was also very hard.
When I got to that point, I was honestly so extatic,
that <a href="https://mobile.twitter.com/espadrine/status/1184542865969614849">I published what I got</a> to learn what the next challenge needed to be.
But it turned out <a href="https://mobile.twitter.com/espadrine/status/1184883565634424832">it failed PractRand</a>.</p>
<p>Then, <a href="https://mobile.twitter.com/espadrine/status/1186358084425400320">passing BigCrush</a> was very hard.</p>
<p>Then, passing 32 tebibytes of PractRand was very hard.</p>
<p>But once I reached that point, I realized that speed was going to be an issue.
It wasn’t just about having a construction that emitted ten megabytes a second, taking a month to pass PractRand.</p>
<p>But I have to admit, <a href="https://github.com/espadrine/combit">passing PractRand at a gigabyte a second</a> was very hard.</p>
<p>Once you get there… what you really want to see is whether you can reach the Pareto frontier.</p>
<p>You want the fastest PRNG in the world that beats the hardest statistical tests.</p>
<p>I got there.</p>
<p>In <a href="https://espadrine.github.io/blog/posts/a-primer-on-randomness.html">the previous entry to the series</a>, I explained all the things I learnt to reach it.
Here, I’ll detail how the winning design works.</p>
<h2>Target</h2>
<p>Let’s start with the obvious: <strong>speed is platform-dependent</strong>.
I focused my optimization on the modern x86-64 architecture (so, Intel and AMD chips).</p>
<p>The classic metric used to compare performance there is <strong>cpb</strong>:
the number of CPU cycles spent to generate a byte of output.
All cryptographic papers <a href="https://bench.cr.yp.to/supercop.html">compute and compare that metric</a>.
A slightly lower cpb, in software or hardware, can weigh in the balance
just enough to make a primitive win a competition,
or become widely used by the major websites of the world.</p>
<p>To improve your cpb, you can do three things:</p>
<ol>
<li>Generate more bytes for the same amount of work, or</li>
<li>Do less work to generate the same amount of bytes, or</li>
<li>Parallelize work.</li>
</ol>
<p>We will do all of the above.</p>
<p>Therefore, to boot with point 1, we need to output more bits on each iteration.</p>
<p>I am worried that people might say,
“this is not a PRNG unless it outputs 32-bit numbers,” or “64-bit numbers”.
Or more generally, “PRNGs must only rely on this subset of x86-64”;
as if some instructions, such as <code>POPCNT</code>, or some registers, such as <code>%xmm7</code>, are off-limits.</p>
<p>But PRNGs are engineering: they try to make the best of the CPU, decade after decade!
They relied on <code>ROL</code> when it came, and on <code>%rax</code> when 64-bit CPUs landed.
Sure, it means that this algorithm might be slower on ARM (although that remains to be seen);
but 64-bit PRNGs were heavily used before 2019’s Android switch to a required 64-bit support!</p>
<p>So things evolve with the hardware.
And today, Intel and AMD CPUs support 256-bit operations through <a href="https://software.intel.com/en-us/articles/how-intel-avx2-improves-performance-on-server-applications">AVX2</a>.</p>
<p>Just like RC4 outputs 1 byte, and drand48 can only output 4 at a time;
just like pcg64 can only output 8 at a time;
we will output 32 bytes at a time.</p>
<p>Obviously, while 8 bytes could be output as a 64-bit number,
which most programming languages have a built-in type for,
few have a type for 16 bytes (C’s <a href="https://gcc.gnu.org/onlinedocs/gcc/_005f_005fint128.html"><code>__uint128_t</code></a> being a notable exception);
fewer yet have one for 32 bytes (aside from intrinsics).</p>
<p>So we must say goodbye to the typical PRNG function prototype
(here taken from Vigna’s <a href="http://xoshiro.di.unimi.it/hwd.c">HWD</a> benchmark program):</p>
<pre><code>static uint64_t next(void);
</code></pre>
<p>Instead, we can have the generator take a buffer to fill
(here taken from <a href="https://github.com/espadrine/shishua/blob/master/prng.c">my own benchmark program</a>):</p>
<pre><code>void prng_gen(prng_state *s, __uint64_t buf[], __uint64_t size);
</code></pre>
<p>Are there disadvantages?</p>
<p>Well, if your generator outputs 32 bytes at a time,
you need the consumer to give an array that is a multiple of 32 bytes;
ideally, an array aligned to 32 bytes.</p>
<p>Although, with a tiny bit more work, you don’t.
Just fill a buffer. Output from it what has not been consumed;
refill it as needed.</p>
<p>That does make <em>latency</em> unpredictable: some calls will only read the buffer.
But it averages out the same.</p>
<p>So now we generate more bytes for the same amount of work.
Next step: how do we parallelize work?</p>
<h2>Parallelism</h2>
<p>The CPU offers an incredible wealth of parallelism at every level.</p>
<p>First, of course, are the SIMD instructions (Single-Instruction, Multiple Data).
For instance, AVX2 does four 64-bit additions in parallel, or eight 32-bit ones, etc.</p>
<p>In cryptography, it has been severely relied upon for fifteen years.
Notably, <a href="https://github.com/floodyberry/supercop/tree/master/crypto_stream/chacha20/dolbeau/amd64-avx2">ChaCha20</a> gains an incredible amount of speed from it;
most important primitives that don’t use AESNI rely on that.
For instance, <a href="https://norx.io/data/norx.pdf">NORX</a> and <a href="https://cryptojedi.org/papers/gimli-20170627.pdf">Gimli</a> are designed with that in mind.</p>
<p>Recently, there has been increasing interest in the non-cryptographic PRNG community.</p>
<p>In particular, existing primitives not designed for SIMD can be the basis
for building a very fast PRNG.</p>
<p>For instance, Sebastiano Vigna, while pushing for his <a href="http://prng.di.unimi.it/#speed">xoshiro256++</a> design
in the Julia programming language’s standard library,
<a href="https://github.com/JuliaLang/julia/issues/27614#issuecomment-548154730">learnt</a> that concatenating the output of eight concurrent instances of the PRNG
initialized differently, was made very fast by having each operation of the design
performed simultaneously on each PRNG.</p>
<p>SIMD is one level of CPU parallelism, but not the only one.
I encourage you to read <a href="https://espadrine.github.io/blog/posts/a-primer-on-randomness.html">the previous article on the subject</a>
to get a better picture, but I’ll mention what I relied upon.</p>
<p><strong>CPU pipelining</strong> processes multiple instructions at different stages of processing.
When well-ordered to limit interstage dependencies, instructions can be processed faster.</p>
<p><strong>Superscalar execution</strong> makes the computation part of instruction happen in parallel.
But they must have no read/write dependencies to do so.
We can fit the design to reduce the risk of stalls,
by making the write part happen long before the read.</p>
<p><strong>Out-of-order execution</strong> lets the processor execute instructions that happen later,
even though a previous instruction is not yet done, if the later instruction has no
read/write dependency to it.</p>
<p>All right, let’s dig our hands into the implementation!</p>
<h2>Design</h2>
<p>Let’s walk through the design of something we will call SHISHUA-half,
for reasons that will slowly become obvious along the article.</p>
<p>It looks like this:</p>
<p><img src="../assets/shishua-the-fastest-prng-in-the-world/shishua-diagram.svg" alt="SHISHUA diagram" /></p>
<p>Let’s dive in line by line.</p>
<pre><code class="language-c">typedef struct prng_state {
  __m256i state[2];
  __m256i output;
  __m256i counter;
} prng_state;
</code></pre>
<p>Our state is cut in two pieces that both fit in an AVX2 register (256 bits).
We keep output around in the state to get a bit of speed,
but it is not actually part of the state.</p>
<p>We also have a 64-bit counter; it is also an AVX2 register to ease computation.
Indeed, AVX2 has a bit of a quirk where regular registers (<code>%rax</code> and the like)
cannot directly be transfered to the SIMD ones with a <code>MOV</code>;
it must go through RAM (typically the stack), which costs both latency and
two CPU instructions (<code>MOV</code> to the stack, <code>VMOV</code> from the stack).</p>
<p>We’re now going to look at generation.
We start by loading everything, then we loop over the buffer,
filling it up by 32 bytes at each iteration.</p>
<pre><code class="language-c">inline void prng_gen(prng_state *s, __uint64_t buf[], __uint64_t size) {
  __m256i s0 = s-&gt;state[0], counter = s-&gt;counter,
          s1 = s-&gt;state[1],       o = s-&gt;output;
  for (__uint64_t i = 0; i &lt; size; i += 4) {
    _mm256_storeu_si256((__m256i*)&amp;buf[i], o);
    // …
  }
  s-&gt;state[0] = s0; s-&gt;counter = counter;
  s-&gt;state[1] = s1; s-&gt;output  = o;
}
</code></pre>
<p>Since the function is inlined, the buffer being immediately filled at the start
lets the CPU execute the instructions that depend on it in the calling function right away,
through out-of-order execution.</p>
<p>Inside the loop, we perform three operations on the state in rapid succession:</p>
<ol>
<li><strong>SHI</strong>ft</li>
<li><strong>SHU</strong>ffle</li>
<li><strong>A</strong>dd</li>
</ol>
<p>Hence the name, SHISHUA!</p>
<h3>First, the shift</h3>
<pre><code class="language-c">u0 = _mm256_srli_epi64(s0, 1);              u1 = _mm256_srli_epi64(s1, 3);
</code></pre>
<p>AVX2 does not support rotations, sadly.
But I want to entangle bits from one position in the 64-bit numbers,
to other bit positions! And shift is the next best thing for that.</p>
<p>We must shift by an odd number so that each bit reaches all 64-bit positions,
and not just half.</p>
<p>Shift loses bits, which removes information from our state.
That is bad, so we minimize the loss: the smallest odd numbers are 1 and 3.
We use different shift values to increase divergence between the two sides,
which should help lower the similarity of their self-correlation.</p>
<p>We use rightward shift because the rightmost bits have the least diffusion in addition:
the low bit of <code>A+B</code> is just a XOR of the low bits of <code>A</code> and <code>B</code>, for instance.</p>
<h3>Second, the shuffle</h3>
<pre><code class="language-c">t0 = _mm256_permutevar8x32_epi32(s0, shu0); t1 = _mm256_permutevar8x32_epi32(s1, shu1);
</code></pre>
<p>We use a 32-bit shuffle because it is the only one that is both a different granularity
than the 64-bit operations that we do everywhere else (which breaks 64-bit alignment),
and that can also cross lanes
(other shuffles can only move bits within the left 128 bits if they started on the left,
or within the right 128 bits if they started on the right).</p>
<p>Here are the shuffle constants:</p>
<pre><code class="language-c">__m256i shu0 = _mm256_set_epi32(4, 3, 2, 1, 0, 7, 6, 5),
        shu1 = _mm256_set_epi32(2, 1, 0, 7, 6, 5, 4, 3);
</code></pre>
<p>To make the shuffle really strenghten the output, we move weak (low-diffusion) 32-bit parts
of the 64-bit additions to strong positions, so that the next addition will enrich it.</p>
<p>The low 32-bit part of a 64-bit chunk never moves to the same 64-bit chunk as its high part.
That way, they do not remain in the same chunk, encouraging mixing between chunks.</p>
<p>Each 32-bit part eventually reaches all positions circularly: A to B, B to C, … H to A.</p>
<p>You might notice that the simplest shuffle that follows all those requirements
are simply those two 256-bit rotations (rotation by 96 bits and 160 bits rightward, respectively).</p>
<h3>Third, the addition</h3>
<p>Let’s add 64-bit chunks from the two temporary variables,
the shift one and the shuffle one, together.</p>
<pre><code class="language-c">s0 = _mm256_add_epi64(t0, u0);              s1 = _mm256_add_epi64(t1, u1);
</code></pre>
<p>The addition is the main source of diffusion: it combines bits
into irreducible combinations of XOR and AND expressions across 64-bit positions.</p>
<p>Storing the result of the addition in the state keeps that diffusion permanently.</p>
<h3>Output function</h3>
<p>So, where do we get the output from?</p>
<p>Easy: the structure we built is laid out in such a way that
we are growing two independent pieces of state: <code>s0</code> and <code>s1</code>,
which never influence each other.</p>
<p>So, we XOR them, and get something very random.</p>
<p>In fact, to increase the independence between the inputs that we XOR,
we take the partial results instead: the shifted piece of one state,
and the shuffled piece of the other.</p>
<pre><code>o = _mm256_xor_si256(u0, t1);
</code></pre>
<p>That also has the effect of reducing the read/write dependencies between superscalar CPU instructions,
as <code>u0</code> and <code>t1</code> are ready to be read before <code>s0</code> and <code>s1</code> are.</p>
<p>You may have noticed that we did not talk about the counter yet.
It turns out we handle it at the start of the loop.
We first change the state, and then increment the counter:</p>
<pre><code class="language-c">s1 = _mm256_add_epi64(s1, counter);
counter = _mm256_add_epi64(counter, increment);
</code></pre>
<p>The reason we change the state first, and then update the counter,
is so that <code>s1</code> becomes available sooner,
reducing the risk that later instructions that will read it get stalled
in the CPU pipeline.
It also avoids a direct read/write dependency on the counter.</p>
<p>The reason we apply the counter to s1 and not s0,
is that both affect the output anyway.
However, <code>s1</code> loses more bits from the shift,
so this helps it get back on its feet after that harmful shearing.</p>
<p>The counter is not necessary to beat PractRand.
Its only purpose is to set a lower bound of 2<sup>69</sup> bytes = 512 EiB
to the period of the PRNG:
we only start repeating the cycle after one millenia at 10 GiB/s,
which is unlikely to ever be too low for practical applications in the coming centuries.
Thanks to this, there are no bad seeds.</p>
<p>Here are the increments:</p>
<pre><code class="language-c">__m256i increment = _mm256_set_epi64x(1, 3, 5, 7);
</code></pre>
<p>The increments are picked as odd numbers,
since only coprimes of the base cover the full cycle of the finite field GF(2<sup>64</sup>),
and all odd numbers are coprime of 2.</p>
<p>(In other words, if you increment by an even number between integers 0 to 4,
wrapping around to 0 when you go past 4,
you get the sequence 0-2-0-2-…, which never outputs 1 or 3;
but an odd increment goes through all integers.)</p>
<p>We use a different odd number of each 64-bit number in the state,
which makes them diverge more, and adds a tiny bit of stirring.</p>
<p>I picked the smallest odd numbers so that they don’t look like magic numbers.</p>
<p>So, there we go! That is how the state transition and output function work.</p>
<p>Now, how do we initialize them?</p>
<h3>Initialization</h3>
<p>We initialize the state with the hex digits of Φ,
the irrational number that is least approximable by a fraction.</p>
<pre><code class="language-c">static __uint64_t phi[8] = {
  0x9E3779B97F4A7C15, 0xF39CC0605CEDC834, 0x1082276BF3A27251, 0xF86C6A11D0C18E95,
  0x2767F0B153D27B7F, 0x0347045B5BF1827F, 0x01886F0928403002, 0xC1D64BA40F335E36,
};
</code></pre>
<p>We take a 256-bit seed, which is common in cryptography,
and doesn’t really hurt in non-cryptographic PRNGs:</p>
<pre><code class="language-c">prng_state prng_init(SEEDTYPE seed[4]) {
  prng_state s;
  // …
  return s;
}
</code></pre>
<p>We don’t want to override a whole piece of state (<code>s0</code> nor <code>s1</code>) with the seed;
we only want to affect half.
That way, we avoid having debilitating seeds that,
purposefully or accidentally, set the state to a known weak start.</p>
<p>With half of each state intact, they still keep control over 128 bits of state,
which is enough entropy to start and stay strong.</p>
<pre><code class="language-c">s.state[0] = _mm256_set_epi64x(phi[3], phi[2] ^ seed[1], phi[1], phi[0] ^ seed[0]);
s.state[1] = _mm256_set_epi64x(phi[7], phi[6] ^ seed[3], phi[5], phi[4] ^ seed[2]);
</code></pre>
<p>Then we do the following thing a <code>ROUNDS</code> number of times:</p>
<ol>
<li>Run <code>STEPS</code> iterations of SHISHUA,</li>
<li>Set one piece of the state to the other, and the other to the output.</li>
</ol>
<pre><code class="language-c">for (char i = 0; i &lt; ROUNDS; i++) {
  prng_gen(&amp;s, buf, 4 * STEPS);
  s.state[0] = s.state[1];
  s.state[1] = s.output;
}
</code></pre>
<p>Setting to the output increases the diffusion of the state.
In the initialization, the added work and state correlation don’t matter,
since this is only done a few times, once.
You only care about diffusion in initialization.</p>
<p>I picked values of 5 for <code>STEPS</code> and 4 for <code>ROUNDS</code>
after looking at how much they impacted seed correlation.</p>
<p>(I computed seed correlation by counting the “unusual” and “suspicious” anomalies
coming out of the PractRand PRNG quality tool.)</p>
<h2>Performance</h2>
<p>Speed measurement benchmarks are tricky for so many reasons.</p>
<ul>
<li><strong>Clock</strong> measurements can lack precision.</li>
<li>The CPU has so much <strong>parallelism</strong>, that tracking when instructions start and end,
is both nondeterministic and heavily dependent on other events on the CPU.</li>
<li>Obviously, from one CPU vendor to the next, the resuts will be different.
That is also true from one CPU <strong>series</strong> to the next from the same vendor.</li>
<li>CPUs nowadays have <strong><a href="https://www.intel.com/content/www/us/en/architecture-and-technology/turbo-boost/turbo-boost-technology.html">variable frequency</a></strong>: they get purposefully slower or faster
depending on the need for low power consumption or the risk of high temperature.</li>
</ul>
<p>I use a dedicated CPU instruction, <code>RDTSC</code>, which computes the number of cycles.</p>
<p>To make sure that everyone can reproduce my results, I use a cloud virtual machine.
It doesn’t change the order of the benchmark results compared to a local test;
it also avoids requesting that other people buy the same computer as the one I have.
Finally, there are many use-cases where PRNGs would be used in the cloud on those instances.</p>
<p>I chose Google Cloud Platform’s N2 (Intel chip) and N2D (AMD chip).
The advantage of GCP is that they have chips from both vendors.
We’ll focus on Intel here, but the orders of magnitude are similar for AMD.</p>
<p>To give a bit of context, let’s first look at an old cryptographic generator, RC4.
Impossible to parallelize; I got <strong>7.5 cpb</strong> (cycles spent per generated byte).</p>
<p>Now, let’s look at a very common and fast MCG: <a href="https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/">Lehmer128</a>,
the simplest PRNG that passes BigCrush: <strong>0.44 cpb</strong>. Wow, not bad!</p>
<p>For kicks, let’s make another detour through modern cryptographic designs.
They rely on a lot of the tricks that we saw.
Take ChaCha8 for instance.
It reaches… <strong>0.46 cpb</strong>! About the same as the really fast one we just saw!</p>
<p>SIMD really works its magic!</p>
<p>To the cryptographic community, <a href="https://twitter.com/hashbreaker/status/1023965175219728386">this is not a complete surprise</a>.
ChaCha8 is just insanely easy to parallelize.
It is just a counter in a diffused state, well-hashed.</p>
<p>Next, a recent mixer that is the basis for fast hash tables: <a href="https://github.com/wangyi-fudan/wyhash/blob/master/wyhash_v6.h">wyrand</a>.
<strong>0.41 cpb</strong>, slightly better!</p>
<p>Among Vigna’s fast PRNG, some don’t pass 32 TiB of PractRand, but are very fast.
<a href="http://prng.di.unimi.it/xoshiro256plus.c">Xoshiro256+</a> fails at 512 MiB but is among the fastest of the bunch: <strong>0.34 cpb</strong>.</p>
<p>Let’s look at a recent entry, from earlier this year: <a href="http://www.romu-random.org/">RomuTrio</a>.
It claims the title of fastest PRNG in the world: <strong>0.31 cpb</strong>.</p>
<p>Alright, enough. How does SHISHUA-half fare?</p>
<p><strong>0.14 cpb</strong>. Twice as fast as RomuTrio.</p>
<p><img src="../assets/shishua-the-fastest-prng-in-the-world/speed-partial.svg" alt="Speed plot" /></p>
<p>Given its quality, it is unmatched.</p>
<p>But remember how the Julia team looked at
combining multiple instances of Vigna’s design
to make a fast SIMD PRNG?
Let’s look at Vigna’s fastest result using this technique:
<a href="http://prng.di.unimi.it/#speed">Xoshiro256+ 8 times</a>. <strong>0.07 cpb</strong>!</p>
<p>(Technically, it varies on the machine;
on my laptop, SHISHUA-half is faster than this.)</p>
<hr />
<p>Sure, the resulting meta-PRNG (which I dub Xoshiro256+x8)
has <em>terrible statistical biases</em> that fail many simple tests.</p>
<p>But, let’s beat its speed anyway, without betraying our high quality standards.</p>
<p>Now you probably guess why we called our earlier primitive SHISHUA-half.</p>
<p>It turns out getting twice as fast is easy by doubling SHISHUA-half.</p>
<p>Similar to the Julia insights, we have two PRNGs initialized differently
(four blocks of 256-bit state),
outputting their thing one after the other.</p>
<p>But with more state, we can output even more stuff,
by combining the four states pairwise:</p>
<pre><code class="language-c">o0 = _mm256_xor_si256(u0, t1);
o1 = _mm256_xor_si256(u2, t3);
o2 = _mm256_xor_si256(s0, s3);
o3 = _mm256_xor_si256(s2, s1);
</code></pre>
<p>And that is how you get SHISHUA, and its <strong>0.06 cpb</strong> speed.</p>
<p>Five times faster than the previously-fastest in the world
that passes 32 TiB of PractRand.
You can barely see it in the graph, so I removed RC4.</p>
<p><img src="../assets/shishua-the-fastest-prng-in-the-world/speed.svg" alt="Speed plot" /></p>
<p>I guess my point is that it is somewhat competitive.</p>
<p>(In fact, it is even faster on my laptop, at 0.03 cpb,
but I want to stick to my benchmark promises.
Maybe we lose a tiny bit of performance on early AVX-512 CPUs.)</p>
<p>Hopefully, SHISHUA stays the fastest in the world for at least a few weeks?
(Please make it so.)</p>
<h2>Quality</h2>
<p>It passes BigCrush and 32 TiB of PractRand without suspicion.</p>
<p>In fact, all of its four outputs do.</p>
<p>One of the not-ideal aspects of the design is that SHISHUA is <strong>not reversible</strong>.</p>
<p>You can see this with a reduction to a four-bit state, with <code>s0 = [a, b]</code> and <code>s1 = [c, d]</code>.
The shift will yield <code>[0, a]</code> and <code>[0, d]</code>; the shuffle will give <code>[b, c]</code> and <code>[d, a]</code>.</p>
<p>The new <code>s0</code> is <code>[b, c] + [0, a] = [b⊕(a∧c), a⊕c]</code>, and <code>s1</code> is <code>[d, a] + [0, c] = [d⊕(a∧c), a⊕c]</code>.</p>
<p>If <code>a = ¬c</code>, then <code>a⊕c = 1</code> and <code>a∧c = 0</code>, thus <code>s0 = [b, 1]</code> and <code>s1 = [d, 1]</code>.
So there are two combinations of <code>a</code> and <code>c</code> that give the same final state.</p>
<p>It is not an issue in our case, because the 64-bit counter is also part of the state.
So you have a minimum cycle of 2⁷¹ bytes (128 bytes per state transition),
which lasts seven millenia at 10 GiB/s.
So that counterbalances the lost states.</p>
<p>Besides, even despite the irreversibility,
the average state transition period is <code>2^((256+1)÷2)</code>.
That gives an average cycle of 2¹³⁵ bytes
(more than a trillion times the age of the universe to reach at 10 GiB/s).
Although, in my opinion, average cycles are overrated,
as they give no indication on the quality of the output.</p>
<p>Alright, here is the distilled benchmark:</p>
<table id=benchmark>
  <tr><th>Name   <th>Performance <th>Quality <th>Seed correlation
  <tr><td>SHISHUA       <td>0.06 <td>>32 TiB <td> >32 TiB
  <tr><td>xoshiro256+x8 <td>0.07 <td>  1 KiB <td>   0 KiB
  <tr><td>RomuTrio      <td>0.31 <td>>32 TiB <td>   1 KiB
  <tr><td>xoshiro256+   <td>0.34 <td>512 MiB <td>   1 KiB
  <tr><td>wyrand        <td>0.41 <td>>32 TiB <td>  32 KiB
  <tr><td>Lehmer128     <td>0.44 <td>>32 TiB <td>   1 KiB
  <tr><td>ChaCha8       <td>0.46 <td>>32 TiB?<td> >32 TiB?
  <tr><td>RC4           <td>8.06 <td>  1 TiB <td>   1 KiB
</table>
<ol>
<li><strong>Performance</strong>: in number of CPU cycles spent per byte generated,
on N2 GCP instances. On N2D (AMD), the order is the same.</li>
<li><strong>Quality</strong>: level at which it fails PractRand. We show a <code>&gt;</code> if it did not fail.
We put a question mark if we have not proved it.</li>
<li><strong>Seed correlation</strong>: PractRand on interleaving of bytes from eight streams
with seeds 1, 2, 4, 8, 16, 32, 64, 128.
We use PractRand with folding 2 and expanded tests.</li>
</ol>
<p>Speed measurement is traditionally in cpb.
Given the speed we get to nowadays,
a more appropriate measurement is in number of bits generated per CPU cycle.
Not only do I find it easier to grasp,
it is also much easier to compare huge differences on the graph:</p>
<p><img src="../assets/shishua-the-fastest-prng-in-the-world/speed-total.svg" alt="Speed plot" /></p>
<h2>Next</h2>
<p>While there are no practical issue with irreversibility in our case,
it also means that we can improve on SHISHUA.</p>
<p>My ideal PRNG would have the following properties:</p>
<ol>
<li><strong>The state transition is a circular permutation</strong>, giving a way-more-than-enough 2¹⁰²⁴ bytes cycle.
As in, it would take more than 10²⁸² times the age of the universe to reach the end at 10 GiB/s,
instead of SHISHUA’s seven millenia.
It is not exactly “better” (impossible is impossible);
but if we can reduce the design to a smaller state without affecting diffusion,
we might be able to get a faster PRNG.
Do you think we might be able to fit one in ARM’s 128-bit NEON registers?
Also, we would no longer need the counter, removing two additions.</li>
<li><strong>The output function is provably irreversible</strong>.
The way SHISHUA XORs two independent numbers already has that property,
but I haven’t proved that the numbers are truly decorrelated.</li>
<li><strong>The state initialization is irreversible</strong>
with each state having 2¹²⁸ possible seeds (to prevent guessing the seed).
The way SHISHUA sets the state to its own output is likely irreversible.
After all, it uses SHISHUA’s state transition (partially irreversible)
and its output function (seemingly irreversible, see point 2).</li>
<li><strong>The state initialization has perfect diffusion</strong>:
all seed bits affect all state bits with equal probability.
I’d like to compute that for SHISHUA.</li>
</ol>
<p>One issue holding back PRNGs and cryptography overall is the lack of better, general-purpose tooling.</p>
<p>I want a tool that can instantly give me an accurate score,
allowing me to compare designs on the spot.</p>
<p>PractRand is great compared to what came before it; but:</p>
<ul>
<li>It cannot rate high-quality generators, making comparisons between them impossible.
We just get to say “well, they both had no anomalies after 32 TiB…”</li>
<li>It takes weeks to run…</li>
</ul>
<p>I believe great improvements are coming.</p>
<hr />
<p>Discussions on
<a href="https://www.reddit.com/r/prng/comments/g3nh4i/shishua_the_fastest_prng_in_the_world/">Reddit</a>
and
<a href="https://news.ycombinator.com/item?id=22907539">Hacker News</a>
.</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2020-04-18T16:59:00Z",
  "keywords": "prng, crypto" }
</script> ]]>
        </content>
      </entry>
      <entry>
        <id>https://espadrine.github.io/blog/posts/a-primer-on-randomness.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/a-primer-on-randomness.html"/>
        <title>A Primer On Randomness</title>
        <published>2020-03-27T15:17:57Z</published>
        <category term="prng"/>
<category term="crypto"/>
        <content type="html">
          <![CDATA[ <h1>A Primer On Randomness</h1>
<p>Last October, during a one-week hiking holiday in the birthplace of alpinism,
I got particularly interested in random generators.</p>
<p>Four reasons why they are fascinating:</p>
<ol>
<li>It is only once you track it that you realize just in which gargatuan proportions you <strong>exude information</strong>. Even tiny systems that encode very little data and whose entire purpose is to never leak it (ie, random generators), do so in ways that can be measured, and even exploited. In every instant of your life, during every interaction with someone, billions of muscle movements, tiny and large, only occur because of past events burnt into your brain’s circuits, and betray this private history. Given enough of it, an aggregator could rewind the world and extract minute details from the past.</li>
<li>All of <strong>symmetric cryptography</strong> completely hinges on randomness. Security proofs fully rely on the analysis of how little information you can extract from a stream, which requires the stream to effectively look random.</li>
<li>Studying them, and trying your hand at making them, helps you understand the <strong>scientific method</strong> better. Most real-world principles can never be proved with absolute certainty; you need to accurately detect a signal in the noise, and measure the likelihood that this signal is not just you seeing patterns in the static.</li>
<li>Finally, it helps both understand <strong>the virtue of mixing</strong>, and how best to stir. The effect of mixing is exponential, which is unnatural to mentally harness. On the plus side, when done well, you get fluid exchange of information, remix, and cultural explosion. On the minus side, you get COVID-19 everywhere. Striking the right balance gets you far: many optimizing algorithms rely on it such as genetic algorithms, stochastic gradient descent, or cross-validation sampling in machine learning, which each are heavy users of pseudo-random sources. The results speak for themselves: AlphaGo, for instance, beat the best human player at one of the hardest games on Earth, using Monte-Carlo Tree Search. Yes, you guessed it, they call it Monte Carlo for a reason.</li>
</ol>
<h2>Information Theory</h2>
<p>A good Pseudo-Random Number Generator (or PRNG for short) is indistinguishable from a true random output.</p>
<p><em>So, where do we get this true random output you speak of?</em></p>
<p>True randomness has statistical meaning, but it is impossible to prove or disprove.
You can only have a high confidence.</p>
<p>You might hope that true randomness can be extracted from nature, but that is also not true.
The physical realm contains a large quantity of data storage (“space”),
and laws that alter it: gravity, electromagnetism, …
Nature is a state transition function and an output; that is also the structure of a PRNG.</p>
<p>Physical processes that claim to output “true” randomness rely on the large amount of information stored in the environment, and that environment’s diffuse state scrambling, that is presumably extremely hard for an attacker to detect.</p>
<p>For instance, the fine trajectory of electrons attracted from atom to atom through an electrical circuit causing minuscule delays, or the chaotic motion of gaseous atoms, or stronger yet, quantum behavior of particles.</p>
<p>Some physicists may argue that the world is not fully deterministic.
However, the Copenhagen Interpretation or Multiverse fans
cannot disprove the possibility of a non-local world that complies with the Bell-EPR paradox,
for instance through superdeterminism or pilot waves.
(Sorry for those that don’t care about quantum mechanics;
you don’t need to understand this paragraph to carry on.)</p>
<p>Since true randomness is not real, how do we get close?</p>
<p>Let’s say that you generate bits. If all the bits were <code>1</code>, it would be pretty predictable, right?
So the frequency of ones should converge to one out of two, which is what probability half is.</p>
<p>But if the output was a one followed by a zero continuously (<code>101010…</code>), it would be predictable too!
So the frequency of the sequence <code>10</code> in the output should converge to one out of four.</p>
<p>More generally, every possible sequence of <code>n</code> bits should appear with a frequency converging to <code>1÷2ⁿ</code>.</p>
<p>(A common romanticization of that idea is the comment that the decimals of π encode the entire works of Shakespeare.
π being irrational, its formulation is <a href="https://mathworld.wolfram.com/WeylsCriterion.html">orthogonal to any fractional representation</a>, which is what decimals are.
That gives strong credence to the conjecture that its digits form a truly random sequence.)</p>
<p>That idea might make you uneasy. After all, it gives an impossible requirement on the memory size of a generator.</p>
<h3>Memory</h3>
<p>If your state contains <code>i</code> bits, what is the largest sequence of consecutive ones it can output?</p>
<p>Well, since the PRNG is deterministic, a given state will always yield the same output.
There are <code>2ⁱ</code> possible state configurations, so with this entropy, you can at best output <code>i·2ⁱ</code> bits
before you arrive at a previous state and start repeating the same output sequence again and again.</p>
<p>At least, with an ideal PRNG, you know that one given configuration will output a sequence of <code>i</code> ones.
The previous configuration (which transitioned to the configuration that outputs the <code>i</code> ones)
cannot also output a sequence of <code>i</code> ones:
if two configurations yielded the same output, then there would be some <code>i</code>-bit output that no configuration produced.
That would not be an ideal PRNG.</p>
<p>So let’s say that the previous configuration gives <code>i-1</code> ones (a zero followed by a ton of ones),
and that the next configuration gives <code>i-1</code> ones (a ton of ones followed by a zero).
That is a total of a maximum of <code>3×i-2</code> consecutive ones.</p>
<p>Thus, you cannot get <code>3×i-1</code> consecutive ones…
which a true random generator would output with a frequency of <code>1 ÷ 2^(3×i-1)</code>.
A statistical deviation that you can detect to disprove that a generator is truly random!</p>
<p>Conversely, it means that <em>true generators require infinite memory</em>, which is impossible in the real world.</p>
<p>(By the way, yes, it does seem like computing all the digits of π requires infinite memory.
All current algorithms need more memory the more digits are output.)</p>
<p>In practice, you get around the issue by picking a state size <code>i</code> large enough that
detecting this statistical anomaly requires a millenia’s worth of random output, too much for anyone to compute.</p>
<h3>Cycle Analysis</h3>
<p>So, once we have picked a state size, now we have an upper bound for the period of the PRNG:
it will repeat the same sequence at least every <code>2ⁱ</code> bits.</p>
<p>But of course, your mileage may vary. An imperfect generator might have a much lower period.
Unless you have a mathematical proof for a <strong>lower bound</strong>, maybe your family of generators
has a seed (an initialization parameter) which results in the same output being repeated over and over…
That is called a fixed point.</p>
<p>Even if there are no fixed point, there could be a large number of seeds that start repeating soon!
(That was a real <a href="https://www.cs.cornell.edu/people/egs/615/rc4_ksaproc.pdf">vulnerability in the RC4 cipher</a>, by the way.)</p>
<p>On the plus side, there is a counterintuitive phenomenon that develops
when a set of links randomly connect with each other in closed chains.
Most links end up on long chains.
For instance, with two links, they will be connected in a chain half the time;
with three links, each link will be connected to another link with probability ⅔; etc.</p>
<p>Better yet, if you increase the number of links linearly,
you decrease the proportion of links that are part of small chains exponentially.</p>
<p>The bottom line is this: you can always put lipstick on the pig by increasing the state size,
and your generator will look good.</p>
<p>However, a fundamentally better generator would have become even better yet with an increased state size.</p>
<h3>Reversibility</h3>
<p>If you build out the design at random, a danger lingers.
Unless you are careful, you might build an irreversible generator.
Given a state after a generation,
can you mathematically compute the previous state?</p>
<p>If you can’t,
then there are multiple initial states that can transition to the current state.
That means some states can never happen,
because there are no initial state that transitions to them;
they got stolen by the states with multiple previous states pointing to it!</p>
<p>That is bad. Why?</p>
<p>First, it reduces the potency of your state size
(since a percentage of possible states are unreachable).</p>
<p>Second, many seeds merge into the rail tracks of other seeds,
converging to a reduced set of possible streams and outputting the same values!
Not only does this create inter-seed output correlation,
it also means that <em>a given stream will likely degrade in period</em>.</p>
<p><img alt='Irreversible PRNG example.' src='../assets/a-primer-on-randomness/irreversible-prng.svg' width=350px'>
<p>It could look good for many terabytes, and suddenly reach a fixed point,
and output the same number over and over.</p>
<p>In fact, if the states transition to randomly picked states,
the average cycle that you eventually get to,
<a href="https://burtleburtle.net/bob/rand/talksmall.html">loops every 2<sup>(n+1)÷2</sup></a>.</p>
<p>If you build a <strong>reversible</strong> algorithm,
at least all streams are a cycle,
so inter-seed correlation is not inevitable.</p>
<p>Some streams can have really long cycles.
Because they include a lot of states,
a starting seed is more likely to land in a long-cycle state.
The average period becomes 2<sup>n-2</sup>, almost the square of the length.</p>
<p><img alt='Reversible PRNG example.' src='../assets/a-primer-on-randomness/reversible-prng.svg' width=350px'>
<p>Note that a reversible design does not mean that the state cycles through all possible combinations.
It just means that each state points to exactly one other state, and has exactly one state leading to it.
In other words, it is a <em>bijection</em>, but not a <em>circular permutation</em>.</p>
<p><img alt='Circular permutation example.' src='../assets/a-primer-on-randomness/circular-prng.svg' width=350px'>
<h3>Diffusion</h3>
<p>Claude Shannon made <a href="https://www.iacr.org/museum/shannon/shannon45.pdf">a very good point the other day</a> (I think it was in 1945?) about ciphers.
An ideal pseudo-random source is such that any bit of the input flips half the bits of the output.</p>
<p>More precisely, ideally, the probability that any bit of the stream flips if a given bit of the state flips, should be ½.
That is called <strong>diffusion</strong> of the state.</p>
<p>After all, if it wasn’t ½, I could start making good guesses about whether this bit of the state is set,
and slowly recover pieces of the state or even the key.
And suddenly, I can predict the whole stream.</p>
<p>A related concept is <strong>confusion</strong> of the key.
Ideally, each bit of the output depends equally on a combination of all bits of the key.
So, each bit of the key should change each bit of the stream,
for half of the set of possible configurations of the key’s other bits.</p>
<p>Each bit of the stream should therefore be a complex combination of all of the key’s bits,
while each bit of the key should have an impact stretched along the whole stream.</p>
<p>These properties particularly matter for cryptographic primitives such as ChaCha20,
where the seed of the PRNG is essentially the cipher key.
Their analysis and understanding still matter for PRNG quality;
although some designs don’t take confusion seriously,
leading to severe correlation of distinct seeds.</p>
<h2>Tooling</h2>
<p>Back in the seventies, there was no tooling to pragmatically study the quality of a generator.
That made the PRNG hobby somewhat impractical.</p>
<p>As a sad result, some people produced subpar results, such as IBM’s infamous <a href="https://en.wikipedia.org/wiki/RANDU">RANDU</a>:</p>
<blockquote>
<p>It fails the spectral test badly for dimensions greater than 2, and every integer result is odd.</p>
</blockquote>
<p>Fortunately, great strides were made since.
Anyone can get going quickly, up until they start having competitive results.</p>
<h3>History</h3>
<p>A first step was Donald Knuth’s description of the use of <strong>Chi-Squared tests</strong> in 1969.</p>
<p>While its application to generators was described in Knuth’s seminal work
<em>The Art of Computer Programming</em>, we have to thank Karl Pearson for the concept.</p>
<p>As the story goes, Pearson was disgruntled at scientists estimating all their results
based on the assumption that their statistical distributions were always normal,
when in some cases they very clearly were not. They just didn’t really have any other tool.</p>
<p>So he worked through the theory. Say you make a claim that some value, for which you have samples,
follows a given statistical distribution. (A uniform one perhaps? Like our PRNG outputs?)
Call that “<strong>the Null Hypothesis</strong>”, because it sounds cool.</p>
<p>Your evidence is a set of samples that belong in various categories.
Your null hypothesis is the belief that each category <code>i ∈ {1,…,k}</code> appears with probability <code>pᵢ</code>.
Maybe the two classes are 0 and 1; maybe they are the 256 possible bytes.</p>
<p>There are <code>oᵢ</code> <em>observed</em> samples in category <code>i</code>.
The theoretical, <em>expected</em> number of samples should be <code>eᵢ</code> = <code>n·pᵢ</code>.
You compute the <strong>Chi-Squared statistic</strong>: <code>χ²</code> = <code>Σ (eᵢ - oᵢ)² ÷ eᵢ</code>.</p>
<p>That statistic follows a distribution of probabilities,
depending on the degrees of freedom of the problem at hand.
If we are looking at random bytes, each generation must be one of 256 possible outputs:
so there are 255 degrees of freedom.
(If it is not in the first 255, it must be in the last, so the last one is not a degree of freedom.)</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/3/35/Chi-square_pdf.svg" alt="Chi-Squared probability density" /></p>
<p>Each possible value of <code>χ²</code> you get has a probability of being valid for your null hypothesis.
One value is the most probable one. The further you get from it, the least likely it is that your samples are random.</p>
<p>But by how much?</p>
<p>You want to know the probability that a true random generator’s <code>χ²</code> lands
as far from the ideal value as your pseudo-random generator did.
(After all, even a perfect generator rarely precisely lands on the most probable <code>χ²</code>,
which for random bytes is 253 with probability 1.8%.)</p>
<p>You can compute the probability that a true random generator’s <code>χ²</code> is bigger (more extreme) than yours.
That probability is called a <strong>p-value</strong>.
If it is tiny, then it is improbable that a true random generator would get this value;
and so, it is improbable that what you have is one.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/8/8e/Chi-square_distributionCDF-English.png" alt="Chi-Squared distribution" /></p>
<p>With this tool in hand, you can easily check that a process that pretends to be random is not actually so.</p>
<p>Or, as <a href="http://www.economics.soton.ac.uk/staff/aldrich/1900.pdf">Pearson puts it</a>:</p>
<blockquote>
<p>From this it will be more than ever evident how little chance had to do
with the results of the Monte Carlo roulette in July 1892.</p>
</blockquote>
<p>(Not sure why his academic paper suddenly becomes so specific;
maybe he had a gambling problem on top of being a well-known racist.)</p>
<p>Fun sidenote: if you look at the <code>χ²</code> formula, notice that if your observed values all hit their expectations,
you will always end up with a <code>χ²</code> equal to zero, whose p-value is 1.</p>
<p>Uniform random numbers have this awesome property that their p-values should also be uniformly random,
and the p-values of the p-values too, and so on.</p>
<p>The p-value you want is simply one that is not too extreme (eg, higher than 10¯⁵, lower than 1-10¯⁵).
A p-value of 1 immediately disqualifies your null hypothesis!
Perfect fits are not random; you must have anomalies some of the time.</p>
<p>Let’s get back to Donald Knuth. His advice of using this tool to study pseudo-random efforts defined all subsequent work.</p>
<p>In 1996, another PRNG fellow, George Marsaglia, looked at the state of tooling with discontent.
Sure, those Chi-Squared tests were neat.
But writing them by hand was tedious.</p>
<p>Worse, nothing defined what to observe. Bytes are one thing, but they only detect byte-wise bias.
What about bitwise? What if we count bits, and compare that count to a <em>Known Statistic</em> (<strong>bit counting</strong>)?
What if we count the number of successive times one byte is bigger than the one generated just before (<strong>runs test</strong>)?
Or maybe count the number of outputs between the appearance of the same value (<strong>gap test</strong>)?
Or take a random matrix, compute its rank, verify that it validates the <em>Known Statistic</em> (<strong>binary rank</strong>)?</p>
<p>Well, he didn’t think about all those tests,
but he did publish a software package that automatically computed p-values
for a dozen of tests. He called it <em>DIEHARD</em>.</p>
<p>Some are like the ones I described, some are a bit wilder and somewhat redundant,
some have a bit too many false positives to be relied upon.</p>
<p>But it was the start of automation!</p>
<p>And the start of the systematic extermination of the weak generators.</p>
<p>In 2003, Robert G. Brown extended it with an easy-to-use command-line interface, <em><a href="https://webhome.phy.duke.edu/~rgb/General/dieharder.php">Dieharder</a></em>,
that allowed testing without having to fiddle with compilation options, just by piping data to a program.
He aggregated a few tests from elsewhere, such as the NIST’s STS
(which are surprisingly weak for their cryptographic purpose… Those were simpler times.)</p>
<p>A big jump in quality came about in 2007.
Pierre L’Écuyer &amp; Richard Simard published <em><a href="http://simul.iro.umontreal.ca/testu01/tu01.html">TestU01</a></em>, a test suite consisting of three bars to clear.</p>
<ul>
<li>SmallCrush picks 10 smart tests that killed a number of weak generators in 30 seconds.</li>
<li>Crush was a very intensive set of 96 tests that killed even more weaklings, but it took 1h to do so.</li>
<li>BigCrush was the real monster. In 8 hours, its set of 106 tests brutalizes 8 TB of output, betraying subtler biases never before uncovered, even in many previously-beloved PRNGs, such as the still-popular Mersenne Twister. A very sobering moment.</li>
</ul>
<p>TestU01 installed two fresh ideas: having multiple levels of intensity, and parameterizing each test.
The latter in particular really helped to weed out bad generators.
Maybe if you look at all the bits, they look fine, but if you look at every eigth bit, maybe not so much?</p>
<p>The feel of using the programs was still similar, though: you ran the battery of tests,
you waited eight hours, and at the end, you were shown the list of all tests whose p-value was too extreme.</p>
<p>Thence came the current nec-plus-ultra: Chris Doty-Humphrey’s <em>Practically Random</em>,
affectionately called <a href="http://pracrand.sourceforge.net/">PractRand</a>, published in 2010.</p>
<p>It was a step up still from TestU01:</p>
<ul>
<li>Instead of eating one output for one test and throwing it away, it uses output for multiple tests, and even overlaps the same test families along the stream, maximizing the extraction of statistics from each bit of output.</li>
<li>It took the concept of levels of intensity to a new level. The program technically never stops; it continuously eats more random data until it finds an unforgivable p-value. On paper, it is guaranteed to find one, at least once it reaches the PRNG’s cycle length; but that assumes you have enough memory for it to store its statistics. In practice, you can go very far: for instance, the author’s own sfc16 design reached flaws after 512 TiB — which took FOUR MONTHS to reach!</li>
<li>It displays results exponentially. For instance, once at 1 MB of random data read, then at 2, then at 4, then at 8, … Every time, it either tells you that there are no anomalies, or the list of tests with their bad p-values.</li>
</ul>
<p><em>(A small note: don’t expect this tooling to be satisfactory for anything cryptographic.
Their study relies on much more advanced tooling and analysis pertaining to diffusion,
differential cryptanalysis, algebraic and integral attacks.)</em></p>
<p>I am a big believer in tooling.
I believe it is THE great accelerator of civilization by excellence.
The step that makes us go from running at 30 km/h, to speeding at 130 km/h, to rocketing at 30 Mm/h.
In fact, by the end of this series of posts, I hope to publish one more tool to add to the belt.</p>
<h3>Hands-On</h3>
<p>I don’t actually recommend you start out with PractRand for the following reasons:</p>
<ul>
<li>You might make silly mistakes. PractRand can kill generators that looked OK in the 80s fairly instantly. You won’t know if your design didn’t even stand a chance back then, or if it was competitive.</li>
<li>You might have a coding bug. It would be too bad if you threw away a good starting design just because a mask had the wrong bit flipped.</li>
<li>Seeing Chi-Square failures helps understand the beginner design space. Yes, you want the output to have high entropy; but while it is obvious that you don’t want a poorly balanced output (eg. one possible sequence appears too often), you also don’t want a highly structured output (eg. all possible sequences appear exactly as often), since random noise must contain anomalies. Seeing a high-entropy generator fail because bytes were slightly too equiprobable helped me appreciate what was undesirable. It is often counter-intuitive, so these beginner lessons help a lot.</li>
</ul>
<p>I would encourage you to build a silly idea, then pipe 10 MB to <a href="https://www.fourmilab.ch/random/">ent</a>.
Check the entropy calculation (it should be somewhere around 7.9999),
and verify that the Chi-Square p-value is between 0.1% and 99.9% with a set of seeds.</p>
<p>Compare it to a good randomness source: <code>&lt;/dev/urandom head -c 10M | ent</code>.
(When I say good, I mean ChaCha20, which is what Linux uses.)</p>
<p>See what happens when you go from 10M to 100M: does the p-value always decrease, or always increase?
That would be bad, very bad indeed.</p>
<p>Once your Chi-Squared is good, skip all the old tests, and hop into PractRand: <code>./prng | RNG_test stdin64</code>.
I recommend specifying the size of your output, so that PractRand can know what to look out for.</p>
<p>Then, goes the contest.</p>
<p>If you pass 1 MiB: you have beat the sadly very widely-used <a href="http://man7.org/linux/man-pages/man3/drand48.3.html">drand48</a>! (Java, C, …)</p>
<p>If you pass 256 GiB: you are now better than the widely-used <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html">Mersenne Twister</a>! (Ruby, Python, …)</p>
<p>If you pass 1 TiB: congratulations, you beat the famous <a href="https://cypherpunks.venona.com/archive/1994/09/msg00304.html">RC4</a> stream cipher!
(Used as macOS’s old arc4random source, and actually most websites used it for TLS at some point…)</p>
<p>If you pass 32 TiB: you have won. The <code>RNG_test</code> program automatically stops.
Beware: it takes about a week to compute… when your generator is fast.</p>
<p>Quick advice: remember that p-values should be uniformly random.
It is inevitable to have some of them be labeled “unusual”, or even, more rarely, “suspicious”.
It does not mean you failed.</p>
<p>When the p-value is too extreme, PractRand will show “FAIL!” with a number of exclamation marks proportional to how horrified it is.
Then, the program will stop immediately.</p>
<p>Some tests will fail progressively.
If the same test shows “unusual” at 4 GiB, and “suspicious” at 8 GiB,
it will probably fail at 16 GiB.</p>
<h3>Speed</h3>
<p>Once you beat 32 TiB of PractRand, you know your generator is good —
but to be useful, it also must be the fastest in its class.</p>
<p>A few notes can really help you get it up to speed.</p>
<p>First, pick your target platform.</p>
<p>You will need different optimization tricks if you build for <code>x86_64</code>
(Intel / AMD), or for ARM (phones),
or if you directly target a CMOS integrated circuit,
if you want to burn your PRNG in an ASIC.</p>
<p>Let’s say you want to get the most out of your Intel or AMD chip.
Go as close to the metal as you can. Code in C, C++, or Rust.</p>
<p>Second, understand the assembly output. Looking at the compiled assembly with <code>gcc prng.c -S -o prng.asm</code> can help.
I recommend <a href="https://software.intel.com/en-us/articles/introduction-to-x64-assembly">Intel’s introduction</a>, <a href="https://www.amd.com/system/files/TechDocs/24592.pdf">AMD’s manual</a> and <a href="https://www.agner.org/optimize/instruction_tables.pdf">Agner’s instruction tables</a>.</p>
<p>In particular, a number of amd64 opcodes are inaccessible from the programming language.
You can access them in various ways:</p>
<ul>
<li>The compiler will smartly use them when they apply. For instance, there is an opcode to rotate the bits of a variable leftward: <code>ROL</code>. But all the C programming language offers is shift (<code>&gt;&gt;</code> for <code>SHR</code>, <code>&lt;&lt;</code> for <code>SHL</code>). However, the compiler will map <code>(a &lt;&lt; 1) | (a &gt;&gt; 63)</code> to the 64-bit <code>ROL</code>.</li>
<li>Compilers usually include header files or libraries to access those instructions, by exporting functions that compile down to the corresponding instruction. Those are called <strong><a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">intrinsics</a></strong>. For instance, our friend the 64-bit <code>ROL</code> appears as <code>_rotl64(a, 1)</code>, if you <code>#include &lt;immintrin.h&gt;</code>.</li>
<li>SIMD operations heavily depend on your mastery of the compiler. You can either access them through assembly, compiler flags, or intrinsics (my favorite).</li>
</ul>
<p>Third, understand the way <a href="https://www.agner.org/optimize/microarchitecture.pdf">the CPU processes the assembly</a>.</p>
<ul>
<li><strong><a href="https://software.intel.com/en-us/blogs/2011/11/22/pipeline-speak-learning-more-about-intel-microarchitecture-codename-sandy-bridge">Instruction pipelining</a></strong>: Every instruction executed goes through a number of phases:<br />
① the instruction is decoded from memory and cut in micro-operations (μops);<br />
② each μop is assigned internal input and output registers;<br />
③ the μop reads input registers;<br />
④ it is executed;<br />
⑤ it writes to the output register; and finally<br />
⑥ the output register is written to the target register or memory.<br />
Each of those stages start processing the next instruction as soon as they are done with the previous one, without waiting for the previous instruction to have cleared all steps. As a result, a good number of instructions are being processed at the same time, each being in a different stage of processing.<br />
<em>Example gain: successive instructions go faster if each stage of the second one does not depend on the first one’s later stages.</em></li>
<li><strong>Superscalar execution</strong>: Each μop can be executed by one of multiple execution units; two μops can be executed by two execution units in parallel as long as they don’t have inter-dependencies. There might be one execution unit with logic, arithmetic, float division, and branches; one execution unit with logic, arithmetic, integer and float multiplication; two with memory loads; one with memory stores; one with logic, arithmetic, SIMD permutations, and jumps. Each have a different combination of capabilities.<br />
<em>Example gain: adding a second instruction doing the same thing, or something belonging to another unit, may not add latency if it acts on independent data.</em></li>
<li><strong>Out-of-order execution</strong>: Actually, after the μop is assigned internal registers, it is queued in a ReOrder Buffer (ROB) which can store about a hundred. As soon as a μop’s input registers are ready (typically because of a read/write constraint: another μop wrote the information that this μop needs to read), it gets processed by the first execution unit that can process it and is idle. As a consequence, the CPU can process instructions 2, 3, etc. while instruction 1 waits on a read/write dependency, as long as the next instructions don’t have read/write dependencies with stalled instructions.<br />
<em>Example gain: you can put fast instructions after a slow (or stalled) instruction without latency cost, if they don’t depend on the slow instruction’s output.</em></li>
<li><strong>Speculative execution</strong>: When there is a branch (eg. an if condition), it would be awful if the whole out-of-order instruction pipeline had to stop until the branch opcode gave its boolean output. So the CPU doesn’t wait to know if the branch is taken: it starts processing the instructions that come after the branch opcode. Once it gets the branch opcode output, it tracks all μops that wrongly executed, and reverts all their work, rewrites the registers, etc.</li>
<li><strong>Branch prediction</strong>: To get the best out of speculative execution, CPUs make guesses as to what the boolean output of a branch is going to be. It starts executing the instructions it believes will occur.<br />
<em>Example gain: make your branches nearly always take the same path. It will minimize branch mispredictions, which avoids all the reverting work.</em></li>
</ul>
<p>Finally, beware of the way you test performance. A few tips:</p>
<ol>
<li>Use the <code>RDTSC</code> CPU opcode to count cycles, as below.</li>
<li>Disable CPU frequency variability. CPUs nowadays have things like Turbo Boost that change your frequency based on how hot your processor gets and other factors. You want your CPU to have a fixed frequency for the whole process.</li>
<li>Have as few other processes running as possible. If a process runs in the background, eating CPU, it will affect the results.</li>
</ol>
<pre><code>#include &lt;x86intrin.h&gt;

int main() {
  __int64_t start = _rdtsc();
  generate_one_gigabyte();
  __int64_t cycles = _rdtsc() - start;
  fprintf(stderr, &quot;%f cpb\n&quot;, ((double)cycles) / 1073741824);
}
</code></pre>
<h3>Designs</h3>
<p>The earliest design is the <strong>LCG</strong> (Linear Congruent Generator).
You can recognize its dirt-simple state transition (a constant addition or multiplication),
which has neat consequences on the analysis of its cycle length (typically 2^statesize).
Usually, the output is treated with a shift or rotation before delivery.
While they look fairly random, they can have severe issues, such as hyperplane alignment.
They also tend to be easy to predict once you reverse-engineer them,
which is why they are not used for anything remotely in need of security.</p>
<p>Examples of LCG abound: <a href="http://man7.org/linux/man-pages/man3/drand48.3.html">drand48</a>, <a href="https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/">Lehmer128</a>, <a href="https://www.pcg-random.org/">PCG</a>, …</p>
<p>Then come <strong>Shufflers</strong> (eg. <a href="https://cypherpunks.venona.com/archive/1994/09/msg00304.html">RC4</a>, <a href="http://burtleburtle.net/bob/rand/isaacafa.html">ISAAC</a>, <a href="http://pracrand.sourceforge.net/RNG_engines.txt">EFIIX</a>).
Usually have an “I” in the name (standing for “indirection”).
They try to get randomness by shuffling a list, and they shuffle the list from the randomness they find.
Do not recommend. It is so easy for bias to seep through and combine destructively.
Besides, weeding out bad seeds is often necessary.</p>
<p><strong>Mixers</strong> rely on a simple transition function,
usually addition to what is sometimes called a “gamma” or “<a href="https://mathworld.wolfram.com/WeylsCriterion.html">Weyl coefficient</a>”.
A common non-cryptographic pattern is a state multiplication, just like in LCG,
and the output is XORed with a shifted or rotated version of itself before delivery.
The second step is basically a hash.
(To the security-minded readers: I am not talking about collision-resistant compression functions.)
In cryptography, usually, the mixer uses some ARX combination for bit diffusion (ARX = Add, Rotate, XOR),
and is scheduled in multiple rounds (which are basically skipping outputs).
Examples include <a href="https://github.com/wangyi-fudan/wyhash">wyrand</a>, <a href="http://gee.cs.oswego.edu/dl/papers/oopsla14.pdf">SplitMix</a>, <a href="http://vigna.di.unimi.it/ftp/papers/xorshiftplus.pdf">Xorshift128+</a>, <a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.197.pdf">AES-CTR</a>, and the beloved <a href="https://cr.yp.to/chacha/chacha-20080128.pdf">ChaCha20</a>.</p>
<p>Finally, the most haphazard of them: <strong>chaotic generators</strong>.
They typically have no minimal cycle length, and they just try to stir things up in the state.
For instance, <a href="https://burtleburtle.net/bob/rand/smallprng.html">jsf</a> and <a href="http://www.romu-random.org/">Romu</a>.</p>
<h2>Parting Fun Facts</h2>
<p>I mentionned ChaCha20 a lot, because it is one of my favorite cryptographic primitives.
I’ll give you a few fun facts about it, as goodbye.</p>
<ol>
<li>ChaCha20 <a href="https://cr.yp.to/snuffle/salsafamily-20071225.pdf">initializes its state</a> with the ASCII for “expand 32-byte k”. It’s a wink on the purpose of the cipher: it takes a 256-bit key, and expands it to a large random stream.</li>
<li>It is based on the design of <a href="https://cr.yp.to/export/1996/0726-bernstein.txt">a joke cipher that plays on a US law</a> cataloguing encryption as munition, except if it is a hash. He built it as a simple construction on top of a carefully-constructed hash. Calling the core construction a hash caused him trouble later as <a href="https://cr.yp.to/snuffle/reoncore-20080224.pdf">reviewers misunderstood it</a>.</li>
<li>The initial name of that cipher was Snuffle. (Yes.)</li>
</ol>
<p><a href="https://www.reddit.com/r/prng/comments/fpy6pg/a_primer_on_randomness/">Find comments on Reddit</a>.</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2020-03-27T15:17:57Z",
  "keywords": "prng, crypto" }
</script> ]]>
        </content>
      </entry>
      <entry>
        <id>https://espadrine.github.io/blog/posts/two-postgresql-sequence-misconceptions.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/two-postgresql-sequence-misconceptions.html"/>
        <title>Two PostgreSQL Sequence Misconceptions</title>
        <published>2019-09-05T17:28:59Z</published>
        <category term="sql"/>
        <content type="html">
          <![CDATA[ <h1>Two PostgreSQL Sequence Misconceptions</h1>
<p>✨ <em>With Examples!</em> ✨</p>
<p>Some constructs seem more powerful than the promises they make.</p>
<p>PostgreSQL sequences are like that. Many assume it offers stronger properties
than it can deliver.</p>
<p>They trust them to be the grail of SQL ordering, the one-size-fits-all of strict
serializability. However, there is a good reason Amazon spent design time on
vector clocks in <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo</a>, Google invested significantly into <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">Chubby</a>, then
<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf">Percolator</a>’s timestamp oracle, then <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf">Spanner</a>’s expensive,
atomic-clock-based TrueTime; why Twitter built <a href="https://developer.twitter.com/en/docs/basics/twitter-ids.html">Snowflake</a>, and so many others
built custom timestamp systems.</p>
<ol>
<li>Strict serializability is hard to achieve, especially in a distributed
system, but even in a centralized system with the possibility of failure.</li>
<li>Developers assume the system is strict-serializable, but it usually is not.</li>
<li>When a system provides timestamps, developers will use those as if they were
monotonically strictly increasing atomically throughout the distributed
system, but they often are not, which causes subtle bugs.</li>
</ol>
<h2>The problem space</h2>
<p>To design your system’s properties right, it is often useful or necessary to
determine the order in which events happened. Ideally, you wish for the <strong>“wall
clock” order</strong> (looking at your watch), although instantaneity gets tricky when
events occur at a distance, even within the same motherboard, but especially
across a datacenter, or between cities.</p>
<p>At the very least, you want to reason about <strong>causal ordering</strong>: when that event
happened, did it already see this other event?</p>
<p>A nice property to have, even for a single centralized database, is to give a
monotonically increasing identifier for each row. Most PostgreSQL users rely on
the <code>SERIAL</code> type for that – a sequence. Each insertion will call <code>nextval()</code>
and store an increasing value.</p>
<p>What you implicitly want is to list rows by insertion order, Your mental model
is that each insertion happens at a set “wall clock” time. A first insertion
will happen at T0 and set the identifier 1, the next one happens at T1 and get
number 2, and so on. Therefore, <em>you expect a row with ID N to have causally
been inserted after a row with ID M &lt; N</em>.</p>
<p>Operational order is a consistency constraint strongly associated with isolation
levels. A PostgreSQL database can handle multiple simultaneous operations.</p>
<p><em>(Side note: I could be talking about threads and locks, but I will not, because
those are just tools to achieve properties. PostgreSQL may switch tools to
better meet a given promise (they did so with the serializable level in 2011),
but the promise won’t change.)</em></p>
<p>By default, it promises <strong>Read Committed</strong> isolation: a transaction can witness
the effects of all transactions that commit “before” it does (but not those that
have not committed yet). Their commits are therefore causally ordered by commit
time.</p>
<p>However, nothing else within a transaction has any causal promise with respect
to other transactions. The same <code>SELECT</code> can yield different values;
simultaneous insertions can happen either before, after, or anything in between,
your own insertion.</p>
<p>The highest isolation level PostgreSQL offers is <strong>Serializable</strong> isolation: all
transactions are causally ordered; from <code>BEGIN</code> to <code>COMMIT</code>. Of course,
transactions still execute in parallel; but the database makes sure that
everything that a transaction witnesses can be explained by executing all its
statements either after all statements of another transaction, or before all of
them. It won’t see a changing state within the execution of the transaction.</p>
<p><em>(By the way, PostgreSQL only achieved serializability in 2011, when they
released <a href="https://www.postgresql.org/docs/release/9.1.0/">version 9.1</a> with support for predicate locks. It is hard.)</em></p>
<p>Having a causal order does not mean that this order follows <em>real time</em>: one
insertion may complete at 9:30am <em>after (in causal order)</em> another that
completes later at 10:40am. If you want the additional property that the order
is consistent with wall clock time, you want <strong><a href="https://jepsen.io/consistency/models/strict-serializable">Strict Serializability</a></strong>.</p>
<p>However, <strong>PostgreSQL makes no claim of Strict Serializability</strong>.</p>
<p>Given all this, sequences probably feel much weaker than you initially thought.</p>
<p>You want them to give a continuous set of numbers, but a sequence can yield
values with gaps (1 2 4).</p>
<p>You want them to give a causal order <em>(2 was inserted before 3)</em>, but it can
yield values out of order (1 3 2).</p>
<p>All a sequence promises is to give values that have an order. Not a continuous
order, nor a time order.</p>
<p>Let’s demonstrate both.</p>
<h2>Gaps</h2>
<p>Let’s create a table with a <code>SERIAL</code> identifier. For the purpose of showing
things going right, let’s insert a row.</p>
<pre><code class="language-sql">CREATE TABLE gaps (id SERIAL);
BEGIN;
INSERT INTO order DEFAULT VALUES;
SELECT * FROM gaps;
</code></pre>
<pre><code> id 
----
  1
(1 row)
</code></pre>
<p>Now comes the gap.</p>
<pre><code class="language-sql">BEGIN;
INSERT INTO order DEFAULT VALUES;
ROLLBACK;
</code></pre>
<p>Since we rolled back, nothing happened – or did it?</p>
<p>Let’s now insert another row.</p>
<pre><code class="language-sql">INSERT INTO order DEFAULT VALUES;
SELECT * FROM gaps;
</code></pre>
<pre><code> id 
----
  1
  3
(2 rows)
</code></pre>
<p>Oops! Despite the rollback, the sequence was incremented without being reverted.
Now, there is a gap.</p>
<p>This is not a PostgreSQL bug per se: the way sequences are stored, it just does
not keep the information necessary to undo the <code>nextval()</code> without potentially
breaking other operations.</p>
<p>Let’s now break the other assumption.</p>
<h2>Order violation</h2>
<p>First, a table with a sequence and a timestamp:</p>
<pre><code class="language-sql">CREATE TABLE orders (id SERIAL, created_at TIMESTAMPTZ);
</code></pre>
<p>Let’s set up two concurrent connections to the database. Each will have the same
instructions. I started the first one yesterday:</p>
<pre><code class="language-sql">-- Connection 1
BEGIN;
</code></pre>
<p>I launch the second one today:</p>
<pre><code class="language-sql">-- Connection 2
BEGIN;
INSERT INTO orders (created_at) VALUES (NOW());
COMMIT;
</code></pre>
<p>Let’s go back to the first one:</p>
<pre><code class="language-sql">-- Connection 1
INSERT INTO orders (created_at) VALUES (NOW());
COMMIT;
</code></pre>
<p>Simple enough. But we actually just got the order violation:</p>
<pre><code class="language-sql">SELECT * FROM orders ORDER BY created_at;
</code></pre>
<pre><code> id |          created_at           
----+-------------------------------
  2 | 2019-09-04 21:10:38.392352+02
  1 | 2019-09-05 08:19:34.423947+02
</code></pre>
<p>The order of the sequence does not follow creation order.</p>
<p>From then on, developers may write some queries ordering by ID, and some
ordering by timestamp, expecting an identical order. That incorrect assumption
may break their business logic.</p>
<p>Lest you turn your heart to another false god, that behavior remains the same
with serializable transactions.</p>
<h2>Are we doomed?</h2>
<p>No.</p>
<p>Sure, the systems we use have weak assumptions. But that is true at every level.
The nice thing about the world is that you can combine weak things to make
strong things. Pure iron is ductile, and carbon is brittle, but their alloy is
steel.</p>
<p>For instance, you can get the best of both worlds, causal order and “wall clock”
timestamps, by having a <code>TIMESTAMPTZ</code> field, only inserting rows within
serializable transactions, and setting the <code>created_at</code> field to now, or after
the latest insertion:</p>
<pre><code class="language-sql">BEGIN ISOLATION LEVEL SERIALIZABLE;
INSERT INTO orders (created_at)
SELECT GREATEST(NOW(), MAX(created_at) + INTERVAL '1 microsecond') FROM orders;
COMMIT;
</code></pre>
<p>Indeed, PostgreSQL’s <code>TIMESTAMPTZ</code> has a precision up to the microsecond. You
don’t want to have conflicts in your <code>created_at</code> (otherwise you could not
determine causal order between the conflicting rows), so you add a microsecond
to the current time if there is a conflict.</p>
<p>However, here, concurrent operations are likely to fail, as we acquire a
(non-blocking) SIReadLock on the whole table (what the documentation calls a
relation lock):</p>
<pre><code class="language-sql">SELECT l.mode, l.relation::regclass, l.page, l.tuple, substring(a.query from 0 for 19)
FROM pg_stat_activity a JOIN pg_locks l ON l.pid = a.pid
WHERE l.relation::regclass::text LIKE 'orders%'
  AND datname = current_database()
  AND granted
ORDER BY a.query_start;
</code></pre>
<pre><code>       mode       | relation | page | tuple |     substring
------------------+----------+------+-------+--------------------
 SIReadLock       | orders   |      |       | INSERT INTO orders
 RowExclusiveLock | orders   |      |       | INSERT INTO orders
 AccessShareLock  | orders   |      |       | INSERT INTO orders
</code></pre>
<p>The reason for that is that we perform a slow Seq Scan in this trivial example,
as the <a href="https://www.postgresql.org/docs/current/using-explain.html">EXPLAIN</a> proves.</p>
<pre><code>                                  QUERY PLAN
-------------------------------------------------------------------------------
 Insert on orders  (cost=38.25..38.28 rows=1 width=8)
   -&gt;  Aggregate  (cost=38.25..38.27 rows=1 width=8)
         -&gt;  Seq Scan on orders orders_1  (cost=0.00..32.60 rows=2260 width=8)
</code></pre>
<p>With an <a href="https://www.postgresql.org/docs/current/sql-createindex.html">index</a>, concurrent operations are much more likely to work:</p>
<pre><code class="language-sql">CREATE INDEX created_at_idx ON orders (created_at);
</code></pre>
<p>We then only take a tuple lock on the table:</p>
<pre><code>       mode       | relation | page | tuple |     substring      
------------------+----------+------+-------+--------------------
 SIReadLock       | orders   |    0 |     5 | INSERT INTO orders
 RowExclusiveLock | orders   |      |       | INSERT INTO orders
 AccessShareLock  | orders   |      |       | INSERT INTO orders
</code></pre>
<p>However, the tuple in question is the latest row in the table. Any two
concurrent insertions will definitely read from the same one: the one with the
latest <code>created_at</code>. Therefore, only one of concurrent insertion will succeed;
the others will need to be retried until they do too.</p>
<h2>Subset Ordering</h2>
<p>In cases where you only need a unique ordering for a subset of rows based on
another field, you can set a combined index with that other field:</p>
<pre><code class="language-sql">CREATE TABLE orders (
  account_id UUID DEFAULT gen_random_uuid(),
  created_at TIMESTAMPTZ);
CREATE INDEX account_created_at_idx ON orders (account_id, created_at DESC);
</code></pre>
<p>Then the <a href="https://www.postgresql.org/docs/current/using-explain.html">query planner</a> goes through the account index:</p>
<pre><code class="language-sql">INSERT INTO orders (account_id, created_at)
SELECT account_id, GREATEST(NOW(), created_at + INTERVAL '1 microsecond')
FROM orders WHERE account_id = '9c99bef6-a05a-48c4-bba3-6080a6ce4f2e'::uuid
ORDER BY created_at DESC LIMIT 1
</code></pre>
<pre><code>                                                      QUERY PLAN
-----------------------------------------------------------------------------------------------------------------------
 Insert on orders  (cost=0.15..3.69 rows=1 width=24)
   -&gt;  Subquery Scan on &quot;*SELECT*&quot;  (cost=0.15..3.69 rows=1 width=24)
         -&gt;  Limit  (cost=0.15..3.68 rows=1 width=32)
               -&gt;  Index Only Scan using account_created_at_idx on orders orders_1  (cost=0.15..28.35 rows=8 width=32)
                     Index Cond: (account_id = '9c99bef6-a05a-48c4-bba3-6080a6ce4f2e'::uuid)
</code></pre>
<p>And concurrent insertions on different accounts work:</p>
<pre><code>       mode       | relation | page | tuple |     substring
------------------+----------+------+-------+--------------------
 SIReadLock       | orders   |    0 |     1 | INSERT INTO orders
 RowExclusiveLock | orders   |      |       | INSERT INTO orders
 AccessShareLock  | orders   |      |       | INSERT INTO orders
 SIReadLock       | orders   |    0 |     2 | COMMIT;
</code></pre>
<p>(The first three row are from one not-finished transaction on account 1, the
last is from a finished one on account 2.)</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2019-09-05T17:28:59Z",
  "keywords": "sql" }
</script> ]]>
        </content>
      </entry>
</feed>
