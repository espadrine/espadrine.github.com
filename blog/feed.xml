<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Espadrine’s blog</title>
  <subtitle>Let’s talk about whatever I learn!</subtitle>
  <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/"/>
  <link rel="self" type="application/atom+xml" href="https://espadrine.github.io/blog/feed.xml"/>
  <id>https://espadrine.github.io/blog/feed.xml</id>
  <updated>2020-04-18T16:59:00Z</updated>
      <entry>
        <id>https://espadrine.github.io/blog/posts/shishua-the-fastest-prng-in-the-world.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/shishua-the-fastest-prng-in-the-world.html"/>
        <title>SHISHUA: The Fastest Pseudo-Random Generator In the World</title>
        <published>2020-04-18T16:59:00Z</published>
        <category term="prng"/>
<category term="crypto"/>
        <content type="html">
          <![CDATA[ <h1>SHISHUA: The Fastest Pseudo-Random Generator In the World</h1>
<p><em>(TLDR: see the <a href="#benchmark">benchmark</a> and the <a href="https://github.com/espadrine/shishua">code</a>.)</em></p>
<p>Six months ago, I wanted to make the best PRNG with an unconventional design,
whatever that design may be.
I expected it to start easy, and slowly get harder;
I wondered whether I would learn fast enough to pass the highest bar.</p>
<p>Surprisingly, difficulty did not increase linearly.
Passing the bytewise Chi-Squared tests was very hard!</p>
<p>Then, when I got the concepts, passing dieharder was also very hard.
When I got to that point, I was honestly so extatic,
that <a href="https://mobile.twitter.com/espadrine/status/1184542865969614849">I published what I got</a> to learn what the next challenge needed to be.
But it turned out <a href="https://mobile.twitter.com/espadrine/status/1184883565634424832">it failed PractRand</a>.</p>
<p>Then, <a href="https://mobile.twitter.com/espadrine/status/1186358084425400320">passing BigCrush</a> was very hard.</p>
<p>Then, passing 32 tebibytes of PractRand was very hard.</p>
<p>But once I reached that point, I realized that speed was going to be an issue.
It wasn’t just about having a construction that emitted ten megabytes a second, taking a month to pass PractRand.</p>
<p>But I have to admit, <a href="https://github.com/espadrine/combit">passing PractRand at a gigabyte a second</a> was very hard.</p>
<p>Once you get there… what you really want to see is whether you can reach the Pareto frontier.</p>
<p>You want the fastest PRNG in the world that beats the hardest statistical tests.</p>
<p>I got there.</p>
<p>In <a href="https://espadrine.github.io/blog/posts/a-primer-on-randomness.html">the previous entry to the series</a>, I explained all the things I learnt to reach it.
Here, I’ll detail how the winning design works.</p>
<h2>Target</h2>
<p>Let’s start with the obvious: <strong>speed is platform-dependent</strong>.
I focused my optimization on the modern x86-64 architecture (so, Intel and AMD chips).</p>
<p>The classic metric used to compare performance there is <strong>cpb</strong>:
the number of CPU cycles spent to generate a byte of output.
All cryptographic papers <a href="https://bench.cr.yp.to/supercop.html">compute and compare that metric</a>.
A slightly lower cpb, in software or hardware, can weigh in the balance
just enough to make a primitive win a competition,
or become widely used by the major websites of the world.</p>
<p>To improve your cpb, you can do three things:</p>
<ol>
<li>Generate more bytes for the same amount of work, or</li>
<li>Do less work to generate the same amount of bytes, or</li>
<li>Parallelize work.</li>
</ol>
<p>We will do all of the above.</p>
<p>Therefore, to boot with point 1, we need to output more bits on each iteration.</p>
<p>I am worried that people might say,
“this is not a PRNG unless it outputs 32-bit numbers,” or “64-bit numbers”.
Or more generally, “PRNGs must only rely on this subset of x86-64”;
as if some instructions, such as <code>POPCNT</code>, or some registers, such as <code>%xmm7</code>, are off-limits.</p>
<p>But PRNGs are engineering: they try to make the best of the CPU, decade after decade!
They relied on <code>ROL</code> when it came, and on <code>%rax</code> when 64-bit CPUs landed.
Sure, it means that this algorithm might be slower on ARM (although that remains to be seen);
but 64-bit PRNGs were heavily used before 2019’s Android switch to a required 64-bit support!</p>
<p>So things evolve with the hardware.
And today, Intel and AMD CPUs support 256-bit operations through <a href="https://software.intel.com/en-us/articles/how-intel-avx2-improves-performance-on-server-applications">AVX2</a>.</p>
<p>Just like RC4 outputs 1 byte, and drand48 can only output 4 at a time;
just like pcg64 can only output 8 at a time;
we will output 32 bytes at a time.</p>
<p>Obviously, while 8 bytes could be output as a 64-bit number,
which most programming languages have a built-in type for,
few have a type for 16 bytes (C’s <a href="https://gcc.gnu.org/onlinedocs/gcc/_005f_005fint128.html"><code>__uint128_t</code></a> being a notable exception);
fewer yet have one for 32 bytes (aside from intrinsics).</p>
<p>So we must say goodbye to the typical PRNG function prototype
(here taken from Vigna’s <a href="http://xoshiro.di.unimi.it/hwd.c">HWD</a> benchmark program):</p>
<pre><code>static uint64_t next(void);
</code></pre>
<p>Instead, we can have the generator take a buffer to fill
(here taken from <a href="https://github.com/espadrine/shishua/blob/master/prng.c">my own benchmark program</a>):</p>
<pre><code>void prng_gen(prng_state *s, __uint64_t buf[], __uint64_t size);
</code></pre>
<p>Are there disadvantages?</p>
<p>Well, if your generator outputs 32 bytes at a time,
you need the consumer to give an array that is a multiple of 32 bytes;
ideally, an array aligned to 32 bytes.</p>
<p>Although, with a tiny bit more work, you don’t.
Just fill a buffer. Output from it what has not been consumed;
refill it as needed.</p>
<p>That does make <em>latency</em> unpredictable: some calls will only read the buffer.
But it averages out the same.</p>
<p>So now we generate more bytes for the same amount of work.
Next step: how do we parallelize work?</p>
<h2>Parallelism</h2>
<p>The CPU offers an incredible wealth of parallelism at every level.</p>
<p>First, of course, are the SIMD instructions (Single-Instruction, Multiple Data).
For instance, AVX2 does four 64-bit additions in parallel, or eight 32-bit ones, etc.</p>
<p>In cryptography, it has been severely relied upon for fifteen years.
Notably, <a href="https://github.com/floodyberry/supercop/tree/master/crypto_stream/chacha20/dolbeau/amd64-avx2">ChaCha20</a> gains an incredible amount of speed from it;
most important primitives that don’t use AESNI rely on that.
For instance, <a href="https://norx.io/data/norx.pdf">NORX</a> and <a href="https://cryptojedi.org/papers/gimli-20170627.pdf">Gimli</a> are designed with that in mind.</p>
<p>Recently, there has been increasing interest in the non-cryptographic PRNG community.</p>
<p>In particular, existing primitives not designed for SIMD can be the basis
for building a very fast PRNG.</p>
<p>For instance, Sebastiano Vigna, while pushing for his <a href="http://prng.di.unimi.it/#speed">xoshiro256++</a> design
in the Julia programming language’s standard library,
<a href="https://github.com/JuliaLang/julia/issues/27614#issuecomment-548154730">he learnt</a> that concatenating the output of eight concurrent instances of the PRNG
initialized differently, was made very fast by having each operation of the design
performed simultaneously on each PRNG.</p>
<p>SIMD is one level of CPU parallelism, but not the only one.
I encourage you to read <a href="https://espadrine.github.io/blog/posts/a-primer-on-randomness.html">the previous article on the subject</a>
to get a better picture, but I’ll mention what I relied upon.</p>
<p><strong>CPU pipelining</strong> processes multiple instructions at different stages of processing.
When well-ordered to limit interstage dependencies, instructions can be processed faster.</p>
<p><strong>Superscalar execution</strong> makes the computation part of instruction happen in parallel.
But they must have no read/write dependencies to do so.
We can fit the design to reduce the risk of stalls,
by making the write part happen long before the read.</p>
<p><strong>Out-of-order execution</strong> lets the processor execute instructions that happen later,
even though a previous instruction is not yet done, if the later instruction has no
read/write dependency to it.</p>
<p>All right, let’s dig our hands into the implementation!</p>
<h2>Design</h2>
<p>Let’s walk through the design of something we will call SHISHUA-half,
for reasons that will slowly become obvious along the article.</p>
<p>It looks like this:</p>
<p><img src="../assets/shishua-the-fastest-prng-in-the-world/shishua-diagram.svg" alt="SHISHUA diagram" /></p>
<p>Let’s dive in line by line.</p>
<pre><code class="language-c">typedef struct prng_state {
  __m256i state[2];
  __m256i output;
  __m256i counter;
} prng_state;
</code></pre>
<p>Our state is cut in two pieces that both fit in an AVX2 register (256 bits).
We keep output around in the state to get a bit of speed,
but it is not actually part of the state.</p>
<p>We also have a 64-bit counter; it is also an AVX2 register to ease computation.
Indeed, AVX2 has a bit of a quirk where regular registers (<code>%rax</code> and the like)
cannot directly be transfered to the SIMD ones with a <code>MOV</code>;
it must go through RAM (typically the stack), which costs both latency and
two CPU instructions (<code>MOV</code> to the stack, <code>VMOV</code> from the stack).</p>
<p>We’re now going to look at generation.
We start by loading everything, then we loop over the buffer,
filling it up by 32 bytes at each iteration.</p>
<pre><code class="language-c">inline void prng_gen(prng_state *s, __uint64_t buf[], __uint64_t size) {
  __m256i s0 = s-&gt;state[0], counter = s-&gt;counter,
          s1 = s-&gt;state[1],       o = s-&gt;output;
  for (__uint64_t i = 0; i &lt; size; i += 4) {
    _mm256_storeu_si256((__m256i*)&amp;buf[i], o);
    // …
  }
  s-&gt;state[0] = s0; s-&gt;counter = counter;
  s-&gt;state[1] = s1; s-&gt;output  = o;
}
</code></pre>
<p>Since the function is inlined, the buffer being immediately filled at the start
lets the CPU execute the instructions that depend on it in the calling function right away,
through out-of-order execution.</p>
<p>Inside the loop, we perform three operations on the state in rapid succession:</p>
<ol>
<li><strong>SHI</strong>ft</li>
<li><strong>SHU</strong>ffle</li>
<li><strong>A</strong>dd</li>
</ol>
<p>Hence the name, SHISHUA!</p>
<h3>First, the shift</h3>
<pre><code class="language-c">u0 = _mm256_srli_epi64(s0, 1);              u1 = _mm256_srli_epi64(s1, 3);
</code></pre>
<p>AVX2 does not support rotations, sadly.
But I want to entangle bits from one position in the 64-bit numbers,
to other bit positions! And shift is the next best thing for that.</p>
<p>We must shift by an odd number so that each bit reaches all 64-bit positions,
and not just half.</p>
<p>Shift loses bits, which removes information from our state.
That is bad, so we minimize the loss: the smalles odd numbers are 1 and 3.
We use different shift values to increase divergence between the two sides,
which should help lower the similarity of their self-correlation.</p>
<p>We use rightward shift because the rightmost bits have the least diffusion in addition:
the low bit of <code>A+B</code> is just a XOR of the low bits of <code>A</code> and <code>B</code>, for instance.</p>
<h3>Second, the shuffle</h3>
<pre><code class="language-c">t0 = _mm256_permutevar8x32_epi32(s0, shu0); t1 = _mm256_permutevar8x32_epi32(s1, shu1);
</code></pre>
<p>We use a 32-bit shuffle because it is the only one that is both a different granularity
than the 64-bit operations that we do everywhere else (which breaks 64-bit alignment),
and that can also cross lanes
(other shuffles can only move bits within the left 128 bits if they started on the left,
or within the right 128 bits if they started on the right).</p>
<p>Here are the shuffle constants:</p>
<pre><code class="language-c">__m256i shu0 = _mm256_set_epi32(4, 3, 2, 1, 0, 7, 6, 5),
        shu1 = _mm256_set_epi32(2, 1, 0, 7, 6, 5, 4, 3);
</code></pre>
<p>To make the shuffle really strenghten the output, we move weak (low-diffusion) 32-bit parts
of the 64-bit additions to strong positions, so that the next addition will enrich it.</p>
<p>The low 32-bit part of a 64-bit chunk never moves to the same 64-bit chunk as its high part.
That way, they do not remain in the same chunk, encouraging mixing between chunks.</p>
<p>Each 32-bit part eventually reaches all positions circularly: A to B, B to C, … H to A.</p>
<p>You might notice that the simplest shuffle that follows all those requirements
are simply those two 256-bit rotations (rotation by 96 bits and 160 bits rightward, respectively).</p>
<h3>Third, the addition</h3>
<p>Let’s add 64-bit chunks from the two temporary variables,
the shift one and the shuffle one, together.</p>
<pre><code class="language-c">s0 = _mm256_add_epi64(t0, u0);              s1 = _mm256_add_epi64(t1, u1);
</code></pre>
<p>The addition is the main source of diffusion: it combines bits
into irreducible combinations of XOR and AND expressions across 64-bit positions.</p>
<p>Storing the result of the addition in the state keeps that diffusion permanently.</p>
<h3>Output function</h3>
<p>So, where do we get the output from?</p>
<p>Easy: the structure we built is laid out in such a way that
we are growing two independent pieces of state: <code>s0</code> and <code>s1</code>,
which never influence each other.</p>
<p>So, we XOR them, and get something very random.</p>
<p>In fact, to increase the independence between the inputs that we XOR,
we take the partial results instead: the shifted piece of one state,
and the shuffled piece of the other.</p>
<pre><code>o = _mm256_xor_si256(u0, t1);
</code></pre>
<p>That also has the effect of reducing the read/write dependencies between superscalar CPU instructions,
as <code>u0</code> and <code>t1</code> are ready to be read before <code>s0</code> and <code>s1</code> are.</p>
<p>You may have noticed that we did not talk about the counter yet.
It turns out we handle it at the start of the loop.
We first change the state, and then increment the counter:</p>
<pre><code class="language-c">s1 = _mm256_add_epi64(s1, counter);
counter = _mm256_add_epi64(counter, increment);
</code></pre>
<p>The reason we change the state first, and then update the counter,
is so that <code>s1</code> becomes available sooner,
reducing the risk that later instructions that will read it get stalled
in the CPU pipeline.
It also avoids a direct read/write dependency on the counter.</p>
<p>The reason we apply the counter to s1 and not s0,
is that both affect the output anyway.
However, <code>s1</code> loses more bits from the shift,
so this helps it get back on its feet after that harmful shearing.</p>
<p>The counter is not necessary to beat PractRand.
Its only purpose is to set a lower bound of 2<sup>69</sup> bytes = 512 EiB
to the period of the PRNG:
we only start repeating the cycle after one millenia at 10 GiB/s,
which is unlikely to ever be too low for practical applications in the coming centuries.
Thanks to this, there are no bad seeds.</p>
<p>Here are the increments:</p>
<pre><code class="language-c">__m256i increment = _mm256_set_epi64x(1, 3, 5, 7);
</code></pre>
<p>The increments are picked as odd numbers,
since only coprimes of the base cover the full cycle of the finite field GF(2<sup>64</sup>),
and all odd numbers are coprime of 2.</p>
<p>(In other words, if you increment by an even number between integers 0 to 4,
wrapping around to 0 when you go past 4,
you get the sequence 0-2-0-2-…, which never outputs 1 or 3;
but an odd increment goes through all integers.)</p>
<p>We use a different odd number of each 64-bit number in the state,
which makes them diverge more, and adds a tiny bit of stirring.</p>
<p>I picked the smallest odd numbers so that they don’t look like magic numbers.</p>
<p>So, there we go! That is how the state transition and output function work.</p>
<p>Now, how do we initialize them?</p>
<h3>Initialization</h3>
<p>We initialize the state with the hex digits of Φ,
the irrational number that is least approximable by a fraction.</p>
<pre><code class="language-c">static __uint64_t phi[8] = {
  0x9E3779B97F4A7C15, 0xF39CC0605CEDC834, 0x1082276BF3A27251, 0xF86C6A11D0C18E95,
  0x2767F0B153D27B7F, 0x0347045B5BF1827F, 0x01886F0928403002, 0xC1D64BA40F335E36,
};
</code></pre>
<p>We take a 256-bit seed, which is common in cryptography,
and doesn’t really hurt in non-cryptographic PRNGs:</p>
<pre><code class="language-c">prng_state prng_init(SEEDTYPE seed[4]) {
  prng_state s;
  // …
  return s;
}
</code></pre>
<p>We don’t want to override a whole piece of state (<code>s0</code> nor <code>s1</code>) with the seed;
we only want to affect half.
That way, we avoid having debilitating seeds that,
purposefully or accidentally, set the state to a known weak start.</p>
<p>With half of each state intact, they still keep control over 128 bits of state,
which is enough entropy to start and stay strong.</p>
<pre><code class="language-c">s.state[0] = _mm256_set_epi64x(phi[3], phi[2] ^ seed[1], phi[1], phi[0] ^ seed[0]);
s.state[1] = _mm256_set_epi64x(phi[7], phi[6] ^ seed[3], phi[5], phi[4] ^ seed[2]);
</code></pre>
<p>Then we do the following thing a <code>ROUNDS</code> number of times:</p>
<ol>
<li>Run <code>STEPS</code> iterations of SHISHUA,</li>
<li>Set one piece of the state to the other, and the other to the output.</li>
</ol>
<pre><code class="language-c">for (char i = 0; i &lt; ROUNDS; i++) {
  prng_gen(&amp;s, buf, 4 * STEPS);
  s.state[0] = s.state[1];
  s.state[1] = s.output;
}
</code></pre>
<p>Setting to the output increases the diffusion of the state.
In the initialization, the added work and state correlation don’t matter,
since this is only done a few times, once.
You only care about diffusion in initialization.</p>
<p>I picked values of 2 for <code>STEPS</code> and 10 for <code>ROUNDS</code>
after looking at how much they impacted seed correlation.</p>
<p>(I computed seed correlation by counting the “unusual” and “suspicious” anomalies
coming out of the PractRand PRNG quality tool.)</p>
<h2>Performance</h2>
<p>Speed measurement benchmarks are tricky for so many reasons.</p>
<ul>
<li><strong>Clock</strong> measurements can lack precision.</li>
<li>The CPU has so much <strong>parallelism</strong>, that tracking when instructions start and end,
is both nondeterministic and heavily dependent on other events on the CPU.</li>
<li>Obviously, from one CPU vendor to the next, the resuts will be different.
That is also true from one CPU <strong>series</strong> to the next from the same vendor.</li>
<li>CPUs nowadays have <strong><a href="https://www.intel.com/content/www/us/en/architecture-and-technology/turbo-boost/turbo-boost-technology.html">variable frequency</a></strong>: they get purposefully slower or faster
depending on the need for low power consumption or the risk of high temperature.</li>
</ul>
<p>I use a dedicated CPU instruction, <code>RDTSC</code>, which computes the number of cycles.</p>
<p>To make sure that everyone can reproduce my results, I use a cloud virtual machine.
It doesn’t change the order of the benchmark results compared to a local test;
it also avoids requesting that other people buy the same computer as the one I have.
Finally, there are many use-cases where PRNGs would be used in the cloud on those instances.</p>
<p>I chose Google Cloud Platform’s N2 (Intel chip) and N2D (AMD chip).
The advantage of GCP is that they have chips from both vendors.
We’ll focus on Intel here, but the orders of magnitude are similar for AMD.</p>
<p>To give a bit of context, let’s first look at an old cryptographic generator, RC4.
Impossible to parallelize; I got <strong>7.5 cpb</strong> (cycles spent per generated byte).</p>
<p>Now, let’s look at a very common and fast MCG: <a href="https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/">Lehmer128</a>,
the simplest PRNG that passes BigCrush: <strong>0.5 cpb</strong>. Wow, not bad!</p>
<p>Next, a recent mixer that is the basis for fast hash tables: <a href="https://github.com/wangyi-fudan/wyhash/blob/master/wyhash_v6.h">wyrand</a>.
<strong>0.41 cpb</strong>, slightly better!</p>
<p>Among Vigna’s fast PRNG, some don’t pass 32 TiB of PractRand, but are very fast.
<a href="http://prng.di.unimi.it/xoshiro256plus.c">Xoshiro256+</a> fails at 512 MiB but is among the fastest of the bunch: <strong>0.34 cpb</strong>.</p>
<p>Let’s look at a recent entry, from earlier this year: <a href="http://www.romu-random.org/">RomuTrio</a>.
It claims the title of fastest PRNG in the world: <strong>0.31 cpb</strong>.</p>
<p>Alright, enough. How does SHISHUA-half fare?</p>
<p><strong>0.14 cpb</strong>. Twice as fast as RomuTrio.</p>
<p><img src="../assets/shishua-the-fastest-prng-in-the-world/speed-partial.svg" alt="Speed plot" /></p>
<p>Cool. For kicks, let’s test a more recent cryptographic generator, ChaCha8.
It reaches… <strong>0.12 cpb</strong>.</p>
<p>Oops.</p>
<p>SIMD really works its magic!</p>
<p>To the cryptographic community, <a href="https://twitter.com/hashbreaker/status/1023965175219728386">this is not a complete surprise</a>.
ChaCha8 is just insanely easy to parallelize.
It is just a counter in a diffused state, well-hashed.</p>
<p>Worse yet, remember how the Julia team looked at combining multiple instances of Vigna’s design
to make a fast SIMD PRNG? Let’s look at Vigna’s fastest result using this technique:
<a href="http://prng.di.unimi.it/#speed">Xoshiro256+ 8 times</a>. <strong>0.09 cpb</strong>!</p>
<p>(Technically, it is not that good on my laptop;
not sure why it is faster than ChaCha8 in GCP but slower locally.
On my machine, SHISHUA-half is faster than this, but slower than ChaCha8.)</p>
<hr />
<p>So, let’s beat all that.</p>
<p>Now you probably guess why we called our earlier primitive SHISHUA-half.</p>
<p>It turns out getting twice as fast is easy by doubling SHISHUA-half.</p>
<p>Similar to the Julia insights, we have two PRNGs initialized differently
(four blocks of 256-bit state),
outputting their thing one after the other.</p>
<p>But with more state, we can output even more stuff,
by combining the four states pairwise:</p>
<pre><code class="language-c">o0 = _mm256_xor_si256(u0, t1);
o1 = _mm256_xor_si256(u2, t3);
o2 = _mm256_xor_si256(s0, s3);
o3 = _mm256_xor_si256(s2, s1);
</code></pre>
<p>And that is how you get SHISHUA, and its <strong>0.06 cpb</strong> speed.</p>
<p>That is twice as fast as the previously-fastest in the world that passes 32 TiB of PractRand.
You can barely see it in the graph.</p>
<p>I guess my point is that it is somewhat competitive.</p>
<p>(In fact, it is even faster on my laptop, at 0.03 cpb,
but I want to stick to my benchmark promises.)</p>
<p>Hopefully, it stays the fastest in the world for at least a few weeks?
(Please make it so.)</p>
<h2>Quality</h2>
<p>It passes BigCrush and 32 TiB of PractRand without suspicion.</p>
<p>In fact, all of its four outputs do.</p>
<p>One of the not-ideal aspects of the design is that SHISHUA is <strong>not reversible</strong>.</p>
<p>You can see this with a reduction to a four-bit state, with <code>s0 = [a, b]</code> and <code>s1 = [c, d]</code>.
The shift will yield <code>[0, a]</code> and <code>[0, d]</code>; the shuffle will give <code>[b, c]</code> and <code>[d, a]</code>.</p>
<p>The new <code>s0</code> is <code>[b, c] + [0, a] = [b⊕(a∧c), a⊕c]</code>, and <code>s1</code> is <code>[d, a] + [0, c] = [d⊕(a∧c), a⊕c]</code>.</p>
<p>If <code>a = ¬c</code>, then <code>a⊕c = 1</code> and <code>a∧c = 0</code>, thus <code>s0 = [b, 1]</code> and <code>s1 = [d, 1]</code>.
So there are two combinations of <code>a</code> and <code>c</code> that give the same final state.</p>
<p>It is not an issue in our case, because the 64-bit counter is also part of the state.
So you have a minimum cycle of 2⁷¹ bytes (128 bytes per state transition),
which lasts seven millenia at 10 GiB/s.
So that counterbalances the lost states.</p>
<p>Besides, even despite the irreversibility,
the average state transition period is <code>2^((256+1)÷2)</code>.
That gives an average cycle of 2¹³⁵ bytes
(more than a trillion times the age of the universe to reach at 10 GiB/s).
Although, in my opinion, average cycles are overrated,
as they give no indication on the quality of the output.</p>
<p>Alright, here is the distilled benchmark:</p>
<table id=benchmark>
  <tr><th>Name   <th>Performance <th>Quality <th>Seed correlation
  <tr><td>SHISHUA       <td>0.06 <td>>32 TiB <td>>256 GiB
  <tr><td>xoshiro256+x8 <td>0.09 <td>  1 KiB <td>   0 KiB
  <tr><td>ChaCha8       <td>0.12 <td>>32 TiB?<td> >32 TiB?
  <tr><td>RomuTrio      <td>0.31 <td>>32 TiB <td>   1 KiB
  <tr><td>xoshiro256+   <td>0.34 <td>512 MiB <td>   1 KiB
  <tr><td>wyrand        <td>0.41 <td>>32 TiB <td>   8 KiB
  <tr><td>Lehmer128     <td>0.44 <td>>32 TiB <td>   1 KiB
  <tr><td>RC4           <td>7.48 <td>  1 TiB <td>   1 KiB
</table>
<ol>
<li><strong>Performance</strong>: in number of CPU cycles spent per byte generated,
on N2 GCP instances. On N2D (AMD), the order is the same.</li>
<li><strong>Quality</strong>: level at which it fails PractRand. We show a <code>&gt;</code> if it did not fail.
We put a question mark if we have not proved it.</li>
<li><strong>Seed correlation</strong>: PractRand on interleaving of bytes from eight streams
with seeds 0, 1, 2, 4, 8, 16, 32, 64.
We use PractRand with folding 2 and expanded tests.</li>
</ol>
<p><img src="../assets/shishua-the-fastest-prng-in-the-world/speed.svg" alt="Speed plot" /></p>
<h2>Next</h2>
<p>While there are no practical issue with irreversibility in our case,
it also means that we can improve on SHISHUA.</p>
<p>My ideal PRNG would have the following properties:</p>
<ol>
<li><strong>The state transition is a circular permutation</strong>, giving a way-more-than-enough 2¹⁰²⁴ bytes cycle.
As in, it would take more than 10²⁸² times the age of the universe to reach the end at 10 GiB/s,
instead of SHISHUA’s seven millenia.
It is not exactly “better” (impossible is impossible);
but if we can reduce the design to a smaller state without affecting diffusion,
we might be able to get a faster PRNG.
Do you think we might be able to fit one in ARM’s 128-bit NEON registers?
Also, we would no longer need the counter, removing two additions.</li>
<li><strong>The output function is provably irreversible</strong>.
The way SHISHUA XORs two independent numbers already has that property,
but I haven’t proved that the numbers are truly decorrelated.</li>
<li><strong>The state initialization is irreversible</strong>
with each state having 2¹²⁸ possible seeds (to prevent guessing the seed).
The way SHISHUA sets the state to its own output is likely irreversible.
After all, it uses SHISHUA’s state transition (partially irreversible)
and its output function (seemingly irreversible, see point 2).</li>
<li><strong>The state initialization has perfect diffusion</strong>:
all seed bits affect all state bits with equal probability.
I’d like to compute that for SHISHUA.</li>
</ol>
<p>One issue holding back PRNGs and cryptography overall is the lack of better, general-purpose tooling.</p>
<p>I want a tool that can instantly give me an accurate score,
allowing me to compare designs on the spot.</p>
<p>PractRand is great compared to what came before it; but:</p>
<ul>
<li>It cannot rate high-quality generators, making comparisons between them impossible.
We just get to say “well, they both had no anomalies after 32 TiB…”</li>
<li>It takes weeks to run…</li>
</ul>
<p>I believe great improvements are coming.</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2020-04-18T16:59:00Z",
  "keywords": "prng, crypto" }
</script> ]]>
        </content>
      </entry>
      <entry>
        <id>https://espadrine.github.io/blog/posts/a-primer-on-randomness.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/a-primer-on-randomness.html"/>
        <title>A Primer On Randomness</title>
        <published>2020-03-27T15:17:57Z</published>
        <category term="prng"/>
<category term="crypto"/>
        <content type="html">
          <![CDATA[ <h1>A Primer On Randomness</h1>
<p>Last October, during a one-week hiking holiday in the birthplace of alpinism,
I got particularly interested in random generators.</p>
<p>Four reasons why they are fascinating:</p>
<ol>
<li>It is only once you track it that you realize just in which gargatuan proportions you <strong>exude information</strong>. Even tiny systems that encode very little data and whose entire purpose is to never leak it (ie, random generators), do so in ways that can be measured, and even exploited. In every instant of your life, during every interaction with someone, billions of muscle movements, tiny and large, only occur because of past events burnt into your brain’s circuits, and betray this private history. Given enough of it, an aggregator could rewind the world and extract minute details from the past.</li>
<li>All of <strong>symmetric cryptography</strong> completely hinges on randomness. Security proofs fully rely on the analysis of how little information you can extract from a stream, which requires the stream to effectively look random.</li>
<li>Studying them, and trying your hand at making them, helps you understand the <strong>scientific method</strong> better. Most real-world principles can never be proved with absolute certainty; you need to accurately detect a signal in the noise, and measure the likelihood that this signal is not just you seeing patterns in the static.</li>
<li>Finally, it helps both understand <strong>the virtue of mixing</strong>, and how best to stir. The effect of mixing is exponential, which is unnatural to mentally harness. On the plus side, when done well, you get fluid exchange of information, remix, and cultural explosion. On the minus side, you get COVID-19 everywhere. Striking the right balance gets you far: many optimizing algorithms rely on it such as genetic algorithms, stochastic gradient descent, or cross-validation sampling in machine learning, which each are heavy users of pseudo-random sources. The results speak for themselves: AlphaGo, for instance, beat the best human player at one of the hardest games on Earth, using Monte-Carlo Tree Search. Yes, you guessed it, they call it Monte Carlo for a reason.</li>
</ol>
<h2>Information Theory</h2>
<p>A good Pseudo-Random Number Generator (or PRNG for short) is indistinguishable from a true random output.</p>
<p><em>So, where do we get this true random output you speak of?</em></p>
<p>True randomness has statistical meaning, but it is impossible to prove or disprove.
You can only have a high confidence.</p>
<p>You might hope that true randomness can be extracted from nature, but that is also not true.
The physical realm contains a large quantity of data storage (“space”),
and laws that alter it: gravity, electromagnetism, …
Nature is a state transition function and an output; that is also the structure of a PRNG.</p>
<p>Physical processes that claim to output “true” randomness rely on the large amount of information stored in the environment, and that environment’s diffuse state scrambling, that is presumably extremely hard for an attacker to detect.</p>
<p>For instance, the fine trajectory of electrons attracted from atom to atom through an electrical circuit causing minuscule delays, or the chaotic motion of gaseous atoms, or stronger yet, quantum behavior of particles.</p>
<p>Some physicists may argue that the world is not fully deterministic.
However, the Copenhagen Interpretation or Multiverse fans
cannot disprove the possibility of a non-local world that complies with the Bell-EPR paradox,
for instance through superdeterminism or pilot waves.
(Sorry for those that don’t care about quantum mechanics;
you don’t need to understand this paragraph to carry on.)</p>
<p>Since true randomness is not real, how do we get close?</p>
<p>Let’s say that you generate bits. If all the bits were <code>1</code>, it would be pretty predictable, right?
So the frequency of ones should converge to one out of two, which is what probability half is.</p>
<p>But if the output was a one followed by a zero continuously (<code>101010…</code>), it would be predictable too!
So the frequency of the sequence <code>10</code> in the output should converge to one out of four.</p>
<p>More generally, every possible sequence of <code>n</code> bits should appear with a frequency converging to <code>1÷2ⁿ</code>.</p>
<p>(A common romanticization of that idea is the comment that the decimals of π encode the entire works of Shakespeare.
π being irrational, its formulation is <a href="https://mathworld.wolfram.com/WeylsCriterion.html">orthogonal to any fractional representation</a>, which is what decimals are.
That gives strong credence to the conjecture that its digits form a truly random sequence.)</p>
<p>That idea might make you uneasy. After all, it gives an impossible requirement on the memory size of a generator.</p>
<h3>Memory</h3>
<p>If your state contains <code>i</code> bits, what is the largest sequence of consecutive ones it can output?</p>
<p>Well, since the PRNG is deterministic, a given state will always yield the same output.
There are <code>2ⁱ</code> possible state configurations, so with this entropy, you can at best output <code>i·2ⁱ</code> bits
before you arrive at a previous state and start repeating the same output sequence again and again.</p>
<p>At least, with an ideal PRNG, you know that one given configuration will output a sequence of <code>i</code> ones.
The previous configuration (which transitioned to the configuration that outputs the <code>i</code> ones)
cannot also output a sequence of <code>i</code> ones:
if two configurations yielded the same output, then there would be some <code>i</code>-bit output that no configuration produced.
That would not be an ideal PRNG.</p>
<p>So let’s say that the previous configuration gives <code>i-1</code> ones (a zero followed by a ton of ones),
and that the next configuration gives <code>i-1</code> ones (a ton of ones followed by a zero).
That is a total of a maximum of <code>3×i-2</code> consecutive ones.</p>
<p>Thus, you cannot get <code>3×i-1</code> consecutive ones…
which a true random generator would output with a frequency of <code>1 ÷ 2^(3×i-1)</code>.
A statistical deviation that you can detect to disprove that a generator is truly random!</p>
<p>Conversely, it means that <em>true generators require infinite memory</em>, which is impossible in the real world.</p>
<p>(By the way, yes, it does seem like computing all the digits of π requires infinite memory.
All current algorithms need more memory the more digits are output.)</p>
<p>In practice, you get around the issue by picking a state size <code>i</code> large enough that
detecting this statistical anomaly requires a millenia’s worth of random output, too much for anyone to compute.</p>
<h3>Cycle Analysis</h3>
<p>So, once we have picked a state size, now we have an upper bound for the period of the PRNG:
it will repeat the same sequence at least every <code>2ⁱ</code> bits.</p>
<p>But of course, your mileage may vary. An imperfect generator might have a much lower period.
Unless you have a mathematical proof for a <strong>lower bound</strong>, maybe your family of generators
has a seed (an initialization parameter) which results in the same output being repeated over and over…
That is called a fixed point.</p>
<p>Even if there are no fixed point, there could be a large number of seeds that start repeating soon!
(That was a real <a href="https://www.cs.cornell.edu/people/egs/615/rc4_ksaproc.pdf">vulnerability in the RC4 cipher</a>, by the way.)</p>
<p>On the plus side, there is a counterintuitive phenomenon that develops
when a set of links randomly connect with each other in closed chains.
Most links end up on long chains.
For instance, with two links, they will be connected in a chain half the time;
with three links, each link will be connected to another link with probability ⅔; etc.</p>
<p>Better yet, if you increase the number of links linearly,
you decrease the proportion of links that are part of small chains exponentially.</p>
<p>The bottom line is this: you can always put lipstick on the pig by increasing the state size,
and your generator will look good.</p>
<p>However, a fundamentally better generator would have become even better yet with an increased state size.</p>
<h3>Reversibility</h3>
<p>If you build out the design at random, a danger lingers.
Unless you are careful, you might build an irreversible generator.
Given a state after a generation,
can you mathematically compute the previous state?</p>
<p>If you can’t,
then there are multiple initial states that can transition to the current state.
That means some states can never happen,
because there are no initial state that transitions to them;
they got stolen by the states with multiple previous states pointing to it!</p>
<p>That is bad. Why?</p>
<p>First, it reduces the potency of your state size
(since a percentage of possible states are unreachable).</p>
<p>Second, many seeds merge into the rail tracks of other seeds,
converging to a reduced set of possible streams and outputting the same values!
Not only does this create inter-seed output correlation,
it also means that <em>a given stream will likely degrade in period</em>.</p>
<p><img alt='Irreversible PRNG example.' src='../assets/a-primer-on-randomness/irreversible-prng.svg' width=350px'>
<p>It could look good for many terabytes, and suddenly reach a fixed point,
and output the same number over and over.</p>
<p>In fact, if the states transition to randomly picked states,
the average cycle that you eventually get to,
<a href="https://burtleburtle.net/bob/rand/talksmall.html">loops every 2<sup>(n+1)÷2</sup></a>.</p>
<p>If you build a <strong>reversible</strong> algorithm,
at least all streams are a cycle,
so inter-seed correlation is not inevitable.</p>
<p>Some streams can have really long cycles.
Because they include a lot of states,
a starting seed is more likely to land in a long-cycle state.
The average period becomes 2<sup>n-2</sup>, almost the square of the length.</p>
<p><img alt='Reversible PRNG example.' src='../assets/a-primer-on-randomness/reversible-prng.svg' width=350px'>
<p>Note that a reversible design does not mean that the state cycles through all possible combinations.
It just means that each state points to exactly one other state, and has exactly one state leading to it.
In other words, it is a <em>bijection</em>, but not a <em>circular permutation</em>.</p>
<p><img alt='Circular permutation example.' src='../assets/a-primer-on-randomness/circular-prng.svg' width=350px'>
<h3>Diffusion</h3>
<p>Claude Shannon made <a href="https://www.iacr.org/museum/shannon/shannon45.pdf">a very good point the other day</a> (I think it was in 1945?) about ciphers.
An ideal pseudo-random source is such that any bit of the input flips half the bits of the output.</p>
<p>More precisely, ideally, the probability that any bit of the stream flips if a given bit of the state flips, should be ½.
That is called <strong>diffusion</strong> of the state.</p>
<p>After all, if it wasn’t ½, I could start making good guesses about whether this bit of the state is set,
and slowly recover pieces of the state or even the key.
And suddenly, I can predict the whole stream.</p>
<p>A related concept is <strong>confusion</strong> of the key.
Ideally, each bit of the output depends equally on a combination of all bits of the key.
So, each bit of the key should change each bit of the stream,
for half of the set of possible configurations of the key’s other bits.</p>
<p>Each bit of the stream should therefore be a complex combination of all of the key’s bits,
while each bit of the key should have an impact stretched along the whole stream.</p>
<p>These properties particularly matter for cryptographic primitives such as ChaCha20,
where the seed of the PRNG is essentially the cipher key.
Their analysis and understanding still matter for PRNG quality;
although some designs don’t take confusion seriously,
leading to severe correlation of distinct seeds.</p>
<h2>Tooling</h2>
<p>Back in the seventies, there was no tooling to pragmatically study the quality of a generator.
That made the PRNG hobby somewhat impractical.</p>
<p>As a sad result, some people produced subpar results, such as IBM’s infamous <a href="https://en.wikipedia.org/wiki/RANDU">RANDU</a>:</p>
<blockquote>
<p>It fails the spectral test badly for dimensions greater than 2, and every integer result is odd.</p>
</blockquote>
<p>Fortunately, great strides were made since.
Anyone can get going quickly, up until they start having competitive results.</p>
<h3>History</h3>
<p>A first step was Donald Knuth’s description of the use of <strong>Chi-Squared tests</strong> in 1969.</p>
<p>While its application to generators was described in Knuth’s seminal work
<em>The Art of Computer Programming</em>, we have to thank Karl Pearson for the concept.</p>
<p>As the story goes, Pearson was disgruntled at scientists estimating all their results
based on the assumption that their statistical distributions were always normal,
when in some cases they very clearly were not. They just didn’t really have any other tool.</p>
<p>So he worked through the theory. Say you make a claim that some value, for which you have samples,
follows a given statistical distribution. (A uniform one perhaps? Like our PRNG outputs?)
Call that “<strong>the Null Hypothesis</strong>”, because it sounds cool.</p>
<p>Your evidence is a set of samples that belong in various categories.
Your null hypothesis is the belief that each category <code>i ∈ {1,…,k}</code> appears with probability <code>pᵢ</code>.
Maybe the two classes are 0 and 1; maybe they are the 256 possible bytes.</p>
<p>There are <code>oᵢ</code> <em>observed</em> samples in category <code>i</code>.
The theoretical, <em>expected</em> number of samples should be <code>eᵢ</code> = <code>n·pᵢ</code>.
You compute the <strong>Chi-Squared statistic</strong>: <code>χ²</code> = <code>Σ (eᵢ - oᵢ)² ÷ eᵢ</code>.</p>
<p>That statistic follows a distribution of probabilities,
depending on the degrees of freedom of the problem at hand.
If we are looking at random bytes, each generation must be one of 256 possible outputs:
so there are 255 degrees of freedom.
(If it is not in the first 255, it must be in the last, so the last one is not a degree of freedom.)</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/3/35/Chi-square_pdf.svg" alt="Chi-Squared probability density" /></p>
<p>Each possible value of <code>χ²</code> you get has a probability of being valid for your null hypothesis.
One value is the most probable one. The further you get from it, the least likely it is that your samples are random.</p>
<p>But by how much?</p>
<p>You want to know the probability that a true random generator’s <code>χ²</code> lands
as far from the ideal value as your pseudo-random generator did.
(After all, even a perfect generator rarely precisely lands on the most probable <code>χ²</code>,
which for random bytes is 253 with probability 1.8%.)</p>
<p>You can compute the probability that a true random generator’s <code>χ²</code> is bigger (more extreme) than yours.
That probability is called a <strong>p-value</strong>.
If it is tiny, then it is improbable that a true random generator would get this value;
and so, it is improbable that what you have is one.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/8/8e/Chi-square_distributionCDF-English.png" alt="Chi-Squared distribution" /></p>
<p>With this tool in hand, you can easily check that a process that pretends to be random is not actually so.</p>
<p>Or, as <a href="http://www.economics.soton.ac.uk/staff/aldrich/1900.pdf">Pearson puts it</a>:</p>
<blockquote>
<p>From this it will be more than ever evident how little chance had to do
with the results of the Monte Carlo roulette in July 1892.</p>
</blockquote>
<p>(Not sure why his academic paper suddenly becomes so specific;
maybe he had a gambling problem on top of being a well-known racist.)</p>
<p>Fun sidenote: if you look at the <code>χ²</code> formula, notice that if your observed values all hit their expectations,
you will always end up with a <code>χ²</code> equal to zero, whose p-value is 1.</p>
<p>Uniform random numbers have this awesome property that their p-values should also be uniformly random,
and the p-values of the p-values too, and so on.</p>
<p>The p-value you want is simply one that is not too extreme (eg, higher than 10¯⁵, lower than 1-10¯⁵).
A p-value of 1 immediately disqualifies your null hypothesis!
Perfect fits are not random; you must have anomalies some of the time.</p>
<p>Let’s get back to Donald Knuth. His advice of using this tool to study pseudo-random efforts defined all subsequent work.</p>
<p>In 1996, another PRNG fellow, George Marsaglia, looked at the state of tooling with discontent.
Sure, those Chi-Squared tests were neat.
But writing them by hand was tedious.</p>
<p>Worse, nothing defined what to observe. Bytes are one thing, but they only detect byte-wise bias.
What about bitwise? What if we count bits, and compare that count to a <em>Known Statistic</em> (<strong>bit counting</strong>)?
What if we count the number of successive times one byte is bigger than the one generated just before (<strong>runs test</strong>)?
Or maybe count the number of outputs between the appearance of the same value (<strong>gap test</strong>)?
Or take a random matrix, compute its rank, verify that it validates the <em>Known Statistic</em> (<strong>binary rank</strong>)?</p>
<p>Well, he didn’t think about all those tests,
but he did publish a software package that automatically computed p-values
for a dozen of tests. He called it <em>DIEHARD</em>.</p>
<p>Some are like the ones I described, some are a bit wilder and somewhat redundant,
some have a bit too many false positives to be relied upon.</p>
<p>But it was the start of automation!</p>
<p>And the start of the systematic extermination of the weak generators.</p>
<p>In 2003, Robert G. Brown extended it with an easy-to-use command-line interface, <em><a href="https://webhome.phy.duke.edu/~rgb/General/dieharder.php">Dieharder</a></em>,
that allowed testing without having to fiddle with compilation options, just by piping data to a program.
He aggregated a few tests from elsewhere, such as the NIST’s STS
(which are surprisingly weak for their cryptographic purpose… Those were simpler times.)</p>
<p>A big jump in quality came about in 2007.
Pierre L’Écuyer &amp; Richard Simard published <em><a href="http://simul.iro.umontreal.ca/testu01/tu01.html">TestU01</a></em>, a test suite consisting of three bars to clear.</p>
<ul>
<li>SmallCrush picks 10 smart tests that killed a number of weak generators in 30 seconds.</li>
<li>Crush was a very intensive set of 96 tests that killed even more weaklings, but it took 1h to do so.</li>
<li>BigCrush was the real monster. In 8 hours, its set of 106 tests brutalizes 8 TB of output, betraying subtler biases never before uncovered, even in many previously-beloved PRNGs, such as the still-popular Mersenne Twister. A very sobering moment.</li>
</ul>
<p>TestU01 installed two fresh ideas: having multiple levels of intensity, and parameterizing each test.
The latter in particular really helped to weed out bad generators.
Maybe if you look at all the bits, they look fine, but if you look at every eigth bit, maybe not so much?</p>
<p>The feel of using the programs was still similar, though: you ran the battery of tests,
you waited eight hours, and at the end, you were shown the list of all tests whose p-value was too extreme.</p>
<p>Thence came the current nec-plus-ultra: Chris Doty-Humphrey’s <em>Practically Random</em>,
affectionately called <a href="http://pracrand.sourceforge.net/">PractRand</a>, published in 2010.</p>
<p>It was a step up still from TestU01:</p>
<ul>
<li>Instead of eating one output for one test and throwing it away, it uses output for multiple tests, and even overlaps the same test families along the stream, maximizing the extraction of statistics from each bit of output.</li>
<li>It took the concept of levels of intensity to a new level. The program technically never stops; it continuously eats more random data until it finds an unforgivable p-value. On paper, it is guaranteed to find one, at least once it reaches the PRNG’s cycle length; but that assumes you have enough memory for it to store its statistics. In practice, you can go very far: for instance, the author’s own sfc16 design reached flaws after 512 TiB — which took FOUR MONTHS to reach!</li>
<li>It displays results exponentially. For instance, once at 1 MB of random data read, then at 2, then at 4, then at 8, … Every time, it either tells you that there are no anomalies, or the list of tests with their bad p-values.</li>
</ul>
<p><em>(A small note: don’t expect this tooling to be satisfactory for anything cryptographic.
Their study relies on much more advanced tooling and analysis pertaining to diffusion,
differential cryptanalysis, algebraic and integral attacks.)</em></p>
<p>I am a big believer in tooling.
I believe it is THE great accelerator of civilization by excellence.
The step that makes us go from running at 30 km/h, to speeding at 130 km/h, to rocketing at 30 Mm/h.
In fact, by the end of this series of posts, I hope to publish one more tool to add to the belt.</p>
<h3>Hands-On</h3>
<p>I don’t actually recommend you start out with PractRand for the following reasons:</p>
<ul>
<li>You might make silly mistakes. PractRand can kill generators that looked OK in the 80s fairly instantly. You won’t know if your design didn’t even stand a chance back then, or if it was competitive.</li>
<li>You might have a coding bug. It would be too bad if you threw away a good starting design just because a mask had the wrong bit flipped.</li>
<li>Seeing Chi-Square failures helps understand the beginner design space. Yes, you want the output to have high entropy; but while it is obvious that you don’t want a poorly balanced output (eg. one possible sequence appears too often), you also don’t want a highly structured output (eg. all possible sequences appear exactly as often), since random noise must contain anomalies. Seeing a high-entropy generator fail because bytes were slightly too equiprobable helped me appreciate what was undesirable. It is often counter-intuitive, so these beginner lessons help a lot.</li>
</ul>
<p>I would encourage you to build a silly idea, then pipe 10 MB to <a href="https://www.fourmilab.ch/random/">ent</a>.
Check the entropy calculation (it should be somewhere around 7.9999),
and verify that the Chi-Square p-value is between 0.1% and 99.9% with a set of seeds.</p>
<p>Compare it to a good randomness source: <code>&lt;/dev/urandom head -c 10M | ent</code>.
(When I say good, I mean ChaCha20, which is what Linux uses.)</p>
<p>See what happens when you go from 10M to 100M: does the p-value always decrease, or always increase?
That would be bad, very bad indeed.</p>
<p>Once your Chi-Squared is good, skip all the old tests, and hop into PractRand: <code>./prng | RNG_test stdin64</code>.
I recommend specifying the size of your output, so that PractRand can know what to look out for.</p>
<p>Then, goes the contest.</p>
<p>If you pass 1 MiB: you have beat the sadly very widely-used <a href="http://man7.org/linux/man-pages/man3/drand48.3.html">drand48</a>! (Java, C, …)</p>
<p>If you pass 256 GiB: you are now better than the widely-used <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/emt.html">Mersenne Twister</a>! (Ruby, Python, …)</p>
<p>If you pass 1 TiB: congratulations, you beat the famous <a href="https://cypherpunks.venona.com/archive/1994/09/msg00304.html">RC4</a> stream cipher!
(Used as macOS’s old arc4random source, and actually most websites used it for TLS at some point…)</p>
<p>If you pass 32 TiB: you have won. The <code>RNG_test</code> program automatically stops.
Beware: it takes about a week to compute… when your generator is fast.</p>
<p>Quick advice: remember that p-values should be uniformly random.
It is inevitable to have some of them be labeled “unusual”, or even, more rarely, “suspicious”.
It does not mean you failed.</p>
<p>When the p-value is too extreme, PractRand will show “FAIL!” with a number of exclamation marks proportional to how horrified it is.
Then, the program will stop immediately.</p>
<p>Some tests will fail progressively.
If the same test shows “unusual” at 4 GiB, and “suspicious” at 8 GiB,
it will probably fail at 16 GiB.</p>
<h3>Speed</h3>
<p>Once you beat 32 TiB of PractRand, you know your generator is good —
but to be useful, it also must be the fastest in its class.</p>
<p>A few notes can really help you get it up to speed.</p>
<p>First, pick your target platform.</p>
<p>You will need different optimization tricks if you build for <code>x86_64</code>
(Intel / AMD), or for ARM (phones),
or if you directly target a CMOS integrated circuit,
if you want to burn your PRNG in an ASIC.</p>
<p>Let’s say you want to get the most out of your Intel or AMD chip.
Go as close to the metal as you can. Code in C, C++, or Rust.</p>
<p>Second, understand the assembly output. Looking at the compiled assembly with <code>gcc prng.c -S -o prng.asm</code> can help.
I recommend <a href="https://software.intel.com/en-us/articles/introduction-to-x64-assembly">Intel’s introduction</a>, <a href="https://www.amd.com/system/files/TechDocs/24592.pdf">AMD’s manual</a> and <a href="https://www.agner.org/optimize/instruction_tables.pdf">Agner’s instruction tables</a>.</p>
<p>In particular, a number of amd64 opcodes are inaccessible from the programming language.
You can access them in various ways:</p>
<ul>
<li>The compiler will smartly use them when they apply. For instance, there is an opcode to rotate the bits of a variable leftward: <code>ROL</code>. But all the C programming language offers is shift (<code>&gt;&gt;</code> for <code>SHR</code>, <code>&lt;&lt;</code> for <code>SHL</code>). However, the compiler will map <code>(a &lt;&lt; 1) | (a &gt;&gt; 63)</code> to the 64-bit <code>ROL</code>.</li>
<li>Compilers usually include header files or libraries to access those instructions, by exporting functions that compile down to the corresponding instruction. Those are called <strong><a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">intrinsics</a></strong>. For instance, our friend the 64-bit <code>ROL</code> appears as <code>_rotl64(a, 1)</code>, if you <code>#include &lt;immintrin.h&gt;</code>.</li>
<li>SIMD operations heavily depend on your mastery of the compiler. You can either access them through assembly, compiler flags, or intrinsics (my favorite).</li>
</ul>
<p>Third, understand the way <a href="https://www.agner.org/optimize/microarchitecture.pdf">the CPU processes the assembly</a>.</p>
<ul>
<li><strong><a href="https://software.intel.com/en-us/blogs/2011/11/22/pipeline-speak-learning-more-about-intel-microarchitecture-codename-sandy-bridge">Instruction pipelining</a></strong>: Every instruction executed goes through a number of phases:<br />
① the instruction is decoded from memory and cut in micro-operations (μops);<br />
② each μop is assigned internal input and output registers;<br />
③ the μop reads input registers;<br />
④ it is executed;<br />
⑤ it writes to the output register; and finally<br />
⑥ the output register is written to the target register or memory.<br />
Each of those stages start processing the next instruction as soon as they are done with the previous one, without waiting for the previous instruction to have cleared all steps. As a result, a good number of instructions are being processed at the same time, each being in a different stage of processing.<br />
<em>Example gain: successive instructions go faster if each stage of the second one does not depend on the first one’s later stages.</em></li>
<li><strong>Superscalar execution</strong>: Each μop can be executed by one of multiple execution units; two μops can be executed by two execution units in parallel as long as they don’t have inter-dependencies. There might be one execution unit with logic, arithmetic, float division, and branches; one execution unit with logic, arithmetic, integer and float multiplication; two with memory loads; one with memory stores; one with logic, arithmetic, SIMD permutations, and jumps. Each have a different combination of capabilities.<br />
<em>Example gain: adding a second instruction doing the same thing, or something belonging to another unit, may not add latency if it acts on independent data.</em></li>
<li><strong>Out-of-order execution</strong>: Actually, after the μop is assigned internal registers, it is queued in a ReOrder Buffer (ROB) which can store about a hundred. As soon as a μop’s input registers are ready (typically because of a read/write constraint: another μop wrote the information that this μop needs to read), it gets processed by the first execution unit that can process it and is idle. As a consequence, the CPU can process instructions 2, 3, etc. while instruction 1 waits on a read/write dependency, as long as the next instructions don’t have read/write dependencies with stalled instructions.<br />
<em>Example gain: you can put fast instructions after a slow (or stalled) instruction without latency cost, if they don’t depend on the slow instruction’s output.</em></li>
<li><strong>Speculative execution</strong>: When there is a branch (eg. an if condition), it would be awful if the whole out-of-order instruction pipeline had to stop until the branch opcode gave its boolean output. So the CPU doesn’t wait to know if the branch is taken: it starts processing the instructions that come after the branch opcode. Once it gets the branch opcode output, it tracks all μops that wrongly executed, and reverts all their work, rewrites the registers, etc.</li>
<li><strong>Branch prediction</strong>: To get the best out of speculative execution, CPUs make guesses as to what the boolean output of a branch is going to be. It starts executing the instructions it believes will occur.<br />
<em>Example gain: make your branches nearly always take the same path. It will minimize branch mispredictions, which avoids all the reverting work.</em></li>
</ul>
<p>Finally, beware of the way you test performance. A few tips:</p>
<ol>
<li>Use the <code>RDTSC</code> CPU opcode to count cycles, as below.</li>
<li>Disable CPU frequency variability. CPUs nowadays have things like Turbo Boost that change your frequency based on how hot your processor gets and other factors. You want your CPU to have a fixed frequency for the whole process.</li>
<li>Have as few other processes running as possible. If a process runs in the background, eating CPU, it will affect the results.</li>
</ol>
<pre><code>#include &lt;x86intrin.h&gt;

int main() {
  __int64_t start = _rdtsc();
  generate_one_gigabyte();
  __int64_t cycles = _rdtsc() - start;
  fprintf(stderr, &quot;%f cpb\n&quot;, ((double)cycles) / 1073741824);
}
</code></pre>
<h3>Designs</h3>
<p>The earliest design is the <strong>LCG</strong> (Linear Congruent Generator).
You can recognize its dirt-simple state transition (a constant addition or multiplication),
which has neat consequences on the analysis of its cycle length (typically 2^statesize).
Usually, the output is treated with a shift or rotation before delivery.
While they look fairly random, they can have severe issues, such as hyperplane alignment.
They also tend to be easy to predict once you reverse-engineer them,
which is why they are not used for anything remotely in need of security.</p>
<p>Examples of LCG abound: <a href="http://man7.org/linux/man-pages/man3/drand48.3.html">drand48</a>, <a href="https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/">Lehmer128</a>, <a href="https://www.pcg-random.org/">PCG</a>, …</p>
<p>Then come <strong>Shufflers</strong> (eg. <a href="https://cypherpunks.venona.com/archive/1994/09/msg00304.html">RC4</a>, <a href="http://burtleburtle.net/bob/rand/isaacafa.html">ISAAC</a>, <a href="http://pracrand.sourceforge.net/RNG_engines.txt">EFIIX</a>).
Usually have an “I” in the name (standing for “indirection”).
They try to get randomness by shuffling a list, and they shuffle the list from the randomness they find.
Do not recommend. It is so easy for bias to seep through and combine destructively.
Besides, weeding out bad seeds is often necessary.</p>
<p><strong>Mixers</strong> rely on a simple transition function,
usually addition to what is sometimes called a “gamma” or “<a href="https://mathworld.wolfram.com/WeylsCriterion.html">Weyl coefficient</a>”.
A common non-cryptographic pattern is a state multiplication, just like in LCG,
and the output is XORed with a shifted or rotated version of itself before delivery.
The second step is basically a hash.
(To the security-minded readers: I am not talking about collision-resistant compression functions.)
In cryptography, usually, the mixer uses some ARX combination for bit diffusion (ARX = Add, Rotate, XOR),
and is scheduled in multiple rounds (which are basically skipping outputs).
Examples include <a href="https://github.com/wangyi-fudan/wyhash">wyrand</a>, <a href="http://gee.cs.oswego.edu/dl/papers/oopsla14.pdf">SplitMix</a>, <a href="http://vigna.di.unimi.it/ftp/papers/xorshiftplus.pdf">Xorshift128+</a>, <a href="https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.197.pdf">AES-CTR</a>, and the beloved <a href="https://cr.yp.to/chacha/chacha-20080128.pdf">ChaCha20</a>.</p>
<p>Finally, the most haphazard of them: <strong>chaotic generators</strong>.
They typically have no minimal cycle length, and they just try to stir things up in the state.
For instance, <a href="https://burtleburtle.net/bob/rand/smallprng.html">jsf</a> and <a href="http://www.romu-random.org/">Romu</a>.</p>
<h2>Parting Fun Facts</h2>
<p>I mentionned ChaCha20 a lot, because it is one of my favorite cryptographic primitives.
I’ll give you a few fun facts about it, as goodbye.</p>
<ol>
<li>ChaCha20 <a href="https://cr.yp.to/snuffle/salsafamily-20071225.pdf">initializes its state</a> with the ASCII for “expand 32-byte k”. It’s a wink on the purpose of the cipher: it takes a 256-bit key, and expands it to a large random stream.</li>
<li>It is based on the design of <a href="https://cr.yp.to/export/1996/0726-bernstein.txt">a joke cipher that plays on a US law</a> cataloguing encryption as munition, except if it is a hash. He built it as a simple construction on top of a carefully-constructed hash. Calling the core construction a hash caused him trouble later as <a href="https://cr.yp.to/snuffle/reoncore-20080224.pdf">reviewers misunderstood it</a>.</li>
<li>The initial name of that cipher was Snuffle. (Yes.)</li>
</ol>
<p><a href="https://www.reddit.com/r/prng/comments/fpy6pg/a_primer_on_randomness/">Find comments on Reddit</a>.</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2020-03-27T15:17:57Z",
  "keywords": "prng, crypto" }
</script> ]]>
        </content>
      </entry>
      <entry>
        <id>https://espadrine.github.io/blog/posts/two-postgresql-sequence-misconceptions.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/two-postgresql-sequence-misconceptions.html"/>
        <title>Two PostgreSQL Sequence Misconceptions</title>
        <published>2019-09-05T17:28:59Z</published>
        <category term="sql"/>
        <content type="html">
          <![CDATA[ <h1>Two PostgreSQL Sequence Misconceptions</h1>
<p>✨ <em>With Examples!</em> ✨</p>
<p>Some constructs seem more powerful than the promises they make.</p>
<p>PostgreSQL sequences are like that. Many assume it offers stronger properties
than it can deliver.</p>
<p>They trust them to be the grail of SQL ordering, the one-size-fits-all of strict
serializability. However, there is a good reason Amazon spent design time on
vector clocks in <a href="https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo</a>, Google invested significantly into <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf">Chubby</a>, then
<a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf">Percolator</a>’s timestamp oracle, then <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf">Spanner</a>’s expensive,
atomic-clock-based TrueTime; why Twitter built <a href="https://developer.twitter.com/en/docs/basics/twitter-ids.html">Snowflake</a>, and so many others
built custom timestamp systems.</p>
<ol>
<li>Strict serializability is hard to achieve, especially in a distributed
system, but even in a centralized system with the possibility of failure.</li>
<li>Developers assume the system is strict-serializable, but it usually is not.</li>
<li>When a system provides timestamps, developers will use those as if they were
monotonically strictly increasing atomically throughout the distributed
system, but they often are not, which causes subtle bugs.</li>
</ol>
<h2>The problem space</h2>
<p>To design your system’s properties right, it is often useful or necessary to
determine the order in which events happened. Ideally, you wish for the <strong>“wall
clock” order</strong> (looking at your watch), although instantaneity gets tricky when
events occur at a distance, even within the same motherboard, but especially
across a datacenter, or between cities.</p>
<p>At the very least, you want to reason about <strong>causal ordering</strong>: when that event
happened, did it already see this other event?</p>
<p>A nice property to have, even for a single centralized database, is to give a
monotonically increasing identifier for each row. Most PostgreSQL users rely on
the <code>SERIAL</code> type for that – a sequence. Each insertion will call <code>nextval()</code>
and store an increasing value.</p>
<p>What you implicitly want is to list rows by insertion order, Your mental model
is that each insertion happens at a set “wall clock” time. A first insertion
will happen at T0 and set the identifier 1, the next one happens at T1 and get
number 2, and so on. Therefore, <em>you expect a row with ID N to have causally
been inserted after a row with ID M &lt; N</em>.</p>
<p>Operational order is a consistency constraint strongly associated with isolation
levels. A PostgreSQL database can handle multiple simultaneous operations.</p>
<p><em>(Side note: I could be talking about threads and locks, but I will not, because
those are just tools to achieve properties. PostgreSQL may switch tools to
better meet a given promise (they did so with the serializable level in 2011),
but the promise won’t change.)</em></p>
<p>By default, it promises <strong>Read Committed</strong> isolation: a transaction can witness
the effects of all transactions that commit “before” it does (but not those that
have not committed yet). Their commits are therefore causally ordered by commit
time.</p>
<p>However, nothing else within a transaction has any causal promise with respect
to other transactions. The same <code>SELECT</code> can yield different values;
simultaneous insertions can happen either before, after, or anything in between,
your own insertion.</p>
<p>The highest isolation level PostgreSQL offers is <strong>Serializable</strong> isolation: all
transactions are causally ordered; from <code>BEGIN</code> to <code>COMMIT</code>. Of course,
transactions still execute in parallel; but the database makes sure that
everything that a transaction witnesses can be explained by executing all its
statements either after all statements of another transaction, or before all of
them. It won’t see a changing state within the execution of the transaction.</p>
<p><em>(By the way, PostgreSQL only achieved serializability in 2011, when they
released <a href="https://www.postgresql.org/docs/release/9.1.0/">version 9.1</a> with support for predicate locks. It is hard.)</em></p>
<p>Having a causal order does not mean that this order follows <em>real time</em>: one
insertion may complete at 9:30am <em>after (in causal order)</em> another that
completes later at 10:40am. If you want the additional property that the order
is consistent with wall clock time, you want <strong><a href="https://jepsen.io/consistency/models/strict-serializable">Strict Serializability</a></strong>.</p>
<p>However, <strong>PostgreSQL makes no claim of Strict Serializability</strong>.</p>
<p>Given all this, sequences probably feel much weaker than you initially thought.</p>
<p>You want them to give a continuous set of numbers, but a sequence can yield
values with gaps (1 2 4).</p>
<p>You want them to give a causal order <em>(2 was inserted before 3)</em>, but it can
yield values out of order (1 3 2).</p>
<p>All a sequence promises is to give values that have an order. Not a continuous
order, nor a time order.</p>
<p>Let’s demonstrate both.</p>
<h2>Gaps</h2>
<p>Let’s create a table with a <code>SERIAL</code> identifier. For the purpose of showing
things going right, let’s insert a row.</p>
<pre><code class="language-sql">CREATE TABLE gaps (id SERIAL);
BEGIN;
INSERT INTO order DEFAULT VALUES;
SELECT * FROM gaps;
</code></pre>
<pre><code> id 
----
  1
(1 row)
</code></pre>
<p>Now comes the gap.</p>
<pre><code class="language-sql">BEGIN;
INSERT INTO order DEFAULT VALUES;
ROLLBACK;
</code></pre>
<p>Since we rolled back, nothing happened – or did it?</p>
<p>Let’s now insert another row.</p>
<pre><code class="language-sql">INSERT INTO order DEFAULT VALUES;
SELECT * FROM gaps;
</code></pre>
<pre><code> id 
----
  1
  3
(2 rows)
</code></pre>
<p>Oops! Despite the rollback, the sequence was incremented without being reverted.
Now, there is a gap.</p>
<p>This is not a PostgreSQL bug per se: the way sequences are stored, it just does
not keep the information necessary to undo the <code>nextval()</code> without potentially
breaking other operations.</p>
<p>Let’s now break the other assumption.</p>
<h2>Order violation</h2>
<p>First, a table with a sequence and a timestamp:</p>
<pre><code class="language-sql">CREATE TABLE orders (id SERIAL, created_at TIMESTAMPTZ);
</code></pre>
<p>Let’s set up two concurrent connections to the database. Each will have the same
instructions. I started the first one yesterday:</p>
<pre><code class="language-sql">-- Connection 1
BEGIN;
</code></pre>
<p>I launch the second one today:</p>
<pre><code class="language-sql">-- Connection 2
BEGIN;
INSERT INTO orders (created_at) VALUES (NOW());
COMMIT;
</code></pre>
<p>Let’s go back to the first one:</p>
<pre><code class="language-sql">-- Connection 1
INSERT INTO orders (created_at) VALUES (NOW());
COMMIT;
</code></pre>
<p>Simple enough. But we actually just got the order violation:</p>
<pre><code class="language-sql">SELECT * FROM orders ORDER BY created_at;
</code></pre>
<pre><code> id |          created_at           
----+-------------------------------
  2 | 2019-09-04 21:10:38.392352+02
  1 | 2019-09-05 08:19:34.423947+02
</code></pre>
<p>The order of the sequence does not follow creation order.</p>
<p>From then on, developers may write some queries ordering by ID, and some
ordering by timestamp, expecting an identical order. That incorrect assumption
may break their business logic.</p>
<p>Lest you turn your heart to another false god, that behavior remains the same
with serializable transactions.</p>
<h2>Are we doomed?</h2>
<p>No.</p>
<p>Sure, the systems we use have weak assumptions. But that is true at every level.
The nice thing about the world is that you can combine weak things to make
strong things. Pure iron is ductile, and carbon is brittle, but their alloy is
steel.</p>
<p>For instance, you can get the best of both worlds, causal order and “wall clock”
timestamps, by having a <code>TIMESTAMPTZ</code> field, only inserting rows within
serializable transactions, and setting the <code>created_at</code> field to now, or after
the latest insertion:</p>
<pre><code class="language-sql">BEGIN ISOLATION LEVEL SERIALIZABLE;
INSERT INTO orders (created_at)
SELECT GREATEST(NOW(), MAX(created_at) + INTERVAL '1 microsecond') FROM orders;
COMMIT;
</code></pre>
<p>Indeed, PostgreSQL’s <code>TIMESTAMPTZ</code> has a precision up to the microsecond. You
don’t want to have conflicts in your <code>created_at</code> (otherwise you could not
determine causal order between the conflicting rows), so you add a microsecond
to the current time if there is a conflict.</p>
<p>However, here, concurrent operations are likely to fail, as we acquire a
(non-blocking) SIReadLock on the whole table (what the documentation calls a
relation lock):</p>
<pre><code class="language-sql">SELECT l.mode, l.relation::regclass, l.page, l.tuple, substring(a.query from 0 for 19)
FROM pg_stat_activity a JOIN pg_locks l ON l.pid = a.pid
WHERE l.relation::regclass::text LIKE 'orders%'
  AND datname = current_database()
  AND granted
ORDER BY a.query_start;
</code></pre>
<pre><code>       mode       | relation | page | tuple |     substring
------------------+----------+------+-------+--------------------
 SIReadLock       | orders   |      |       | INSERT INTO orders
 RowExclusiveLock | orders   |      |       | INSERT INTO orders
 AccessShareLock  | orders   |      |       | INSERT INTO orders
</code></pre>
<p>The reason for that is that we perform a slow Seq Scan in this trivial example,
as the <a href="https://www.postgresql.org/docs/current/using-explain.html">EXPLAIN</a> proves.</p>
<pre><code>                                  QUERY PLAN
-------------------------------------------------------------------------------
 Insert on orders  (cost=38.25..38.28 rows=1 width=8)
   -&gt;  Aggregate  (cost=38.25..38.27 rows=1 width=8)
         -&gt;  Seq Scan on orders orders_1  (cost=0.00..32.60 rows=2260 width=8)
</code></pre>
<p>With an <a href="https://www.postgresql.org/docs/current/sql-createindex.html">index</a>, concurrent operations are much more likely to work:</p>
<pre><code class="language-sql">CREATE INDEX created_at_idx ON orders (created_at);
</code></pre>
<p>We then only take a tuple lock on the table:</p>
<pre><code>       mode       | relation | page | tuple |     substring      
------------------+----------+------+-------+--------------------
 SIReadLock       | orders   |    0 |     5 | INSERT INTO orders
 RowExclusiveLock | orders   |      |       | INSERT INTO orders
 AccessShareLock  | orders   |      |       | INSERT INTO orders
</code></pre>
<p>However, the tuple in question is the latest row in the table. Any two
concurrent insertions will definitely read from the same one: the one with the
latest <code>created_at</code>. Therefore, only one of concurrent insertion will succeed;
the others will need to be retried until they do too.</p>
<h2>Subset Ordering</h2>
<p>In cases where you only need a unique ordering for a subset of rows based on
another field, you can set a combined index with that other field:</p>
<pre><code class="language-sql">CREATE TABLE orders (
  account_id UUID DEFAULT gen_random_uuid(),
  created_at TIMESTAMPTZ);
CREATE INDEX account_created_at_idx ON orders (account_id, created_at DESC);
</code></pre>
<p>Then the <a href="https://www.postgresql.org/docs/current/using-explain.html">query planner</a> goes through the account index:</p>
<pre><code class="language-sql">INSERT INTO orders (account_id, created_at)
SELECT account_id, GREATEST(NOW(), created_at + INTERVAL '1 microsecond')
FROM orders WHERE account_id = '9c99bef6-a05a-48c4-bba3-6080a6ce4f2e'::uuid
ORDER BY created_at DESC LIMIT 1
</code></pre>
<pre><code>                                                      QUERY PLAN
-----------------------------------------------------------------------------------------------------------------------
 Insert on orders  (cost=0.15..3.69 rows=1 width=24)
   -&gt;  Subquery Scan on &quot;*SELECT*&quot;  (cost=0.15..3.69 rows=1 width=24)
         -&gt;  Limit  (cost=0.15..3.68 rows=1 width=32)
               -&gt;  Index Only Scan using account_created_at_idx on orders orders_1  (cost=0.15..28.35 rows=8 width=32)
                     Index Cond: (account_id = '9c99bef6-a05a-48c4-bba3-6080a6ce4f2e'::uuid)
</code></pre>
<p>And concurrent insertions on different accounts work:</p>
<pre><code>       mode       | relation | page | tuple |     substring
------------------+----------+------+-------+--------------------
 SIReadLock       | orders   |    0 |     1 | INSERT INTO orders
 RowExclusiveLock | orders   |      |       | INSERT INTO orders
 AccessShareLock  | orders   |      |       | INSERT INTO orders
 SIReadLock       | orders   |    0 |     2 | COMMIT;
</code></pre>
<p>(The first three row are from one not-finished transaction on account 1, the
last is from a finished one on account 2.)</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2019-09-05T17:28:59Z",
  "keywords": "sql" }
</script> ]]>
        </content>
      </entry>
</feed>
