<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>The espadrine blog.</title>
  <subtitle>Tech deep dives, discoveries, and analyses.</subtitle>
  <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/"/>
  <link rel="self" type="application/atom+xml" href="https://espadrine.github.io/blog/feed.xml"/>
  <id>https://espadrine.github.io/blog/feed.xml</id>
  <updated>2023-07-23T17:35:02Z</updated>
      <entry>
        <id>https://espadrine.github.io/blog/posts/chinchilla-s-death.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/chinchilla-s-death.html"/>
        <title>Chinchilla’s Death</title>
        <published>2023-07-23T17:35:02Z</published>
        <category term="gpu"/>
<category term="ml"/>
        <content type="html">
          <![CDATA[ <h1 id="Chinchilla_s_Death">Chinchilla’s Death <a href="#Chinchilla_s_Death" class="autolink-clicker" aria-hidden="true">§</a></h1>
<blockquote>
<p>“With more careful calculations, one can win; with less, one cannot”
— Sun Tzu, <em>The Art of War</em>.</p>
</blockquote>
<p>Making extrapolations is crucial to avoid wasting our computing power on slow
convergence. After all, if you had to walk to the Everest,
you wouldn’t eyeball it: you would use a GPS.</p>
<p>Sometimes you have to look away from the GPS and onto the road, though.
Sometimes things don’t extrapolate through simple formulae.
It was true for XIXth-century physicists with the <a href="https://en.wikipedia.org/wiki/Ultraviolet_catastrophe">ultraviolet catastrophe</a>;
it is true for LLMs too.
What we estimate to be true near the center can deviate widely in the far lands…</p>
<p><img src="https://i.imgur.com/Mf85NuW.png" alt="Image of Minecraft far lands: a terrain that suddenly becomes distorted and overlaps itself cliffly" /></p>
<h2 id="What_s_this_Chinchilla_thing_anyway_">What’s this Chinchilla thing anyway? <a href="#What_s_this_Chinchilla_thing_anyway_" class="autolink-clicker" aria-hidden="true">§</a></h2>
<p>Smaller models have fewer multiplications.
Thus they run faster. Thus they train faster.
However, the theory goes, they eventually reach the limit of their capacity for
knowledge, and their learning slows, while that of a larger model,
with a larger capacity, will overtake them and reach better performance
past a given amount of training time.</p>
<p>While estimating how to get the best bang for the buck during training,
both <a href="https://arxiv.org/abs/2001.08361">OpenAI</a> and <a href="https://arxiv.org/abs/2203.15556">DeepMind</a> attempted to draw the Pareto
frontier. They don’t state explicitly that they use that theory to draw it;
the closest quote that hints at this hidden assumption is from OpenAI:</p>
<blockquote>
<p>We expect that larger models should always perform better than smaller models.
[…]
A model with fixed size will be capacity-limited.</p>
</blockquote>
<p>This presumption is the bedrock of how they compute the Pareto frontier.
In the Chinchilla work, figure 2 shows the training loss of a large number of
training runs for models with varying size.
At a first glance, those curves follow the theory:
the smaller models initially have a lower loss (good),
but eventually it slows down,
and gets overtaken by the curve from a larger model (bad).</p>
<p><img src="../assets/chinchilla-s-death/chinchilla.png" alt="Chinchilla graph comparing the loss curves for many different model sizes" /></p>
<p>In that chart, they drew grey dots every time they pinpointed the smaller model
starting to lose out to a larger model.
The grey line, the Pareto frontier, is how they computed their scaling laws.</p>
<p>The problem with this assumption is that
we have no idea what would happen if we let the smaller model train for longer,
since they stopped its training as soon as it was overtaken.</p>
<p>Enter the LLaMA paper.</p>
<h2 id="Can_Chinchillas_picture_a_Llama_s_sights_">Can Chinchillas picture a Llama’s sights? <a href="#Can_Chinchillas_picture_a_Llama_s_sights_" class="autolink-clicker" aria-hidden="true">§</a></h2>
<p>Earlier this year, Meta trained four models with varying sizes.
Unlike other works, they trained each of them for a very large amount of time;
even the smaller ones.</p>
<p>They published the training run curves:</p>
<p><img src="../assets/chinchilla-s-death/llama1-training.png" alt="Training loss curves for the four LLaMA model sizes" /></p>
<ol>
<li>Each curve first plummets in a <strong>power law</strong>,</li>
<li>and then seemingly enters a <strong>nearly-linear</strong> decrease in loss
(corresponding to a fairly constant rate of knowledge acquisition).</li>
<li>At the very tip of the curve, they all break this line by <strong>flattening</strong>
slightly.</li>
</ol>
<p>Right off the bat, I want to tackle a subtle misconception that people can have
related to the end-of-curve flattening.
They are all trained with gradient descent using a variable learning rate
(which is, roughly,
a hyperparameter for how much to go in the direction of the gradient).
To get a good training, they had to constantly decrease the learning rate,
so that it can detect ever-subtler patterns in the source material.
The formula they use for that decrease is the most widely used:
the cosine schedule.</p>
<p><img src="../assets/chinchilla-s-death/warmup_cosine_schedule.png" alt="Learning rate as a function of training steps under a cosine schedule with
warmup: it first increases linearly, then slopes down with increasing speed,
before reaching an inflection point halfway and slowing down ever slower. Image from Huggingface documentation" /></p>
<p>As you can see from the graph, towards the end of the training run,
the cosine schedule stops decreasing the learning rate at the speed which
yielded such a good, near-linear training loss curve.
The slowdown in learning is an artefact of that.
The model does not necessarily cease to have
the capacity to learn at the same near-linear rate!
In fact, if we had more text to give it,
we would have stretched the cosine schedule,
so its learning rate would have continued to go down at the same rate.</p>
<p>The model’s fitness landscape does not depend on the amount of data
we can feed its training; so the change in learning rate decrease
is not well-justified.</p>
<p>That is not the main point of this article, though.</p>
<p>The training loss curve can be misleading in another way.
Sure, they are all trained on the same data;
but they don’t go through that data at the same speed.
What we want to know is <strong>not</strong> how sample-efficient the model is
(on this front, the larger model clearly learns more from what it saw).
Let’s picture instead a race:
all those models start at the same time,
and we want to know which one crosses the finish line first.
In other words, when throwing a fixed amount of compute at the training,
who learns the most in that time?</p>
<p>Thankfully, we can combine the loss curves with another piece of data that Meta
provided: the amount of time that each model took to train.</p>
<table>
 <tr><th>   Model   </th><th> GPU-hours </th><th> Tokens/second </th>
 <tr><td> LLaMA1-7B  </td><td>   82432  </td><td>    3384.3    </td>
 <tr><td> LLaMA1-13B </td><td>  135168  </td><td>    2063.9    </td>
 <tr><td> LLaMA1-33B </td><td>  530432  </td><td>     730.5    </td>
 <tr><td> LLaMA1-65B </td><td> 1022362  </td><td>     379.0    </td>
</table>
<p><img src="../assets/chinchilla-s-death/llama1-training-speed.svg" alt="LLaMA 1 training loss vs GPU-hours spent" /></p>
<p><a href="https://github.com/espadrine/espadrine.github.com/blob/master/blog/assets/chinchilla-s-death/llama-data.py"><em>(Code for generating the graph here.)</em></a></p>
<p>Let’s first mention that the whole Chinchilla graph that we saw,
covers only a small sliver on the left of this graph.
In that sliver, we see the same behaviour that Chinchilla documents.
Look at the 7B, for instance (which in the Chinchilla graph would actually be
among the top two curves in terms of size):
it initially drops its loss much faster than the bigger models, then slows down,
and the 13B model overtakes it and reaches 1.9 first.</p>
<p>But then, comes a far-lands, unexpected twist: the 7B enters a near-linear
regime, with a steep downward trend, and seems on its way to maybe overpass the
13B again? It is hard to tell on that graph what would happen if the 7B was
trained for longer.</p>
<p>However, the same behaviour seemed to be true between the 13B and the 33B,
where the initial Chinchilla slowdown also gives way to a near-linear regime,
at which point the 13B goes down fast! It is only surpassed by the 33B unfairly,
by granting the latter more than double the compute time.</p>
<p>And the same slowdown-then-speedup occurs between the 33B and the 65B,
to such an extent that the 33B never actually gets overtaken by the 65B.
What the graph shows breaks OpenAI’s and Chinchilla’s assumption:
<strong>the bigger model hasn’t won</strong> (yet).
The slowdown they detected is not actually caused by reaching some capacity limit!</p>
<p>Still, that 7B line is a bit unsatisfactory.
If only Meta had trained it for longer…</p>
<p>Suspense over: they did! They released LLaMA 2 this week!</p>
<h2 id="Time_to_confirm_our_suspicions">Time to confirm our suspicions <a href="#Time_to_confirm_our_suspicions" class="autolink-clicker" aria-hidden="true">§</a></h2>
<p><img src="../assets/chinchilla-s-death/llama2-training.png" alt="Training loss curves for the four LLaMA 2 model sizes" /></p>
<p>We also, again, got the training times:</p>
<table>
 <tr><th>   Model   </th><th> GPU-hours </th><th> Tokens/second </th>
 <tr><td> LLaMA2-7B  </td><td>  184320  </td><td>    3031.9    </td>
 <tr><td> LLaMA2-13B </td><td>  368640  </td><td>    1515.9    </td>
 <tr><td> LLaMA2-34B </td><td> 1038336  </td><td>     533.7    </td>
 <tr><td> LLaMA2-70B </td><td> 1720320  </td><td>     322.1    </td>
</table>
<p><img src="../assets/chinchilla-s-death/llama2-training-speed.svg" alt="LLaMA 2 training loss vs GPU-hours spent" /></p>
<p>Immediately, at a glance, we notice that the training curves don’t match those
of LLaMA 1, even when the models are identical.
As it turns out, LLaMA 2 was trained on double the context size,
and a longer cosine schedule, which unfortunately
has negatively impacted all model sizes.
However, smaller models have been impacted worse than larger ones.
As a result, the 34B model, which in LLaMA 1 remained always better than the 65B
model at any training time spent, now dips slightly above the 70B model,
before overtaking it:</p>
<p><img src="../assets/chinchilla-s-death/llama-training-speed-comparison.webp" alt="LLaMA 1 vs 2 training loss vs GPU-hours spent" /></p>
<p>More importantly, comparing the training speeds strongly confirms our suspicions
from LLaMA 1:</p>
<ol>
<li>First, they are faster than bigger models,</li>
<li>Then, they slow down, and are overtaken by larger models (as per
Chinchilla),</li>
<li>BUT THEN, they enter the near-linear regime, in which smaller models have a
steeper descent into superior knowledge, and they overtake larger models
yet again!</li>
</ol>
<p>A fascinating consequence ties into making the right choices
when starting a training run:
contrary to popular belief, <strong>larger models yield worse results</strong>.
If you had to pick a parameter size and dataset, you might be better off opting
for a 7B model and training for 7 epochs on trillions of tokens.</p>
<p>Look at the near-linear regime of the 7B model, and extrapolate its line to when
the 70B model stopped:
had the 70B computation been spent on the 7B instead,
it would potentially have reached a lower perplexity!</p>
<p>Another thing we notice from LLaMA 2 is that the learning slowdown at the end of
the LLaMA 1 curves was indeed an artefact of the cosine schedule.
That slowdown is completely absent from the LLaMA 2 training run at the
corresponding mark of 1 trillion tokens read.</p>
<p>In fact, maybe the reason that, at that same mark, the LLaMA 2 7B model has a
worse quality than the LLaMA 1 7B model had,
may be because <em>its cosine schedule is stretched</em>!</p>
<p>Let’s go back to the Chinchilla paper to argue that point.
In appendix A, figure A1, they show an ablation study for various cosine
schedule parameters (phrased another way:
various ways to stretch the learning rate curve).</p>
<p><img src="../assets/chinchilla-s-death/chinchilla-cosine-ablation-study.png" alt="Chinchilla cosine schedule ablation study" /></p>
<p>They make the point that the lowest loss is achieved when the curve is not
stretched. That is supported by the graphs, but we notice something off.
After reading 6 million tokens, the training loss at the top is below 2.8;
meanwhile, at the same mark, the training loss of the bottom model is above.
Yet the only difference between the models is the cosine schedule!
Because the bottom model was slated to go through more training data,
the “unstretched” cosine schedule was computed for a bigger number of steps,
which effectively stretches it.
If the learning rate had instead followed
the schedule assigned to fewer training steps,
it would have had a better loss for the same amount of training time.</p>
<p>More broadly, that raises a question that I leave open:
if the cosine schedule is not optimal,
how should the shape of its tail be instead?</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2023-07-23T17:35:02Z",
  "keywords": "gpu, ml" }
</script> ]]>
        </content>
      </entry>
      <entry>
        <id>https://espadrine.github.io/blog/posts/recomputing-gpu-performance.html</id>
        <link rel="alternate" type="text/html" href="https://espadrine.github.io/blog/posts/recomputing-gpu-performance.html"/>
        <title>Recomputing ML GPU performance: AMD vs. NVIDIA</title>
        <published>2023-06-18T21:40:09Z</published>
        <category term="gpu"/>
<category term="ml"/>
        <content type="html">
          <![CDATA[ <h1 id="Recomputing_ML_GPU_performance_AMD_vs_NVIDIA">Recomputing ML GPU performance: AMD vs. NVIDIA <a href="#Recomputing_ML_GPU_performance_AMD_vs_NVIDIA" class="autolink-clicker" aria-hidden="true">§</a></h1>
<p>I am pretty impressed seeing <a href="https://twitter.com/LisaSu/status/1669848494637735936">Lisa Su</a> doing her best to steer the AMD ship towards
better AI support in GPUs, with the <a href="https://huggingface.co/blog/huggingface-and-amd">Huggingface partnership</a> and by convincing
George Hotz to submit more bug reports.</p>
<p>(For context, <a href="https://geohot.github.io//blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html">Hotz raised $5M</a> to improve RX 7900 XTX support and sell a $15K
prebuilt consumer computer that runs 65B-parameter LLMs. A plethora of driver
crashes later, he almost <a href="https://github.com/RadeonOpenCompute/ROCm/issues/2198#issuecomment-1574383483">gave up on AMD</a>.)</p>
<p>There’s quite a few issues to overcome, though.
While that GPU is great
(<a href="https://www.tomshardware.com/news/stable-diffusion-gpu-benchmarks">Stable Diffusion iteration speed per GPU cost</a> is top-tier),
a cursory study would be flawed:
public GPU benchmarks like TechPowerUp, TomsHardware, etc. give:</p>
<ul>
<li><strong>RX 7900 XTX:</strong> <a href="https://www.techpowerup.com/gpu-specs/geforce-rtx-4090.c3889">123 TFLOPS</a></li>
<li><strong>RTX 4090:</strong> <a href="https://www.tomshardware.com/reviews/amd-radeon-rx-7900-xtx-and-xt-review-shooting-for-the-top">82.58 TFLOPS</a></li>
</ul>
<p>Where do the figures come from?</p>
<p>While there is no official breakdown,
only <a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx">official figures</a>, people widely compute it this way:</p>
<ul>
<li>For <strong>NVIDIA</strong>:
<a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0">Boost Clock (THz) × CUDA Cores × 2</a>
(since the FMA instruction does two floating-point operations
(a multiplication and an addition) in 1 CUDA core cycle).</li>
<li>For <strong>AMD</strong> on RDNA3:
<a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx">Boost Frequency (THz) × Stream processors × 2 (dual issue) × 4 (dot product)</a>,
as <a href="https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf">RDNA3 has <code>V_DUAL_DOT2ACC_F32_F16</code></a>,
which does two dot products (a×b+c×d+e, 4 operations),
in 1 processor cycle.</li>
</ul>
<table>
  <tr><th> Name </th><th> Price </th><th> Processors </th><th> Frequency </th><th> TFLOPS (FP16) </th><th> Perf/€ </th>
  <tr><td> <a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx">RX 7900 XTX</a> </td>
      <td> €1110 </td><td>  6144 </td><td>  2.5 GHz </td><td> 122.88 </td><td> 0.1107 </td>
  <tr><td> <a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xt">RX 7900 XT</a> </td>
      <td>  €942 </td><td>  5376 </td><td>  2.4 GHz </td><td> 103.22 </td><td> 0.1096 </td>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0">RTX 4090</a> </td>
      <td> €1770 </td><td> 16384 </td><td> 2.52 GHz </td><td>  82.58 </td><td> 0.0467 </td>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85">RTX 3060</a> </td>
      <td>  €314 </td><td>  3584 </td><td> 1.78 GHz </td><td>  12.76 </td><td> 0.0405 </td>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85">RTX 3080</a> </td>
      <td>  €905 </td><td>  8704 </td><td> 1.71 GHz </td><td>  29.76 </td><td> 0.0329 </td>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85">RTX 3090</a> </td>
      <td> €1500 </td><td> 10496 </td><td> 1.70 GHz </td><td>  35.68 </td><td> 0.0238 </td>
</table>
<p>That is an unjust comparison, though, because AMD’s instruction is more niche
than FMA (hitting this performance sweet spot is thus uncommon),
and because both of those GPUs have other tricks up their sleeves,
yielding superior FLOPS.</p>
<p>The <a href="https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#Will_AMD_GPUs_ROCm_ever_catch_up_with_NVIDIA_GPUs_CUDA">big one</a> on NVIDIA are <a href="https://www.nvidia.com/en-us/data-center/tensor-cores/">Tensor cores</a>.
With them, you can run an instruction that does
<a href="https://www.nvidia.com/content/PDF/nvidia-ampere-ga-102-gpu-architecture-whitepaper-v2.pdf">a 4×4 to 4×8 matrix multiplication (page 25)</a>
in 1 cycle within a single Tensor Core (32 CUDA cores).</p>
<p>2×4^2×8 (matmul ops) ÷ 1 (cycles) = 256 ops/TC/cycle.</p>
<p>(There is <a href="https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/">some variation between NVIDIA GPUs</a>
on which matrix sizes are supported and on how many cycles the instruction takes,
and NVIDIA keeps major aspects of their instruction set secret,
but on recent 30- and 40-series, this 256 number seems fairly constant.)</p>
<p><img src="https://www.nvidia.com/content/dam/en-zz/Solutions/gtcs22/tensor-cores/hopper-tensor-core-ampere-2c50-t.jpg" alt="Official image showing the matrix size for third-generation tensor cores with V100 FP32" /></p>
<p>That actually puts the RTX 4090 at
256 × 512 (Tensor Cores) × 2.52 (GHz)
÷ 1K (GHz per teracycle/s) = <a href="https://en.wikipedia.org/wiki/GeForce_40_series#Desktop">330 TFLOPS in FP16</a>…
Much higher than the 123 TFLOPS that impressed Hotz on the RX 7900 XTX!</p>
<p>But AMD now has the same trick.
In <a href="https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf">RDNA3</a>, with <a href="https://gpuopen.com/learn/wmma_on_rdna3/">WMMA</a>, the RX 7900 XTX has an instruction,
<a href="https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf"><code>V_WMMA_F16_16X16X16_F16</code></a>
that do two 16×16 matrix multiplications in <a href="https://github.com/RadeonOpenCompute/amd_matrix_instruction_calculator/blob/339d784e56e55752495192b0781ea162fc32e323/matrix_calculator.py#LL1139C26-L1139C26">32 cycles</a>,
in a single Compute Unit (two sets of 32 threads).</p>
<p>2×16^3 (matmul ops) × 2 ÷ 32 (cycles) = 512 ops/CU/cycle.</p>
<p>This uses the same underlying silicon circuits as <code>V_DUAL_DOT2ACC_F32_F16</code>:
the architecture lays out the matrices in Vector General-Purpose Registers.
Each cell of the output matrix is computed by multiplying
one row from input matrix A with one column from input matrix B,
two input cells at a time
(two adjacent input A row cells packed inside the same VGPR,
and two adjacent input B column cells packed together inside another VGPR),
so they can be used by the packed dot product single-cycle instruction.
Within that same instruction, encoded in VOPQ
(a SIMD-like system to execute one operation
on an even register while it executes on an odd one at the same time),
an adjacent output cell also multiplies through its first two input cells
at the same time using dual issue.</p>
<p>The input row has size 16, so those two output cells are completed in 8 cycles.
Each two adjacent output cells in their diagonal
are computed with 16 parallel threads (on separate stream processors)
within the same 8 cycles.
We have done two diagonals (32 output cells); there are 14 diagonals left.
Inside that Compute Unit, we still have 16 stream processors that we can use;
they can handle two more output diagonals within the same 8 cycles.</p>
<p>Once our first four diagonals are computed,
we sequentially compute the next 4 diagonals in the next 8 cycles.
So forth for the next 4, and the last 4 after that.
In total, we have computed the matrix multiplication
in 32 cycles, which checks out.</p>
<p>Why can’t we do the matrix multiplication in 16 cycles
by using all 64 threads inside of the Compute Unit?
<a href="https://www.amd.com/system/files/TechDocs/rdna3-shader-instruction-set-architecture-feb-2023_0.pdf">Section 7.6 of the instruction set manual</a> indicates:</p>
<blockquote>
<p>[Dual issue] is legal only for wave32.</p>
</blockquote>
<p>WMMA supports both wave32 and wave64, but it sounds like dual issue is
deactivated in wave64, and thus it would still take 32 cycles,
making it an ill-documentedly unfavorable proposition, I believe.</p>
<p>All in all, using <a href="https://gpuopen.com/learn/wmma_on_rdna3/">WMMA</a>, the RX 7900 XTX can crank through
512 × <a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx">96 (Compute Units) × 2.5 (GHz)</a>
÷ 1K (GHz per teracycle/s) = <a href="https://en.wikipedia.org/wiki/RDNA_3#Desktop">123 TFLOPS in FP16</a>…</p>
<p>That ends up being less than half the performance of the RTX 4090.
The superior number of operations per Compute Unit is offset by the
crushingly lower number of cores.
Perhaps the AMD strategy is to have the better circuit ready
before migrating to the TSMC N5 (“5 nm”) process at a less affordable price.</p>
<p>In practice, the lower performance is less of an issue for AI training,
because they are famously limited in the amount of parallelization opportunities
(even the best training runs typically incur only 50% GPU use at a given time).
The VRAM bandwidth then matters a lot for large models,
and the <a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx">RX 7900 XTX</a>, despite using GDDR6 instead of GDDR6X,
has a higher bandwidth than the RTX 3090, thanks to its faster memory clock.
Still, it also is lower than the RTX 4090 on that front
(but at a lower price point).</p>
<table>
  <tr><th> Name </th><th> Price </th><th> TFLOPS (FP16) </th><th> Memory bandwidth (GB/s)</th><th> Value (TFLOPS·GB/s/€) </th>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_54756033603dff4c2_db18_46bd_9cc1_e7ad0debbbd0">RTX 4090</a> </td>
      <td> €1770 </td><td> 330 </td><td> 1008 </td><td> 188 </td>
  <tr><td> <a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xtx">RX 7900 XTX</a> </td>
      <td> €1110 </td><td> 123 </td><td> 960 </td><td> 106 </td>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85">RTX 3080</a> </td>
      <td>  €905 </td><td> 119 </td><td> 760 </td><td> 100 </td>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85">RTX 3090</a> </td>
      <td> €1500 </td><td> 143 </td><td> 936 </td><td> 89 </td>
  <tr><td> <a href="https://www.amd.com/en/products/graphics/amd-radeon-rx-7900xt">RX 7900 XT</a> </td>
      <td>  €942 </td><td> 103 </td><td> 800 </td><td> 87 </td>
  <tr><td> <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/compare/#sectionenhanced_copy_44862952d932bba4_58ad_4ca4_a3d3_84a2295d2b85">RTX 3060</a> </td>
      <td>  €314 </td><td> 51 </td><td> 360 </td><td> 58 </td>
</table>
<p>Thus the RX 7900 XTX is not technically the best TFLOPS per price,
as was presumed in Hotz’s <a href="https://geohot.github.io//blog/jekyll/update/2023/05/24/the-tiny-corp-raised-5M.html">raise announcement</a>.
But that metric is not crucial for the purpose of making LLM machines,
and purely looking at hardware, that GPU is a fine choice for that,
in part because it has a fairer RAM per dollar offer,
so that it can hold a large model without needing pricier GPUS,
yet likely reaching reasonable inference speeds.</p>
<p>The other thorns on the side of AMD in AI, though, rear their ugly heads:</p>
<ul>
<li><a href="https://chipsandcheese.com/2023/01/07/microbenchmarking-amds-rdna-3-graphics-architecture/">The compilers don’t produce great instructions</a>;</li>
<li>The drivers crash frequently: ML workloads feel experimental;</li>
<li>Software adoption is getting there,
but kernels are less optimized within frameworks,
in particular because of the fracture between ROCm and CUDA.
When you are a developer and you need to write code twice,
one version won’t be as good, and it is the one with less adoption;</li>
<li>StackOverflow mindshare is lesser. Debugging problems is thus harder,
as fewer people have encountered them.</li>
</ul>
<p>(I will note, however, that the wealth of information provided by AMD
outshines that from NVIDIA tremendously,
even though they could better vulgarize those subtleties and
explain how to perform specific workloads like BERT training,
into which NVIDIA puts welcome care.
Just contrast <a href="https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html">NVIDIA’s matmul page</a> to <a href="https://gpuopen.com/learn/wmma_on_rdna3/">AMD’s</a>.
<a href="https://rocm.docs.amd.com/en/latest/release/gpu_os_support.html">AMD doesn’t even recognize its own flagship GPUs as supported for ROCm</a>,
which is mindboggling coming from NVIDIA’s superior CUDA support.)</p>
<hr />
<p><a href="https://www.reddit.com/r/espadrine/comments/156bbmj/recomputing_gpu_performance/">Comments on Reddit</a>.</p>
<script type="application/ld+json">
{ "@context": "http://schema.org",
  "@type": "BlogPosting",
  "datePublished": "2023-06-18T21:40:09Z",
  "keywords": "gpu, ml" }
</script> ]]>
        </content>
      </entry>
</feed>
